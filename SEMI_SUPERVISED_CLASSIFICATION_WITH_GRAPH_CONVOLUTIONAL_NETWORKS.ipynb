{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tae-Hwanii/SEMI-SUPERVISED-CLASSIFICATION-WITH-GRAPH-CONVOLUTIONAL-NETWORKS/blob/main/SEMI_SUPERVISED_CLASSIFICATION_WITH_GRAPH_CONVOLUTIONAL_NETWORKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "    # Module 클래스를 상속받는 GraphConvolution 클래스의 생성자\n",
        "    # in_feauters : 입력 특징의 수, out_features : 출력 특성의 수, bias : 편향 사용 여부\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # 상위 클래스(Module)의 생성자를 호출하여 초기화\n",
        "        self.in_features = in_features\n",
        "        # 입력 특성 수를 클래스 속성으로 저장\n",
        "        self.out_features = out_features\n",
        "        # 출력 특성의 수를 클래스 속성으로 저장\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        # 레이어의 가중치 행렬을 정의하고 Parameter로 감싸서 모델 파라미터로 만듦\n",
        "        # 크기는 (입력 특성의 수, 출력 특성의 수)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "            # 편향 사용 여부에 따라 편향 항을 정의하고 Parameter로 감싸서 모델 파라미터로 만듦\n",
        "            # 크기는 (출력 특성의 수)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            # 편향을 사용하지 않는 경우, bias를 None으로 등록하여 레이어에서 사용되지 않도록 힘\n",
        "        self.reset_parameters()\n",
        "        # reset_parameters() 함수를 호출하여 가중치와 편향을 초기화\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        # 가중치 초기화에 사용할 표준 편차를 계산\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # 가중치 행렬을 -stdv에서 stdv 사이의 균일한 랜덤 값으로 초기화\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "            # 편향이 존재하는 경우, 편향을 -stdv에서 stdv 사이의 균일한 랜덤 값으로 초기화\n",
        "\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # 그래프 합성곱 레이어의 순방향 연산을 정의하는 메서드\n",
        "        # input : 입력 데이터 (노드의 특성 행렬)\n",
        "        # adj : 그래프의 연결성 정보를 나타내는 희소 행렬 (인접 행렬)\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # 입력 데이터와 가중치 행렬을 곱하여 support를 계산\n",
        "        # support는 입력 데이터에 가중치를 적용한 결과로, 각 노드에 대한 새로운 특성 행렬\n",
        "        output = torch.spmm(adj, support)\n",
        "        # 희소 행렬과 support를 곱하여 그래프 합성곱 연산을 수행하고 output을 얻음\n",
        "        # output은 그래프에 따라 연결된 이웃 노드의 정보를 사용한 결과로, 각 노드에 대한 새로운 특성 행렬\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "            # 편향(bias)이 존재하는 경우, 결과에 편향을 더하고 반환\n",
        "        else:\n",
        "            return output\n",
        "            # 편향이 존재하지 않는 경우, 그래프 합성곱 연산 결과만 반환\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        # 객체를 문자열로 표현하는 메서드 정의\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "                + str(self.in_features) + ' -> ' \\\n",
        "                + str(self.out_features) + ')'\n",
        "        # 객체의 클래스 이름과 입력 및 출력 특성의 수를 포함한 문자열 반환"
      ],
      "metadata": {
        "id": "UquzPzJKgyHG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        # GCN 모델의 생성자\n",
        "        # nfeat : 입력 특성의 수, nhid : 은닉층의 특성 수, nclass : 출력 클래스 수, dropout : 드롭아웃 확률\n",
        "        super(GCN, self).__init__()\n",
        "        # 상위 클래스(nn.Moudle)의 생성자 호출\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        # 첫 번째 그래프 합성곱 레이어 (입력 특성 -> 은닉층)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        # 두 번째 그래프 합성곱 레이어 (은닉층 -> 출력 클래스)\n",
        "        self.dropout = dropout\n",
        "        # 드롭아웃 확률\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # 순방향 연산 메서드\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        # 첫 번째 그래프 합성곱 레이어를 통과한 후 ReLU 활성화 함수 적용\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # 드롭아웃 적용\n",
        "        x = self.gc2(x, adj)\n",
        "        # 두 번째 그래프 합성곱 레이어를 통과\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        # 소프트맥스 함수를 사용하여 출력을 확률 분포로 변환하고, 로그 확률값 반환"
      ],
      "metadata": {
        "id": "Le0wKbChotFs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    # 레이블을 원-핫 인코딩하는 함수 정의\n",
        "    # labels : 입력으로 주어진 레이블 리스트\n",
        "    classes = set(labels)\n",
        "    # 주어진 레이블 리스트에서 고유한 클래스(레이블)를 추출하여 집합(set)으로 만듦\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    # 각 클래스(레이블)를 원-핫 인코딩으로 나타내는 딕셔너리(class_dict) 생성\n",
        "    # np.identity(len(classes))는 크기가 클래스 수와 같은 단위 행렬을 생성\n",
        "    # enumerate(classes)를 통해 클래스(레이블)를 순회하며 해당 클래스에 해당하는 단위 행렬 행을 추출\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    # labels 리스트의 각 레이블을 classes_dict에서 찾아 원-핫 인코딩으로 반환\n",
        "    # 반환된 결과를 Numpy 배열로 변환하고 데이터 타입을 int32로 설정\n",
        "    return labels_onehot\n",
        "    # 원-핫 인코딩된 레이블을 반환\n",
        "\n",
        "def load_data(path=\"/content/drive/MyDrive/cora/\", dataset=\"cora\"):\n",
        "    # 데이터를 로드하고 전처리하는 함수 정의\n",
        "    # path : 데이터 파일이 위치한 경로, dataset : 데이터셋 이름\n",
        "    print('Loading {} dataset. . .'.format(dataset))\n",
        "    # 데이터셋 로딩 메시지 출력\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    print(idx_features_labels)\n",
        "    # 데이터셋 파일에서 데이터를 읽어와서 numpy 배열로 저장\n",
        "    # 데이터는 공백으로 구분되어 있고, dtype(str)을 사용하여 문자열로 읽음\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    # 데이터에서 특성(features) 부분을 추출하여 희소 특성 행렬(csr_matrix)로 변환\n",
        "    # 1번 열부터 마지막에서 1번 열까지를 선택하며, 데이터 타입을 float32로 설정\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "    # 데이터에서 레이블(Label) 부분을 추출하여 원-핫 인코딩된 레이블로 변환\n",
        "    # encode_onehot 함수를 사용하여 레이블을 원-핫 인코딩\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    # 데이터에서 인덱스 정보를 추출하여 NumPy 배열로 저장\n",
        "    # 인덱스는 정수형(int32)으로 저장됨\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    # 인덱스를 매핑하는 딕셔너리(idx_map) 생성\n",
        "    # 기존 인덱스를 새로운 인덱스로 매핑하는 역할을 함\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    # 그래프의 엣지 정보를 포함한 텍스트 파일을 읽어와서 NumPy 배열로 저장\n",
        "    # 데이터 타입은 정수형(int32)으로 설정\n",
        "    print(edges_unordered)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    # 엣지 정보의 인덱스를 새로운 인덱스로 변환\n",
        "    # idx_map을 사용하여 엣지 정보의 인덱스를 새로운 인덱스로 매핑\n",
        "    # 엣지 정보의 배열을 펼친 다음, 새로운 인덱스로 변환하고 다시 원래 형태로 변환\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
        "    # 희소 고유값(coo) 행렬을 생성하여 그래프의 인접 행렬(adjacenct matrix)을 나타냄\n",
        "    # (edges.shape[0])은 엣지의 수를 나타냄\n",
        "    # (edges[:, 0], edges[:, i])은 각 엣지의 연결된 노드 인덱스를 나타냄\n",
        "    # shape=(labels.shape[0], labels.shape[0])은 인접 행렬의 크기를 설정\n",
        "    # dtype=np.float32은 데이터 타입을 부동소수점(float32)으로 설정\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    # 인접 행렬(adj)을 대칭으로 만듦\n",
        "    # adj.T는 인접 행렬을 전치한 것이며, '>' 연산자로 대칭성을 검사\n",
        "    # multiply 함수를 사용하여 대칭된 부분만 남기고, 대칭되지 않은 부분을 0으로 만듦\n",
        "\n",
        "    features = normalize(features)\n",
        "    # 특성 행렬(features)을 정규화(normalize)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    # 대칭 인접 행렬(adj)에 단위 행렬(eye)을 더하고 다시 정규화\n",
        "    # 이렇게 함으로써 자기 루프(self-loop)를 고려한 인접 행렬 생성\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "    # 훈련, 검증, 테스트 데이터의 인덱스를 설정\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    # 특성 행렬을 NumPy 배열로 변환하고, PyTorch의 Float Tensor로 변환\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    # 레이블을 NumPy 배열로 변환하고, PyTorch의 LongTensor로 변환\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    # 인접 행렬을 PyTorch의 희소 텐서(Sparse Tensor)로 변환\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "    # 인덱스들을 PyTorch의 LongTensor로 변환\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "    # 준비된 데이터를 반환\n",
        "\n",
        "def normalize(mx):\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    # 행렬의 각 행(row)의 합을 계산하여 배열(rowsum)로 저장\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    # 각 행의 합에 역수를 취하고, 0으로 나누는 경우(무한대 역수)에 대한 처리를 수행\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    # 대각 행렬(diagonal matrix)을 생성하고, 대각 성분에 역수 값을 포함\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    # 입력 행렬(mx)에 대각 행렬을 왼쪽에서 곱하여 행렬을 정규화\n",
        "    return mx\n",
        "    # 정규화된 행렬을 반환\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    # 모델의 출력(output)에서 가장 큰 값의 인덱스(max(1)[1])를 선택하여 예측(preds)을 구함\n",
        "    # 예측(preds)을 실제 레이블(labels)의 데이터 타입(type_as)으로 변환\n",
        "    correct = preds.eq(labels).double()\n",
        "    # 예측(preds)과 실제 레이블(labels)을 비교하여 일치하는 경우 1, 아닌 경우 0인 이진 값으로 변환\n",
        "    correct = correct.sum()\n",
        "    # 일치하는 값을 합산하여 정확한 예측의 수를 계산\n",
        "    return correct / len(labels)\n",
        "    # 정확한 예측의 수를 전체 레이블(labels)의 수로 나누어 정확도(accuracy)를 계산\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "     # 희소 행렬(sparse_mx)을 COO(coordinate List) 형식으로 변환하고 데이터 타입을 float32로 변환\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "     # COO 형식의 희소 행렬을 PyTorch 텐서로 변환하기 위해 인덱스(indices), 값(values), 크기(shape)를 추출\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "    # PyTorch의 희소 텐서(Sparse Tensor)를 생성하여 변환"
      ],
      "metadata": {
        "id": "jSkjVNdLsQ73"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#training setting\n",
        "\n",
        "no_cuda = False\n",
        "fastmode = False\n",
        "seed = 12\n",
        "epochs = 1000\n",
        "lr = 0.01\n",
        "weight_decay = 5e-4\n",
        "hidden = 16\n",
        "dropout = 0.5\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# 실험의 재현성을 위해 난수 시드 설정\n",
        "# NumPy 및 PyTorch의 난수 생성을 제어하기 위한 시드 설정\n",
        "\n",
        "#Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "#Model and optimizer\n",
        "model = GCN(nfeat = features.shape[1],                    # 입력 특성의 크기를 모델에 전달\n",
        "            nhid = hidden,                           # 은닉층의 크기 설정 (hidden units)\n",
        "            nclass = labels.max().item() + 1,             # 클래스 수를 레이블에서 결정\n",
        "            dropout = dropout)                       # 드롭아웃 확률 설정\n",
        "optimizer = optim.Adam(model.parameters(),              # Adam 최적화 알고리즘 사용\n",
        "                       lr = lr,                      # 학습률(learning rate) 설정\n",
        "                       weight_decay = weight_decay)  # 가중치 감쇠(L2 손실) 설정\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    # 현재 학습 epoch의 시작 시간 기록\n",
        "    model.train()\n",
        "    # 모델 학습 모드로 설정\n",
        "    optimizer.zero_grad()\n",
        "    # 기울기 초기화\n",
        "    output = model(features, adj)\n",
        "    # 모델에 입력 데이터와 인접 행렬을 전달하여 출력 예측 계산\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    # 학습 데이터에 대한 손실(loss) 및 정확도(accuracy) 계산\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    # 역전파 및 모델 파라미터 업데이트\n",
        "\n",
        "    if not fastmode:\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        # 만약 'fastmode'가 비활성화된 경우, 검증 데이터에 대한 평가 수행\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    # 검증 데이터에 대한 손실(loss) 및 정확도(accuracy) 계산\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    # 모델을 평가 모드로 설정\n",
        "    output = model(features, adj)\n",
        "    # 모델을 사용하여 출력 예측 계산\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    # 테스트 데이터에 대한 손실(loss)과 정확도(accuracy) 계산\n",
        "    print(\"Test set results\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "t_total = time.time()\n",
        "# 학습 시간 측정을 위한 변수 설정\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time slapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "DIX0dvSOC9oG",
        "outputId": "469ff31e-2267-4b87-c3de-cf8797a2e591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset. . .\n",
            "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
            " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
            " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
            " ...\n",
            " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
            " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
            " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']]\n",
            "[[     35    1033]\n",
            " [     35  103482]\n",
            " [     35  103515]\n",
            " ...\n",
            " [ 853118 1140289]\n",
            " [ 853155  853118]\n",
            " [ 954315 1155073]]\n",
            "Epoch: 0001 loss_train: 1.9497 acc_train: 0.1429 loss_val: 1.9219 acc_val: 0.1267 time: 0.0315s\n",
            "Epoch: 0002 loss_train: 1.9366 acc_train: 0.1500 loss_val: 1.9043 acc_val: 0.1267 time: 0.0250s\n",
            "Epoch: 0003 loss_train: 1.8958 acc_train: 0.1714 loss_val: 1.8871 acc_val: 0.2433 time: 0.0243s\n",
            "Epoch: 0004 loss_train: 1.8905 acc_train: 0.2500 loss_val: 1.8708 acc_val: 0.3500 time: 0.0278s\n",
            "Epoch: 0005 loss_train: 1.8682 acc_train: 0.2643 loss_val: 1.8556 acc_val: 0.3500 time: 0.0250s\n",
            "Epoch: 0006 loss_train: 1.8493 acc_train: 0.2714 loss_val: 1.8405 acc_val: 0.3500 time: 0.0247s\n",
            "Epoch: 0007 loss_train: 1.8336 acc_train: 0.3214 loss_val: 1.8263 acc_val: 0.3500 time: 0.0251s\n",
            "Epoch: 0008 loss_train: 1.8224 acc_train: 0.3000 loss_val: 1.8127 acc_val: 0.3567 time: 0.0252s\n",
            "Epoch: 0009 loss_train: 1.8168 acc_train: 0.3786 loss_val: 1.7999 acc_val: 0.3600 time: 0.0254s\n",
            "Epoch: 0010 loss_train: 1.7806 acc_train: 0.3286 loss_val: 1.7877 acc_val: 0.3633 time: 0.0247s\n",
            "Epoch: 0011 loss_train: 1.7765 acc_train: 0.3500 loss_val: 1.7760 acc_val: 0.3633 time: 0.0255s\n",
            "Epoch: 0012 loss_train: 1.7549 acc_train: 0.3714 loss_val: 1.7647 acc_val: 0.3633 time: 0.0293s\n",
            "Epoch: 0013 loss_train: 1.7207 acc_train: 0.3643 loss_val: 1.7538 acc_val: 0.3633 time: 0.0253s\n",
            "Epoch: 0014 loss_train: 1.7121 acc_train: 0.3643 loss_val: 1.7435 acc_val: 0.3633 time: 0.0283s\n",
            "Epoch: 0015 loss_train: 1.6999 acc_train: 0.3500 loss_val: 1.7338 acc_val: 0.3633 time: 0.0248s\n",
            "Epoch: 0016 loss_train: 1.6992 acc_train: 0.3857 loss_val: 1.7249 acc_val: 0.3633 time: 0.0273s\n",
            "Epoch: 0017 loss_train: 1.7123 acc_train: 0.3714 loss_val: 1.7162 acc_val: 0.3633 time: 0.0245s\n",
            "Epoch: 0018 loss_train: 1.6925 acc_train: 0.4000 loss_val: 1.7079 acc_val: 0.3633 time: 0.0256s\n",
            "Epoch: 0019 loss_train: 1.6904 acc_train: 0.4000 loss_val: 1.6997 acc_val: 0.3633 time: 0.0312s\n",
            "Epoch: 0020 loss_train: 1.6360 acc_train: 0.4214 loss_val: 1.6914 acc_val: 0.3667 time: 0.0281s\n",
            "Epoch: 0021 loss_train: 1.6642 acc_train: 0.4143 loss_val: 1.6829 acc_val: 0.3667 time: 0.0292s\n",
            "Epoch: 0022 loss_train: 1.6506 acc_train: 0.3786 loss_val: 1.6743 acc_val: 0.3767 time: 0.0284s\n",
            "Epoch: 0023 loss_train: 1.6529 acc_train: 0.3929 loss_val: 1.6656 acc_val: 0.3767 time: 0.0250s\n",
            "Epoch: 0024 loss_train: 1.6181 acc_train: 0.4214 loss_val: 1.6568 acc_val: 0.3833 time: 0.0264s\n",
            "Epoch: 0025 loss_train: 1.6402 acc_train: 0.4429 loss_val: 1.6477 acc_val: 0.3900 time: 0.0253s\n",
            "Epoch: 0026 loss_train: 1.5755 acc_train: 0.4143 loss_val: 1.6385 acc_val: 0.4033 time: 0.0299s\n",
            "Epoch: 0027 loss_train: 1.5514 acc_train: 0.4571 loss_val: 1.6290 acc_val: 0.4133 time: 0.0265s\n",
            "Epoch: 0028 loss_train: 1.5793 acc_train: 0.4000 loss_val: 1.6192 acc_val: 0.4133 time: 0.0252s\n",
            "Epoch: 0029 loss_train: 1.5445 acc_train: 0.4286 loss_val: 1.6092 acc_val: 0.4167 time: 0.0256s\n",
            "Epoch: 0030 loss_train: 1.5433 acc_train: 0.4643 loss_val: 1.5992 acc_val: 0.4233 time: 0.0272s\n",
            "Epoch: 0031 loss_train: 1.4928 acc_train: 0.4643 loss_val: 1.5889 acc_val: 0.4200 time: 0.0246s\n",
            "Epoch: 0032 loss_train: 1.4841 acc_train: 0.4643 loss_val: 1.5785 acc_val: 0.4233 time: 0.0259s\n",
            "Epoch: 0033 loss_train: 1.4971 acc_train: 0.4714 loss_val: 1.5677 acc_val: 0.4233 time: 0.0265s\n",
            "Epoch: 0034 loss_train: 1.4631 acc_train: 0.4500 loss_val: 1.5568 acc_val: 0.4300 time: 0.0285s\n",
            "Epoch: 0035 loss_train: 1.4113 acc_train: 0.4643 loss_val: 1.5456 acc_val: 0.4333 time: 0.0272s\n",
            "Epoch: 0036 loss_train: 1.4244 acc_train: 0.4714 loss_val: 1.5337 acc_val: 0.4467 time: 0.0318s\n",
            "Epoch: 0037 loss_train: 1.3777 acc_train: 0.4714 loss_val: 1.5215 acc_val: 0.4600 time: 0.0272s\n",
            "Epoch: 0038 loss_train: 1.3828 acc_train: 0.5429 loss_val: 1.5087 acc_val: 0.4667 time: 0.0267s\n",
            "Epoch: 0039 loss_train: 1.3586 acc_train: 0.4714 loss_val: 1.4960 acc_val: 0.4833 time: 0.0258s\n",
            "Epoch: 0040 loss_train: 1.3369 acc_train: 0.5286 loss_val: 1.4831 acc_val: 0.5033 time: 0.0353s\n",
            "Epoch: 0041 loss_train: 1.3289 acc_train: 0.5429 loss_val: 1.4702 acc_val: 0.5033 time: 0.0392s\n",
            "Epoch: 0042 loss_train: 1.3008 acc_train: 0.5643 loss_val: 1.4572 acc_val: 0.5067 time: 0.0373s\n",
            "Epoch: 0043 loss_train: 1.3260 acc_train: 0.5429 loss_val: 1.4442 acc_val: 0.5133 time: 0.0343s\n",
            "Epoch: 0044 loss_train: 1.2607 acc_train: 0.5643 loss_val: 1.4311 acc_val: 0.5267 time: 0.0339s\n",
            "Epoch: 0045 loss_train: 1.2637 acc_train: 0.5714 loss_val: 1.4180 acc_val: 0.5300 time: 0.0357s\n",
            "Epoch: 0046 loss_train: 1.2388 acc_train: 0.5429 loss_val: 1.4052 acc_val: 0.5333 time: 0.0410s\n",
            "Epoch: 0047 loss_train: 1.2890 acc_train: 0.5786 loss_val: 1.3921 acc_val: 0.5433 time: 0.0343s\n",
            "Epoch: 0048 loss_train: 1.1972 acc_train: 0.6214 loss_val: 1.3790 acc_val: 0.5433 time: 0.0352s\n",
            "Epoch: 0049 loss_train: 1.1806 acc_train: 0.5500 loss_val: 1.3659 acc_val: 0.5600 time: 0.0352s\n",
            "Epoch: 0050 loss_train: 1.2110 acc_train: 0.5786 loss_val: 1.3529 acc_val: 0.5633 time: 0.0323s\n",
            "Epoch: 0051 loss_train: 1.1779 acc_train: 0.6571 loss_val: 1.3401 acc_val: 0.5700 time: 0.0349s\n",
            "Epoch: 0052 loss_train: 1.0801 acc_train: 0.6643 loss_val: 1.3276 acc_val: 0.5833 time: 0.0468s\n",
            "Epoch: 0053 loss_train: 1.0595 acc_train: 0.7071 loss_val: 1.3154 acc_val: 0.6000 time: 0.0391s\n",
            "Epoch: 0054 loss_train: 1.0695 acc_train: 0.6643 loss_val: 1.3029 acc_val: 0.6233 time: 0.0329s\n",
            "Epoch: 0055 loss_train: 1.0896 acc_train: 0.6643 loss_val: 1.2902 acc_val: 0.6200 time: 0.0326s\n",
            "Epoch: 0056 loss_train: 1.0867 acc_train: 0.7000 loss_val: 1.2773 acc_val: 0.6333 time: 0.0318s\n",
            "Epoch: 0057 loss_train: 1.0253 acc_train: 0.7000 loss_val: 1.2643 acc_val: 0.6467 time: 0.0326s\n",
            "Epoch: 0058 loss_train: 1.0403 acc_train: 0.7571 loss_val: 1.2512 acc_val: 0.6533 time: 0.0376s\n",
            "Epoch: 0059 loss_train: 1.0427 acc_train: 0.7286 loss_val: 1.2379 acc_val: 0.6567 time: 0.0334s\n",
            "Epoch: 0060 loss_train: 1.0171 acc_train: 0.7214 loss_val: 1.2248 acc_val: 0.6633 time: 0.0330s\n",
            "Epoch: 0061 loss_train: 1.0027 acc_train: 0.7286 loss_val: 1.2119 acc_val: 0.6667 time: 0.0319s\n",
            "Epoch: 0062 loss_train: 0.9832 acc_train: 0.7429 loss_val: 1.1993 acc_val: 0.6733 time: 0.0325s\n",
            "Epoch: 0063 loss_train: 0.9595 acc_train: 0.7571 loss_val: 1.1871 acc_val: 0.6900 time: 0.0328s\n",
            "Epoch: 0064 loss_train: 0.9840 acc_train: 0.7286 loss_val: 1.1751 acc_val: 0.6933 time: 0.0420s\n",
            "Epoch: 0065 loss_train: 0.9218 acc_train: 0.7429 loss_val: 1.1634 acc_val: 0.7033 time: 0.0323s\n",
            "Epoch: 0066 loss_train: 0.9211 acc_train: 0.7643 loss_val: 1.1514 acc_val: 0.7067 time: 0.0369s\n",
            "Epoch: 0067 loss_train: 0.9170 acc_train: 0.7429 loss_val: 1.1399 acc_val: 0.7067 time: 0.0353s\n",
            "Epoch: 0068 loss_train: 0.9031 acc_train: 0.7714 loss_val: 1.1288 acc_val: 0.7167 time: 0.0344s\n",
            "Epoch: 0069 loss_train: 0.8509 acc_train: 0.8214 loss_val: 1.1181 acc_val: 0.7233 time: 0.0323s\n",
            "Epoch: 0070 loss_train: 0.8981 acc_train: 0.7643 loss_val: 1.1078 acc_val: 0.7300 time: 0.0395s\n",
            "Epoch: 0071 loss_train: 0.8864 acc_train: 0.7643 loss_val: 1.0979 acc_val: 0.7333 time: 0.0323s\n",
            "Epoch: 0072 loss_train: 0.8477 acc_train: 0.7857 loss_val: 1.0878 acc_val: 0.7400 time: 0.0325s\n",
            "Epoch: 0073 loss_train: 0.8598 acc_train: 0.8214 loss_val: 1.0780 acc_val: 0.7433 time: 0.0316s\n",
            "Epoch: 0074 loss_train: 0.7984 acc_train: 0.8143 loss_val: 1.0688 acc_val: 0.7433 time: 0.0316s\n",
            "Epoch: 0075 loss_train: 0.8013 acc_train: 0.8214 loss_val: 1.0602 acc_val: 0.7467 time: 0.0337s\n",
            "Epoch: 0076 loss_train: 0.8050 acc_train: 0.8143 loss_val: 1.0525 acc_val: 0.7567 time: 0.0402s\n",
            "Epoch: 0077 loss_train: 0.7992 acc_train: 0.8643 loss_val: 1.0442 acc_val: 0.7633 time: 0.0321s\n",
            "Epoch: 0078 loss_train: 0.7597 acc_train: 0.8500 loss_val: 1.0362 acc_val: 0.7667 time: 0.0369s\n",
            "Epoch: 0079 loss_train: 0.7618 acc_train: 0.8500 loss_val: 1.0274 acc_val: 0.7700 time: 0.0340s\n",
            "Epoch: 0080 loss_train: 0.7753 acc_train: 0.8071 loss_val: 1.0187 acc_val: 0.7700 time: 0.0418s\n",
            "Epoch: 0081 loss_train: 0.7446 acc_train: 0.8571 loss_val: 1.0105 acc_val: 0.7700 time: 0.0462s\n",
            "Epoch: 0082 loss_train: 0.7440 acc_train: 0.8357 loss_val: 1.0022 acc_val: 0.7733 time: 0.0362s\n",
            "Epoch: 0083 loss_train: 0.7708 acc_train: 0.8429 loss_val: 0.9942 acc_val: 0.7733 time: 0.0338s\n",
            "Epoch: 0084 loss_train: 0.7112 acc_train: 0.8571 loss_val: 0.9859 acc_val: 0.7767 time: 0.0343s\n",
            "Epoch: 0085 loss_train: 0.7572 acc_train: 0.8571 loss_val: 0.9786 acc_val: 0.7767 time: 0.0329s\n",
            "Epoch: 0086 loss_train: 0.7582 acc_train: 0.8357 loss_val: 0.9721 acc_val: 0.7767 time: 0.0332s\n",
            "Epoch: 0087 loss_train: 0.6637 acc_train: 0.8714 loss_val: 0.9662 acc_val: 0.7733 time: 0.0388s\n",
            "Epoch: 0088 loss_train: 0.7133 acc_train: 0.8571 loss_val: 0.9609 acc_val: 0.7733 time: 0.0340s\n",
            "Epoch: 0089 loss_train: 0.6558 acc_train: 0.8857 loss_val: 0.9564 acc_val: 0.7700 time: 0.0318s\n",
            "Epoch: 0090 loss_train: 0.6896 acc_train: 0.8500 loss_val: 0.9519 acc_val: 0.7700 time: 0.0322s\n",
            "Epoch: 0091 loss_train: 0.6538 acc_train: 0.8714 loss_val: 0.9477 acc_val: 0.7733 time: 0.0317s\n",
            "Epoch: 0092 loss_train: 0.6963 acc_train: 0.8714 loss_val: 0.9437 acc_val: 0.7767 time: 0.0323s\n",
            "Epoch: 0093 loss_train: 0.7353 acc_train: 0.8429 loss_val: 0.9391 acc_val: 0.7733 time: 0.0400s\n",
            "Epoch: 0094 loss_train: 0.6462 acc_train: 0.9071 loss_val: 0.9336 acc_val: 0.7767 time: 0.0316s\n",
            "Epoch: 0095 loss_train: 0.6448 acc_train: 0.8857 loss_val: 0.9276 acc_val: 0.7733 time: 0.0323s\n",
            "Epoch: 0096 loss_train: 0.6302 acc_train: 0.9071 loss_val: 0.9219 acc_val: 0.7733 time: 0.0326s\n",
            "Epoch: 0097 loss_train: 0.5888 acc_train: 0.8786 loss_val: 0.9167 acc_val: 0.7767 time: 0.0332s\n",
            "Epoch: 0098 loss_train: 0.6658 acc_train: 0.8714 loss_val: 0.9106 acc_val: 0.7800 time: 0.0322s\n",
            "Epoch: 0099 loss_train: 0.5976 acc_train: 0.8786 loss_val: 0.9050 acc_val: 0.7800 time: 0.0421s\n",
            "Epoch: 0100 loss_train: 0.5963 acc_train: 0.9143 loss_val: 0.8996 acc_val: 0.7800 time: 0.0350s\n",
            "Epoch: 0101 loss_train: 0.5846 acc_train: 0.9071 loss_val: 0.8939 acc_val: 0.7867 time: 0.0326s\n",
            "Epoch: 0102 loss_train: 0.5824 acc_train: 0.9000 loss_val: 0.8885 acc_val: 0.7900 time: 0.0346s\n",
            "Epoch: 0103 loss_train: 0.5590 acc_train: 0.9071 loss_val: 0.8834 acc_val: 0.7933 time: 0.0328s\n",
            "Epoch: 0104 loss_train: 0.6152 acc_train: 0.8714 loss_val: 0.8794 acc_val: 0.7933 time: 0.0368s\n",
            "Epoch: 0105 loss_train: 0.5394 acc_train: 0.9286 loss_val: 0.8760 acc_val: 0.8000 time: 0.0422s\n",
            "Epoch: 0106 loss_train: 0.5659 acc_train: 0.9071 loss_val: 0.8732 acc_val: 0.8000 time: 0.0334s\n",
            "Epoch: 0107 loss_train: 0.5837 acc_train: 0.8857 loss_val: 0.8702 acc_val: 0.7900 time: 0.0323s\n",
            "Epoch: 0108 loss_train: 0.5746 acc_train: 0.8929 loss_val: 0.8674 acc_val: 0.7933 time: 0.0332s\n",
            "Epoch: 0109 loss_train: 0.6029 acc_train: 0.8857 loss_val: 0.8646 acc_val: 0.7933 time: 0.0403s\n",
            "Epoch: 0110 loss_train: 0.5741 acc_train: 0.9000 loss_val: 0.8614 acc_val: 0.8000 time: 0.0351s\n",
            "Epoch: 0111 loss_train: 0.5054 acc_train: 0.9214 loss_val: 0.8585 acc_val: 0.8033 time: 0.0412s\n",
            "Epoch: 0112 loss_train: 0.5137 acc_train: 0.9571 loss_val: 0.8553 acc_val: 0.8033 time: 0.0320s\n",
            "Epoch: 0113 loss_train: 0.5481 acc_train: 0.8857 loss_val: 0.8523 acc_val: 0.8033 time: 0.0344s\n",
            "Epoch: 0114 loss_train: 0.5101 acc_train: 0.9286 loss_val: 0.8497 acc_val: 0.8000 time: 0.0343s\n",
            "Epoch: 0115 loss_train: 0.5378 acc_train: 0.9071 loss_val: 0.8469 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0116 loss_train: 0.4856 acc_train: 0.9000 loss_val: 0.8439 acc_val: 0.8000 time: 0.0317s\n",
            "Epoch: 0117 loss_train: 0.5468 acc_train: 0.8786 loss_val: 0.8413 acc_val: 0.8000 time: 0.0389s\n",
            "Epoch: 0118 loss_train: 0.5282 acc_train: 0.9071 loss_val: 0.8379 acc_val: 0.8033 time: 0.0320s\n",
            "Epoch: 0119 loss_train: 0.5424 acc_train: 0.9286 loss_val: 0.8342 acc_val: 0.8000 time: 0.0320s\n",
            "Epoch: 0120 loss_train: 0.5125 acc_train: 0.8786 loss_val: 0.8307 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0121 loss_train: 0.5154 acc_train: 0.9143 loss_val: 0.8279 acc_val: 0.8000 time: 0.0318s\n",
            "Epoch: 0122 loss_train: 0.4961 acc_train: 0.9071 loss_val: 0.8254 acc_val: 0.8033 time: 0.0321s\n",
            "Epoch: 0123 loss_train: 0.4830 acc_train: 0.9000 loss_val: 0.8225 acc_val: 0.8033 time: 0.0442s\n",
            "Epoch: 0124 loss_train: 0.5436 acc_train: 0.9000 loss_val: 0.8200 acc_val: 0.8067 time: 0.0322s\n",
            "Epoch: 0125 loss_train: 0.5053 acc_train: 0.9000 loss_val: 0.8175 acc_val: 0.8067 time: 0.0333s\n",
            "Epoch: 0126 loss_train: 0.4787 acc_train: 0.8929 loss_val: 0.8154 acc_val: 0.8100 time: 0.0335s\n",
            "Epoch: 0127 loss_train: 0.5221 acc_train: 0.9143 loss_val: 0.8134 acc_val: 0.8067 time: 0.0327s\n",
            "Epoch: 0128 loss_train: 0.5479 acc_train: 0.8857 loss_val: 0.8119 acc_val: 0.8067 time: 0.0324s\n",
            "Epoch: 0129 loss_train: 0.4300 acc_train: 0.9214 loss_val: 0.8100 acc_val: 0.8033 time: 0.0427s\n",
            "Epoch: 0130 loss_train: 0.4879 acc_train: 0.9214 loss_val: 0.8080 acc_val: 0.8000 time: 0.0367s\n",
            "Epoch: 0131 loss_train: 0.4583 acc_train: 0.9214 loss_val: 0.8058 acc_val: 0.8000 time: 0.0327s\n",
            "Epoch: 0132 loss_train: 0.4594 acc_train: 0.9429 loss_val: 0.8043 acc_val: 0.7933 time: 0.0342s\n",
            "Epoch: 0133 loss_train: 0.4551 acc_train: 0.9214 loss_val: 0.8027 acc_val: 0.7933 time: 0.0325s\n",
            "Epoch: 0134 loss_train: 0.4721 acc_train: 0.9000 loss_val: 0.8004 acc_val: 0.7967 time: 0.0364s\n",
            "Epoch: 0135 loss_train: 0.5137 acc_train: 0.9071 loss_val: 0.7986 acc_val: 0.7967 time: 0.0419s\n",
            "Epoch: 0136 loss_train: 0.4947 acc_train: 0.9143 loss_val: 0.7966 acc_val: 0.7967 time: 0.0386s\n",
            "Epoch: 0137 loss_train: 0.4523 acc_train: 0.9286 loss_val: 0.7947 acc_val: 0.7967 time: 0.0397s\n",
            "Epoch: 0138 loss_train: 0.4299 acc_train: 0.9571 loss_val: 0.7929 acc_val: 0.7933 time: 0.0327s\n",
            "Epoch: 0139 loss_train: 0.4843 acc_train: 0.9357 loss_val: 0.7907 acc_val: 0.7967 time: 0.0320s\n",
            "Epoch: 0140 loss_train: 0.4607 acc_train: 0.9000 loss_val: 0.7894 acc_val: 0.7933 time: 0.0334s\n",
            "Epoch: 0141 loss_train: 0.5034 acc_train: 0.8786 loss_val: 0.7896 acc_val: 0.7933 time: 0.0401s\n",
            "Epoch: 0142 loss_train: 0.4860 acc_train: 0.9143 loss_val: 0.7899 acc_val: 0.7933 time: 0.0344s\n",
            "Epoch: 0143 loss_train: 0.4517 acc_train: 0.9357 loss_val: 0.7889 acc_val: 0.7933 time: 0.0339s\n",
            "Epoch: 0144 loss_train: 0.5009 acc_train: 0.9000 loss_val: 0.7875 acc_val: 0.7933 time: 0.0320s\n",
            "Epoch: 0145 loss_train: 0.4618 acc_train: 0.9286 loss_val: 0.7858 acc_val: 0.7967 time: 0.0340s\n",
            "Epoch: 0146 loss_train: 0.4320 acc_train: 0.9357 loss_val: 0.7834 acc_val: 0.8000 time: 0.0361s\n",
            "Epoch: 0147 loss_train: 0.4220 acc_train: 0.9357 loss_val: 0.7810 acc_val: 0.7933 time: 0.0448s\n",
            "Epoch: 0148 loss_train: 0.5091 acc_train: 0.9071 loss_val: 0.7791 acc_val: 0.7933 time: 0.0349s\n",
            "Epoch: 0149 loss_train: 0.4195 acc_train: 0.9143 loss_val: 0.7772 acc_val: 0.8000 time: 0.0344s\n",
            "Epoch: 0150 loss_train: 0.4378 acc_train: 0.9143 loss_val: 0.7753 acc_val: 0.7967 time: 0.0345s\n",
            "Epoch: 0151 loss_train: 0.4658 acc_train: 0.9214 loss_val: 0.7732 acc_val: 0.8033 time: 0.0254s\n",
            "Epoch: 0152 loss_train: 0.4099 acc_train: 0.9286 loss_val: 0.7720 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0153 loss_train: 0.4262 acc_train: 0.9286 loss_val: 0.7719 acc_val: 0.8000 time: 0.0265s\n",
            "Epoch: 0154 loss_train: 0.4037 acc_train: 0.9429 loss_val: 0.7730 acc_val: 0.7900 time: 0.0323s\n",
            "Epoch: 0155 loss_train: 0.4121 acc_train: 0.9357 loss_val: 0.7747 acc_val: 0.7933 time: 0.0254s\n",
            "Epoch: 0156 loss_train: 0.4074 acc_train: 0.9357 loss_val: 0.7761 acc_val: 0.7900 time: 0.0277s\n",
            "Epoch: 0157 loss_train: 0.4215 acc_train: 0.9571 loss_val: 0.7772 acc_val: 0.7900 time: 0.0282s\n",
            "Epoch: 0158 loss_train: 0.3831 acc_train: 0.9429 loss_val: 0.7783 acc_val: 0.7933 time: 0.0258s\n",
            "Epoch: 0159 loss_train: 0.4411 acc_train: 0.9214 loss_val: 0.7787 acc_val: 0.7933 time: 0.0279s\n",
            "Epoch: 0160 loss_train: 0.4182 acc_train: 0.9500 loss_val: 0.7774 acc_val: 0.7967 time: 0.0302s\n",
            "Epoch: 0161 loss_train: 0.3971 acc_train: 0.9357 loss_val: 0.7753 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0162 loss_train: 0.4325 acc_train: 0.9214 loss_val: 0.7738 acc_val: 0.8000 time: 0.0254s\n",
            "Epoch: 0163 loss_train: 0.3899 acc_train: 0.9357 loss_val: 0.7725 acc_val: 0.8000 time: 0.0284s\n",
            "Epoch: 0164 loss_train: 0.3934 acc_train: 0.9214 loss_val: 0.7716 acc_val: 0.8000 time: 0.0258s\n",
            "Epoch: 0165 loss_train: 0.4085 acc_train: 0.9143 loss_val: 0.7695 acc_val: 0.8033 time: 0.0254s\n",
            "Epoch: 0166 loss_train: 0.4189 acc_train: 0.9214 loss_val: 0.7669 acc_val: 0.8033 time: 0.0254s\n",
            "Epoch: 0167 loss_train: 0.3798 acc_train: 0.9500 loss_val: 0.7653 acc_val: 0.8000 time: 0.0326s\n",
            "Epoch: 0168 loss_train: 0.3908 acc_train: 0.9500 loss_val: 0.7632 acc_val: 0.7967 time: 0.0302s\n",
            "Epoch: 0169 loss_train: 0.4098 acc_train: 0.9357 loss_val: 0.7615 acc_val: 0.7967 time: 0.0250s\n",
            "Epoch: 0170 loss_train: 0.4317 acc_train: 0.9071 loss_val: 0.7605 acc_val: 0.7933 time: 0.0253s\n",
            "Epoch: 0171 loss_train: 0.4100 acc_train: 0.9357 loss_val: 0.7588 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0172 loss_train: 0.4254 acc_train: 0.9214 loss_val: 0.7577 acc_val: 0.7900 time: 0.0258s\n",
            "Epoch: 0173 loss_train: 0.4272 acc_train: 0.9500 loss_val: 0.7565 acc_val: 0.7933 time: 0.0267s\n",
            "Epoch: 0174 loss_train: 0.3883 acc_train: 0.9643 loss_val: 0.7549 acc_val: 0.7933 time: 0.0289s\n",
            "Epoch: 0175 loss_train: 0.4024 acc_train: 0.9214 loss_val: 0.7538 acc_val: 0.7900 time: 0.0295s\n",
            "Epoch: 0176 loss_train: 0.3899 acc_train: 0.9143 loss_val: 0.7525 acc_val: 0.7900 time: 0.0289s\n",
            "Epoch: 0177 loss_train: 0.3597 acc_train: 0.9429 loss_val: 0.7517 acc_val: 0.7900 time: 0.0344s\n",
            "Epoch: 0178 loss_train: 0.4173 acc_train: 0.9286 loss_val: 0.7510 acc_val: 0.8000 time: 0.0259s\n",
            "Epoch: 0179 loss_train: 0.3894 acc_train: 0.9357 loss_val: 0.7504 acc_val: 0.8000 time: 0.0256s\n",
            "Epoch: 0180 loss_train: 0.3975 acc_train: 0.9214 loss_val: 0.7505 acc_val: 0.8000 time: 0.0275s\n",
            "Epoch: 0181 loss_train: 0.3496 acc_train: 0.9571 loss_val: 0.7501 acc_val: 0.7967 time: 0.0284s\n",
            "Epoch: 0182 loss_train: 0.3732 acc_train: 0.9571 loss_val: 0.7494 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0183 loss_train: 0.3990 acc_train: 0.9357 loss_val: 0.7482 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0184 loss_train: 0.3901 acc_train: 0.9357 loss_val: 0.7472 acc_val: 0.7967 time: 0.0260s\n",
            "Epoch: 0185 loss_train: 0.3474 acc_train: 0.9571 loss_val: 0.7457 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0186 loss_train: 0.4082 acc_train: 0.9214 loss_val: 0.7452 acc_val: 0.7967 time: 0.0265s\n",
            "Epoch: 0187 loss_train: 0.3547 acc_train: 0.9429 loss_val: 0.7444 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0188 loss_train: 0.3545 acc_train: 0.9571 loss_val: 0.7429 acc_val: 0.7867 time: 0.0269s\n",
            "Epoch: 0189 loss_train: 0.3679 acc_train: 0.9286 loss_val: 0.7429 acc_val: 0.7833 time: 0.0308s\n",
            "Epoch: 0190 loss_train: 0.3405 acc_train: 0.9429 loss_val: 0.7432 acc_val: 0.7833 time: 0.0300s\n",
            "Epoch: 0191 loss_train: 0.3591 acc_train: 0.9429 loss_val: 0.7440 acc_val: 0.7833 time: 0.0296s\n",
            "Epoch: 0192 loss_train: 0.3833 acc_train: 0.9429 loss_val: 0.7444 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0193 loss_train: 0.3400 acc_train: 0.9500 loss_val: 0.7443 acc_val: 0.7900 time: 0.0265s\n",
            "Epoch: 0194 loss_train: 0.3994 acc_train: 0.9357 loss_val: 0.7444 acc_val: 0.7933 time: 0.0279s\n",
            "Epoch: 0195 loss_train: 0.3801 acc_train: 0.9286 loss_val: 0.7449 acc_val: 0.8000 time: 0.0295s\n",
            "Epoch: 0196 loss_train: 0.3510 acc_train: 0.9571 loss_val: 0.7448 acc_val: 0.8000 time: 0.0264s\n",
            "Epoch: 0197 loss_train: 0.3250 acc_train: 0.9500 loss_val: 0.7447 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0198 loss_train: 0.3556 acc_train: 0.9143 loss_val: 0.7454 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0199 loss_train: 0.3919 acc_train: 0.9500 loss_val: 0.7458 acc_val: 0.7967 time: 0.0291s\n",
            "Epoch: 0200 loss_train: 0.4270 acc_train: 0.9286 loss_val: 0.7437 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0201 loss_train: 0.3932 acc_train: 0.9357 loss_val: 0.7413 acc_val: 0.7967 time: 0.0297s\n",
            "Epoch: 0202 loss_train: 0.3819 acc_train: 0.9500 loss_val: 0.7398 acc_val: 0.7967 time: 0.0346s\n",
            "Epoch: 0203 loss_train: 0.3725 acc_train: 0.9571 loss_val: 0.7393 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0204 loss_train: 0.3834 acc_train: 0.9357 loss_val: 0.7396 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0205 loss_train: 0.3358 acc_train: 0.9500 loss_val: 0.7399 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0206 loss_train: 0.4079 acc_train: 0.9071 loss_val: 0.7402 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0207 loss_train: 0.3969 acc_train: 0.9286 loss_val: 0.7403 acc_val: 0.7933 time: 0.0259s\n",
            "Epoch: 0208 loss_train: 0.3835 acc_train: 0.9429 loss_val: 0.7415 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0209 loss_train: 0.3761 acc_train: 0.9071 loss_val: 0.7438 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0210 loss_train: 0.3612 acc_train: 0.9429 loss_val: 0.7452 acc_val: 0.7967 time: 0.0288s\n",
            "Epoch: 0211 loss_train: 0.4031 acc_train: 0.9286 loss_val: 0.7445 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0212 loss_train: 0.3572 acc_train: 0.9643 loss_val: 0.7417 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0213 loss_train: 0.3414 acc_train: 0.9500 loss_val: 0.7391 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0214 loss_train: 0.3691 acc_train: 0.9214 loss_val: 0.7363 acc_val: 0.8000 time: 0.0275s\n",
            "Epoch: 0215 loss_train: 0.3110 acc_train: 0.9643 loss_val: 0.7346 acc_val: 0.7933 time: 0.0256s\n",
            "Epoch: 0216 loss_train: 0.3466 acc_train: 0.9357 loss_val: 0.7341 acc_val: 0.7867 time: 0.0301s\n",
            "Epoch: 0217 loss_train: 0.3714 acc_train: 0.9500 loss_val: 0.7343 acc_val: 0.7867 time: 0.0279s\n",
            "Epoch: 0218 loss_train: 0.3722 acc_train: 0.9500 loss_val: 0.7348 acc_val: 0.7900 time: 0.0255s\n",
            "Epoch: 0219 loss_train: 0.3868 acc_train: 0.9214 loss_val: 0.7347 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0220 loss_train: 0.3457 acc_train: 0.9357 loss_val: 0.7350 acc_val: 0.7867 time: 0.0259s\n",
            "Epoch: 0221 loss_train: 0.3602 acc_train: 0.9500 loss_val: 0.7358 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0222 loss_train: 0.3668 acc_train: 0.9286 loss_val: 0.7357 acc_val: 0.7933 time: 0.0244s\n",
            "Epoch: 0223 loss_train: 0.3856 acc_train: 0.9286 loss_val: 0.7354 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0224 loss_train: 0.3260 acc_train: 0.9357 loss_val: 0.7345 acc_val: 0.7967 time: 0.0278s\n",
            "Epoch: 0225 loss_train: 0.3295 acc_train: 0.9429 loss_val: 0.7331 acc_val: 0.7967 time: 0.0309s\n",
            "Epoch: 0226 loss_train: 0.3649 acc_train: 0.9357 loss_val: 0.7311 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0227 loss_train: 0.3607 acc_train: 0.9357 loss_val: 0.7299 acc_val: 0.7967 time: 0.0269s\n",
            "Epoch: 0228 loss_train: 0.3329 acc_train: 0.9786 loss_val: 0.7288 acc_val: 0.7967 time: 0.0290s\n",
            "Epoch: 0229 loss_train: 0.2955 acc_train: 0.9429 loss_val: 0.7284 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0230 loss_train: 0.3349 acc_train: 0.9643 loss_val: 0.7288 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0231 loss_train: 0.3116 acc_train: 0.9357 loss_val: 0.7291 acc_val: 0.7967 time: 0.0258s\n",
            "Epoch: 0232 loss_train: 0.3411 acc_train: 0.9357 loss_val: 0.7300 acc_val: 0.7967 time: 0.0260s\n",
            "Epoch: 0233 loss_train: 0.3421 acc_train: 0.9429 loss_val: 0.7305 acc_val: 0.8000 time: 0.0290s\n",
            "Epoch: 0234 loss_train: 0.3258 acc_train: 0.9714 loss_val: 0.7301 acc_val: 0.8000 time: 0.0251s\n",
            "Epoch: 0235 loss_train: 0.3393 acc_train: 0.9357 loss_val: 0.7293 acc_val: 0.8000 time: 0.0254s\n",
            "Epoch: 0236 loss_train: 0.3385 acc_train: 0.9286 loss_val: 0.7285 acc_val: 0.8000 time: 0.0268s\n",
            "Epoch: 0237 loss_train: 0.3384 acc_train: 0.9571 loss_val: 0.7281 acc_val: 0.7967 time: 0.0318s\n",
            "Epoch: 0238 loss_train: 0.3533 acc_train: 0.9643 loss_val: 0.7278 acc_val: 0.7833 time: 0.0304s\n",
            "Epoch: 0239 loss_train: 0.3007 acc_train: 0.9643 loss_val: 0.7284 acc_val: 0.7900 time: 0.0291s\n",
            "Epoch: 0240 loss_train: 0.3930 acc_train: 0.9143 loss_val: 0.7290 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0241 loss_train: 0.3139 acc_train: 0.9500 loss_val: 0.7295 acc_val: 0.7933 time: 0.0252s\n",
            "Epoch: 0242 loss_train: 0.3329 acc_train: 0.9357 loss_val: 0.7295 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0243 loss_train: 0.3085 acc_train: 0.9357 loss_val: 0.7291 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0244 loss_train: 0.2952 acc_train: 0.9714 loss_val: 0.7288 acc_val: 0.7967 time: 0.0260s\n",
            "Epoch: 0245 loss_train: 0.3589 acc_train: 0.9571 loss_val: 0.7269 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0246 loss_train: 0.3286 acc_train: 0.9429 loss_val: 0.7256 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0247 loss_train: 0.3014 acc_train: 0.9500 loss_val: 0.7247 acc_val: 0.7967 time: 0.0396s\n",
            "Epoch: 0248 loss_train: 0.3427 acc_train: 0.9286 loss_val: 0.7251 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0249 loss_train: 0.3061 acc_train: 0.9500 loss_val: 0.7252 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0250 loss_train: 0.3017 acc_train: 0.9643 loss_val: 0.7257 acc_val: 0.7967 time: 0.0251s\n",
            "Epoch: 0251 loss_train: 0.3245 acc_train: 0.9571 loss_val: 0.7259 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0252 loss_train: 0.3221 acc_train: 0.9286 loss_val: 0.7246 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0253 loss_train: 0.3440 acc_train: 0.9357 loss_val: 0.7232 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0254 loss_train: 0.3028 acc_train: 0.9571 loss_val: 0.7226 acc_val: 0.7900 time: 0.0344s\n",
            "Epoch: 0255 loss_train: 0.3267 acc_train: 0.9643 loss_val: 0.7220 acc_val: 0.7933 time: 0.0285s\n",
            "Epoch: 0256 loss_train: 0.3022 acc_train: 0.9429 loss_val: 0.7214 acc_val: 0.7967 time: 0.0285s\n",
            "Epoch: 0257 loss_train: 0.3191 acc_train: 0.9429 loss_val: 0.7207 acc_val: 0.7933 time: 0.0287s\n",
            "Epoch: 0258 loss_train: 0.2706 acc_train: 0.9643 loss_val: 0.7201 acc_val: 0.7933 time: 0.0277s\n",
            "Epoch: 0259 loss_train: 0.4007 acc_train: 0.9071 loss_val: 0.7197 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0260 loss_train: 0.3397 acc_train: 0.9214 loss_val: 0.7197 acc_val: 0.7867 time: 0.0254s\n",
            "Epoch: 0261 loss_train: 0.2835 acc_train: 0.9714 loss_val: 0.7202 acc_val: 0.7867 time: 0.0301s\n",
            "Epoch: 0262 loss_train: 0.3004 acc_train: 0.9500 loss_val: 0.7211 acc_val: 0.7933 time: 0.0285s\n",
            "Epoch: 0263 loss_train: 0.2708 acc_train: 0.9714 loss_val: 0.7222 acc_val: 0.7967 time: 0.0309s\n",
            "Epoch: 0264 loss_train: 0.2861 acc_train: 0.9643 loss_val: 0.7222 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0265 loss_train: 0.2914 acc_train: 0.9429 loss_val: 0.7226 acc_val: 0.8000 time: 0.0258s\n",
            "Epoch: 0266 loss_train: 0.3075 acc_train: 0.9571 loss_val: 0.7218 acc_val: 0.8000 time: 0.0252s\n",
            "Epoch: 0267 loss_train: 0.3670 acc_train: 0.9214 loss_val: 0.7208 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0268 loss_train: 0.2941 acc_train: 0.9643 loss_val: 0.7200 acc_val: 0.8000 time: 0.0270s\n",
            "Epoch: 0269 loss_train: 0.2866 acc_train: 0.9786 loss_val: 0.7196 acc_val: 0.8000 time: 0.0282s\n",
            "Epoch: 0270 loss_train: 0.2828 acc_train: 0.9500 loss_val: 0.7195 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0271 loss_train: 0.3543 acc_train: 0.9429 loss_val: 0.7196 acc_val: 0.7967 time: 0.0329s\n",
            "Epoch: 0272 loss_train: 0.3091 acc_train: 0.9357 loss_val: 0.7201 acc_val: 0.7967 time: 0.0268s\n",
            "Epoch: 0273 loss_train: 0.3012 acc_train: 0.9500 loss_val: 0.7214 acc_val: 0.7933 time: 0.0287s\n",
            "Epoch: 0274 loss_train: 0.2345 acc_train: 0.9571 loss_val: 0.7216 acc_val: 0.7933 time: 0.0258s\n",
            "Epoch: 0275 loss_train: 0.2872 acc_train: 0.9571 loss_val: 0.7205 acc_val: 0.7933 time: 0.0293s\n",
            "Epoch: 0276 loss_train: 0.3016 acc_train: 0.9500 loss_val: 0.7209 acc_val: 0.7933 time: 0.0266s\n",
            "Epoch: 0277 loss_train: 0.3828 acc_train: 0.9143 loss_val: 0.7208 acc_val: 0.8000 time: 0.0269s\n",
            "Epoch: 0278 loss_train: 0.2498 acc_train: 0.9643 loss_val: 0.7197 acc_val: 0.8000 time: 0.0264s\n",
            "Epoch: 0279 loss_train: 0.2886 acc_train: 0.9500 loss_val: 0.7183 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0280 loss_train: 0.2960 acc_train: 0.9571 loss_val: 0.7171 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0281 loss_train: 0.2780 acc_train: 0.9714 loss_val: 0.7164 acc_val: 0.7933 time: 0.0256s\n",
            "Epoch: 0282 loss_train: 0.3288 acc_train: 0.9357 loss_val: 0.7168 acc_val: 0.7900 time: 0.0264s\n",
            "Epoch: 0283 loss_train: 0.2745 acc_train: 0.9571 loss_val: 0.7179 acc_val: 0.8000 time: 0.0305s\n",
            "Epoch: 0284 loss_train: 0.2751 acc_train: 0.9786 loss_val: 0.7181 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0285 loss_train: 0.2941 acc_train: 0.9500 loss_val: 0.7197 acc_val: 0.7967 time: 0.0258s\n",
            "Epoch: 0286 loss_train: 0.3122 acc_train: 0.9357 loss_val: 0.7212 acc_val: 0.7933 time: 0.0289s\n",
            "Epoch: 0287 loss_train: 0.2600 acc_train: 0.9571 loss_val: 0.7220 acc_val: 0.7933 time: 0.0262s\n",
            "Epoch: 0288 loss_train: 0.2974 acc_train: 0.9429 loss_val: 0.7218 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0289 loss_train: 0.2734 acc_train: 0.9429 loss_val: 0.7203 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0290 loss_train: 0.3086 acc_train: 0.9071 loss_val: 0.7192 acc_val: 0.7933 time: 0.0273s\n",
            "Epoch: 0291 loss_train: 0.2810 acc_train: 0.9786 loss_val: 0.7184 acc_val: 0.7867 time: 0.0335s\n",
            "Epoch: 0292 loss_train: 0.3077 acc_train: 0.9286 loss_val: 0.7188 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0293 loss_train: 0.2591 acc_train: 0.9786 loss_val: 0.7186 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0294 loss_train: 0.3137 acc_train: 0.9429 loss_val: 0.7187 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0295 loss_train: 0.3270 acc_train: 0.9429 loss_val: 0.7197 acc_val: 0.7900 time: 0.0269s\n",
            "Epoch: 0296 loss_train: 0.2983 acc_train: 0.9571 loss_val: 0.7207 acc_val: 0.7967 time: 0.0275s\n",
            "Epoch: 0297 loss_train: 0.3141 acc_train: 0.9429 loss_val: 0.7216 acc_val: 0.7967 time: 0.0279s\n",
            "Epoch: 0298 loss_train: 0.2600 acc_train: 0.9786 loss_val: 0.7229 acc_val: 0.7967 time: 0.0300s\n",
            "Epoch: 0299 loss_train: 0.2752 acc_train: 0.9786 loss_val: 0.7236 acc_val: 0.7967 time: 0.0275s\n",
            "Epoch: 0300 loss_train: 0.2775 acc_train: 0.9286 loss_val: 0.7242 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0301 loss_train: 0.3195 acc_train: 0.9500 loss_val: 0.7223 acc_val: 0.7933 time: 0.0250s\n",
            "Epoch: 0302 loss_train: 0.2837 acc_train: 0.9643 loss_val: 0.7201 acc_val: 0.7933 time: 0.0254s\n",
            "Epoch: 0303 loss_train: 0.2864 acc_train: 0.9500 loss_val: 0.7184 acc_val: 0.7933 time: 0.0248s\n",
            "Epoch: 0304 loss_train: 0.2820 acc_train: 0.9786 loss_val: 0.7172 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0305 loss_train: 0.2376 acc_train: 0.9786 loss_val: 0.7163 acc_val: 0.7867 time: 0.0333s\n",
            "Epoch: 0306 loss_train: 0.2791 acc_train: 0.9857 loss_val: 0.7155 acc_val: 0.7867 time: 0.0302s\n",
            "Epoch: 0307 loss_train: 0.3322 acc_train: 0.9143 loss_val: 0.7152 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0308 loss_train: 0.3053 acc_train: 0.9643 loss_val: 0.7140 acc_val: 0.7867 time: 0.0283s\n",
            "Epoch: 0309 loss_train: 0.3485 acc_train: 0.9357 loss_val: 0.7133 acc_val: 0.7900 time: 0.0252s\n",
            "Epoch: 0310 loss_train: 0.2772 acc_train: 0.9429 loss_val: 0.7132 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0311 loss_train: 0.2702 acc_train: 0.9500 loss_val: 0.7135 acc_val: 0.8000 time: 0.0253s\n",
            "Epoch: 0312 loss_train: 0.2889 acc_train: 0.9500 loss_val: 0.7137 acc_val: 0.7967 time: 0.0273s\n",
            "Epoch: 0313 loss_train: 0.2983 acc_train: 0.9429 loss_val: 0.7137 acc_val: 0.7967 time: 0.0271s\n",
            "Epoch: 0314 loss_train: 0.2686 acc_train: 0.9500 loss_val: 0.7115 acc_val: 0.7967 time: 0.0258s\n",
            "Epoch: 0315 loss_train: 0.3154 acc_train: 0.9429 loss_val: 0.7092 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0316 loss_train: 0.2708 acc_train: 0.9571 loss_val: 0.7085 acc_val: 0.8000 time: 0.0267s\n",
            "Epoch: 0317 loss_train: 0.2731 acc_train: 0.9500 loss_val: 0.7088 acc_val: 0.8000 time: 0.0279s\n",
            "Epoch: 0318 loss_train: 0.2635 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.8000 time: 0.0315s\n",
            "Epoch: 0319 loss_train: 0.2809 acc_train: 0.9500 loss_val: 0.7125 acc_val: 0.8000 time: 0.0274s\n",
            "Epoch: 0320 loss_train: 0.2812 acc_train: 0.9571 loss_val: 0.7149 acc_val: 0.8000 time: 0.0261s\n",
            "Epoch: 0321 loss_train: 0.2786 acc_train: 0.9500 loss_val: 0.7158 acc_val: 0.8000 time: 0.0262s\n",
            "Epoch: 0322 loss_train: 0.2563 acc_train: 0.9786 loss_val: 0.7167 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0323 loss_train: 0.2562 acc_train: 0.9643 loss_val: 0.7168 acc_val: 0.8000 time: 0.0278s\n",
            "Epoch: 0324 loss_train: 0.2723 acc_train: 0.9500 loss_val: 0.7170 acc_val: 0.8000 time: 0.0262s\n",
            "Epoch: 0325 loss_train: 0.3114 acc_train: 0.9429 loss_val: 0.7166 acc_val: 0.7967 time: 0.0297s\n",
            "Epoch: 0326 loss_train: 0.2787 acc_train: 0.9429 loss_val: 0.7169 acc_val: 0.7967 time: 0.0307s\n",
            "Epoch: 0327 loss_train: 0.2558 acc_train: 0.9714 loss_val: 0.7174 acc_val: 0.7967 time: 0.0276s\n",
            "Epoch: 0328 loss_train: 0.3262 acc_train: 0.9357 loss_val: 0.7187 acc_val: 0.7967 time: 0.0275s\n",
            "Epoch: 0329 loss_train: 0.2771 acc_train: 0.9286 loss_val: 0.7196 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0330 loss_train: 0.2638 acc_train: 0.9643 loss_val: 0.7212 acc_val: 0.7967 time: 0.0270s\n",
            "Epoch: 0331 loss_train: 0.2749 acc_train: 0.9429 loss_val: 0.7223 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0332 loss_train: 0.2914 acc_train: 0.9500 loss_val: 0.7226 acc_val: 0.7967 time: 0.0281s\n",
            "Epoch: 0333 loss_train: 0.2581 acc_train: 0.9500 loss_val: 0.7224 acc_val: 0.7967 time: 0.0291s\n",
            "Epoch: 0334 loss_train: 0.2702 acc_train: 0.9429 loss_val: 0.7218 acc_val: 0.7933 time: 0.0271s\n",
            "Epoch: 0335 loss_train: 0.2989 acc_train: 0.9571 loss_val: 0.7199 acc_val: 0.7933 time: 0.0251s\n",
            "Epoch: 0336 loss_train: 0.3251 acc_train: 0.9500 loss_val: 0.7179 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0337 loss_train: 0.2844 acc_train: 0.9643 loss_val: 0.7164 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0338 loss_train: 0.2705 acc_train: 0.9571 loss_val: 0.7161 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0339 loss_train: 0.2915 acc_train: 0.9429 loss_val: 0.7170 acc_val: 0.7933 time: 0.0258s\n",
            "Epoch: 0340 loss_train: 0.2592 acc_train: 0.9643 loss_val: 0.7180 acc_val: 0.7933 time: 0.0345s\n",
            "Epoch: 0341 loss_train: 0.1979 acc_train: 0.9857 loss_val: 0.7188 acc_val: 0.7933 time: 0.0284s\n",
            "Epoch: 0342 loss_train: 0.2803 acc_train: 0.9500 loss_val: 0.7208 acc_val: 0.7933 time: 0.0257s\n",
            "Epoch: 0343 loss_train: 0.2854 acc_train: 0.9286 loss_val: 0.7221 acc_val: 0.7933 time: 0.0275s\n",
            "Epoch: 0344 loss_train: 0.2116 acc_train: 0.9643 loss_val: 0.7240 acc_val: 0.7933 time: 0.0253s\n",
            "Epoch: 0345 loss_train: 0.2882 acc_train: 0.9214 loss_val: 0.7243 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0346 loss_train: 0.2987 acc_train: 0.9500 loss_val: 0.7230 acc_val: 0.7933 time: 0.0273s\n",
            "Epoch: 0347 loss_train: 0.2792 acc_train: 0.9571 loss_val: 0.7211 acc_val: 0.7933 time: 0.0304s\n",
            "Epoch: 0348 loss_train: 0.2679 acc_train: 0.9857 loss_val: 0.7177 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0349 loss_train: 0.2865 acc_train: 0.9500 loss_val: 0.7146 acc_val: 0.7933 time: 0.0259s\n",
            "Epoch: 0350 loss_train: 0.3030 acc_train: 0.9286 loss_val: 0.7124 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0351 loss_train: 0.2532 acc_train: 0.9571 loss_val: 0.7107 acc_val: 0.7933 time: 0.0263s\n",
            "Epoch: 0352 loss_train: 0.2376 acc_train: 0.9643 loss_val: 0.7099 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0353 loss_train: 0.2458 acc_train: 0.9286 loss_val: 0.7101 acc_val: 0.7933 time: 0.0246s\n",
            "Epoch: 0354 loss_train: 0.2365 acc_train: 0.9714 loss_val: 0.7111 acc_val: 0.7967 time: 0.0266s\n",
            "Epoch: 0355 loss_train: 0.2232 acc_train: 0.9857 loss_val: 0.7116 acc_val: 0.7967 time: 0.0300s\n",
            "Epoch: 0356 loss_train: 0.3037 acc_train: 0.9571 loss_val: 0.7118 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0357 loss_train: 0.3032 acc_train: 0.9500 loss_val: 0.7116 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0358 loss_train: 0.2748 acc_train: 0.9500 loss_val: 0.7127 acc_val: 0.7933 time: 0.0286s\n",
            "Epoch: 0359 loss_train: 0.2627 acc_train: 0.9643 loss_val: 0.7125 acc_val: 0.7933 time: 0.0336s\n",
            "Epoch: 0360 loss_train: 0.2482 acc_train: 0.9571 loss_val: 0.7118 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0361 loss_train: 0.2905 acc_train: 0.9500 loss_val: 0.7112 acc_val: 0.7967 time: 0.0252s\n",
            "Epoch: 0362 loss_train: 0.3333 acc_train: 0.9643 loss_val: 0.7092 acc_val: 0.7967 time: 0.0300s\n",
            "Epoch: 0363 loss_train: 0.2276 acc_train: 0.9571 loss_val: 0.7080 acc_val: 0.7967 time: 0.0315s\n",
            "Epoch: 0364 loss_train: 0.2519 acc_train: 0.9643 loss_val: 0.7079 acc_val: 0.7967 time: 0.0266s\n",
            "Epoch: 0365 loss_train: 0.2819 acc_train: 0.9500 loss_val: 0.7087 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0366 loss_train: 0.2709 acc_train: 0.9714 loss_val: 0.7090 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0367 loss_train: 0.2328 acc_train: 0.9643 loss_val: 0.7094 acc_val: 0.8000 time: 0.0251s\n",
            "Epoch: 0368 loss_train: 0.3018 acc_train: 0.9357 loss_val: 0.7089 acc_val: 0.8000 time: 0.0277s\n",
            "Epoch: 0369 loss_train: 0.2516 acc_train: 0.9571 loss_val: 0.7075 acc_val: 0.7967 time: 0.0293s\n",
            "Epoch: 0370 loss_train: 0.2469 acc_train: 0.9571 loss_val: 0.7076 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0371 loss_train: 0.2482 acc_train: 0.9643 loss_val: 0.7073 acc_val: 0.7933 time: 0.0249s\n",
            "Epoch: 0372 loss_train: 0.2341 acc_train: 0.9786 loss_val: 0.7082 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0373 loss_train: 0.2460 acc_train: 0.9571 loss_val: 0.7101 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0374 loss_train: 0.2617 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.7967 time: 0.0252s\n",
            "Epoch: 0375 loss_train: 0.2407 acc_train: 0.9786 loss_val: 0.7110 acc_val: 0.7967 time: 0.0345s\n",
            "Epoch: 0376 loss_train: 0.3191 acc_train: 0.9571 loss_val: 0.7104 acc_val: 0.7933 time: 0.0288s\n",
            "Epoch: 0377 loss_train: 0.2757 acc_train: 0.9571 loss_val: 0.7090 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0378 loss_train: 0.2463 acc_train: 0.9500 loss_val: 0.7083 acc_val: 0.7967 time: 0.0266s\n",
            "Epoch: 0379 loss_train: 0.2956 acc_train: 0.9500 loss_val: 0.7091 acc_val: 0.7967 time: 0.0313s\n",
            "Epoch: 0380 loss_train: 0.2198 acc_train: 0.9714 loss_val: 0.7102 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0381 loss_train: 0.2397 acc_train: 0.9571 loss_val: 0.7116 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0382 loss_train: 0.2431 acc_train: 0.9714 loss_val: 0.7120 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0383 loss_train: 0.3025 acc_train: 0.9286 loss_val: 0.7125 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0384 loss_train: 0.2623 acc_train: 0.9643 loss_val: 0.7126 acc_val: 0.7967 time: 0.0289s\n",
            "Epoch: 0385 loss_train: 0.2579 acc_train: 0.9571 loss_val: 0.7138 acc_val: 0.7933 time: 0.0252s\n",
            "Epoch: 0386 loss_train: 0.2359 acc_train: 0.9714 loss_val: 0.7149 acc_val: 0.7900 time: 0.0256s\n",
            "Epoch: 0387 loss_train: 0.2332 acc_train: 0.9786 loss_val: 0.7159 acc_val: 0.8000 time: 0.0262s\n",
            "Epoch: 0388 loss_train: 0.2643 acc_train: 0.9571 loss_val: 0.7168 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0389 loss_train: 0.2361 acc_train: 0.9500 loss_val: 0.7176 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0390 loss_train: 0.2042 acc_train: 0.9786 loss_val: 0.7197 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0391 loss_train: 0.2522 acc_train: 0.9500 loss_val: 0.7204 acc_val: 0.7900 time: 0.0282s\n",
            "Epoch: 0392 loss_train: 0.2551 acc_train: 0.9714 loss_val: 0.7206 acc_val: 0.7900 time: 0.0332s\n",
            "Epoch: 0393 loss_train: 0.2643 acc_train: 0.9500 loss_val: 0.7207 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0394 loss_train: 0.2456 acc_train: 0.9571 loss_val: 0.7213 acc_val: 0.7900 time: 0.0247s\n",
            "Epoch: 0395 loss_train: 0.2996 acc_train: 0.9500 loss_val: 0.7196 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0396 loss_train: 0.2293 acc_train: 0.9714 loss_val: 0.7182 acc_val: 0.7900 time: 0.0262s\n",
            "Epoch: 0397 loss_train: 0.2594 acc_train: 0.9500 loss_val: 0.7151 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0398 loss_train: 0.3057 acc_train: 0.9500 loss_val: 0.7121 acc_val: 0.7933 time: 0.0270s\n",
            "Epoch: 0399 loss_train: 0.2106 acc_train: 0.9857 loss_val: 0.7090 acc_val: 0.7967 time: 0.0280s\n",
            "Epoch: 0400 loss_train: 0.2344 acc_train: 0.9571 loss_val: 0.7070 acc_val: 0.7933 time: 0.0247s\n",
            "Epoch: 0401 loss_train: 0.2528 acc_train: 0.9643 loss_val: 0.7066 acc_val: 0.7933 time: 0.0250s\n",
            "Epoch: 0402 loss_train: 0.2661 acc_train: 0.9786 loss_val: 0.7074 acc_val: 0.7933 time: 0.0246s\n",
            "Epoch: 0403 loss_train: 0.2411 acc_train: 0.9571 loss_val: 0.7080 acc_val: 0.7933 time: 0.0246s\n",
            "Epoch: 0404 loss_train: 0.2701 acc_train: 0.9500 loss_val: 0.7091 acc_val: 0.8000 time: 0.0239s\n",
            "Epoch: 0405 loss_train: 0.2520 acc_train: 0.9571 loss_val: 0.7097 acc_val: 0.8000 time: 0.0240s\n",
            "Epoch: 0406 loss_train: 0.2487 acc_train: 0.9714 loss_val: 0.7100 acc_val: 0.7967 time: 0.0266s\n",
            "Epoch: 0407 loss_train: 0.3133 acc_train: 0.9286 loss_val: 0.7106 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0408 loss_train: 0.2199 acc_train: 0.9786 loss_val: 0.7108 acc_val: 0.7933 time: 0.0256s\n",
            "Epoch: 0409 loss_train: 0.2469 acc_train: 0.9500 loss_val: 0.7109 acc_val: 0.7933 time: 0.0247s\n",
            "Epoch: 0410 loss_train: 0.2790 acc_train: 0.9571 loss_val: 0.7108 acc_val: 0.7933 time: 0.0330s\n",
            "Epoch: 0411 loss_train: 0.2408 acc_train: 0.9571 loss_val: 0.7099 acc_val: 0.7967 time: 0.0275s\n",
            "Epoch: 0412 loss_train: 0.2970 acc_train: 0.9357 loss_val: 0.7084 acc_val: 0.8000 time: 0.0243s\n",
            "Epoch: 0413 loss_train: 0.2297 acc_train: 0.9786 loss_val: 0.7071 acc_val: 0.7967 time: 0.0289s\n",
            "Epoch: 0414 loss_train: 0.2633 acc_train: 0.9643 loss_val: 0.7065 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0415 loss_train: 0.2600 acc_train: 0.9500 loss_val: 0.7057 acc_val: 0.7967 time: 0.0237s\n",
            "Epoch: 0416 loss_train: 0.2297 acc_train: 0.9714 loss_val: 0.7047 acc_val: 0.7933 time: 0.0252s\n",
            "Epoch: 0417 loss_train: 0.2155 acc_train: 0.9571 loss_val: 0.7043 acc_val: 0.7933 time: 0.0250s\n",
            "Epoch: 0418 loss_train: 0.3271 acc_train: 0.9214 loss_val: 0.7043 acc_val: 0.7900 time: 0.0274s\n",
            "Epoch: 0419 loss_train: 0.3058 acc_train: 0.9143 loss_val: 0.7057 acc_val: 0.7900 time: 0.0275s\n",
            "Epoch: 0420 loss_train: 0.2542 acc_train: 0.9500 loss_val: 0.7071 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0421 loss_train: 0.2535 acc_train: 0.9714 loss_val: 0.7070 acc_val: 0.7900 time: 0.0280s\n",
            "Epoch: 0422 loss_train: 0.2179 acc_train: 0.9786 loss_val: 0.7071 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0423 loss_train: 0.2766 acc_train: 0.9571 loss_val: 0.7064 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0424 loss_train: 0.2197 acc_train: 0.9643 loss_val: 0.7063 acc_val: 0.7933 time: 0.0248s\n",
            "Epoch: 0425 loss_train: 0.2627 acc_train: 0.9643 loss_val: 0.7065 acc_val: 0.8000 time: 0.0250s\n",
            "Epoch: 0426 loss_train: 0.2273 acc_train: 0.9500 loss_val: 0.7065 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0427 loss_train: 0.2275 acc_train: 0.9500 loss_val: 0.7062 acc_val: 0.7967 time: 0.0289s\n",
            "Epoch: 0428 loss_train: 0.2420 acc_train: 0.9571 loss_val: 0.7062 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0429 loss_train: 0.2455 acc_train: 0.9500 loss_val: 0.7083 acc_val: 0.7933 time: 0.0254s\n",
            "Epoch: 0430 loss_train: 0.2078 acc_train: 0.9643 loss_val: 0.7101 acc_val: 0.7933 time: 0.0243s\n",
            "Epoch: 0431 loss_train: 0.2921 acc_train: 0.9500 loss_val: 0.7106 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0432 loss_train: 0.2913 acc_train: 0.9429 loss_val: 0.7103 acc_val: 0.8000 time: 0.0252s\n",
            "Epoch: 0433 loss_train: 0.2331 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.8000 time: 0.0242s\n",
            "Epoch: 0434 loss_train: 0.2268 acc_train: 0.9714 loss_val: 0.7118 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0435 loss_train: 0.2511 acc_train: 0.9714 loss_val: 0.7111 acc_val: 0.7933 time: 0.0292s\n",
            "Epoch: 0436 loss_train: 0.2527 acc_train: 0.9643 loss_val: 0.7107 acc_val: 0.7933 time: 0.0290s\n",
            "Epoch: 0437 loss_train: 0.2417 acc_train: 0.9429 loss_val: 0.7109 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0438 loss_train: 0.1864 acc_train: 0.9857 loss_val: 0.7108 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0439 loss_train: 0.2833 acc_train: 0.9429 loss_val: 0.7122 acc_val: 0.7967 time: 0.0240s\n",
            "Epoch: 0440 loss_train: 0.2060 acc_train: 0.9857 loss_val: 0.7126 acc_val: 0.7967 time: 0.0241s\n",
            "Epoch: 0441 loss_train: 0.1990 acc_train: 0.9643 loss_val: 0.7111 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0442 loss_train: 0.2663 acc_train: 0.9357 loss_val: 0.7103 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0443 loss_train: 0.2791 acc_train: 0.9429 loss_val: 0.7105 acc_val: 0.7967 time: 0.0287s\n",
            "Epoch: 0444 loss_train: 0.2801 acc_train: 0.9429 loss_val: 0.7099 acc_val: 0.7967 time: 0.0251s\n",
            "Epoch: 0445 loss_train: 0.1901 acc_train: 0.9714 loss_val: 0.7098 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0446 loss_train: 0.2652 acc_train: 0.9571 loss_val: 0.7096 acc_val: 0.8000 time: 0.0311s\n",
            "Epoch: 0447 loss_train: 0.2064 acc_train: 0.9714 loss_val: 0.7093 acc_val: 0.8000 time: 0.0290s\n",
            "Epoch: 0448 loss_train: 0.2161 acc_train: 0.9714 loss_val: 0.7082 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0449 loss_train: 0.2891 acc_train: 0.9357 loss_val: 0.7075 acc_val: 0.8000 time: 0.0261s\n",
            "Epoch: 0450 loss_train: 0.2503 acc_train: 0.9643 loss_val: 0.7079 acc_val: 0.8000 time: 0.0337s\n",
            "Epoch: 0451 loss_train: 0.2373 acc_train: 0.9714 loss_val: 0.7081 acc_val: 0.8033 time: 0.0292s\n",
            "Epoch: 0452 loss_train: 0.2639 acc_train: 0.9429 loss_val: 0.7090 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0453 loss_train: 0.2309 acc_train: 0.9643 loss_val: 0.7092 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0454 loss_train: 0.2267 acc_train: 0.9714 loss_val: 0.7093 acc_val: 0.7967 time: 0.0274s\n",
            "Epoch: 0455 loss_train: 0.2429 acc_train: 0.9571 loss_val: 0.7092 acc_val: 0.7933 time: 0.0261s\n",
            "Epoch: 0456 loss_train: 0.2789 acc_train: 0.9500 loss_val: 0.7082 acc_val: 0.7933 time: 0.0279s\n",
            "Epoch: 0457 loss_train: 0.2501 acc_train: 0.9643 loss_val: 0.7077 acc_val: 0.7933 time: 0.0290s\n",
            "Epoch: 0458 loss_train: 0.2355 acc_train: 0.9643 loss_val: 0.7077 acc_val: 0.7933 time: 0.0274s\n",
            "Epoch: 0459 loss_train: 0.2266 acc_train: 0.9643 loss_val: 0.7084 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0460 loss_train: 0.2330 acc_train: 0.9714 loss_val: 0.7097 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0461 loss_train: 0.2371 acc_train: 0.9571 loss_val: 0.7098 acc_val: 0.7967 time: 0.0307s\n",
            "Epoch: 0462 loss_train: 0.2210 acc_train: 0.9786 loss_val: 0.7083 acc_val: 0.7967 time: 0.0268s\n",
            "Epoch: 0463 loss_train: 0.2284 acc_train: 0.9714 loss_val: 0.7066 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0464 loss_train: 0.2715 acc_train: 0.9429 loss_val: 0.7047 acc_val: 0.7933 time: 0.0280s\n",
            "Epoch: 0465 loss_train: 0.2731 acc_train: 0.9286 loss_val: 0.7052 acc_val: 0.7967 time: 0.0332s\n",
            "Epoch: 0466 loss_train: 0.2389 acc_train: 0.9786 loss_val: 0.7069 acc_val: 0.7933 time: 0.0253s\n",
            "Epoch: 0467 loss_train: 0.2660 acc_train: 0.9429 loss_val: 0.7081 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0468 loss_train: 0.2412 acc_train: 0.9786 loss_val: 0.7097 acc_val: 0.7933 time: 0.0239s\n",
            "Epoch: 0469 loss_train: 0.2527 acc_train: 0.9500 loss_val: 0.7106 acc_val: 0.7933 time: 0.0241s\n",
            "Epoch: 0470 loss_train: 0.2668 acc_train: 0.9571 loss_val: 0.7108 acc_val: 0.7967 time: 0.0238s\n",
            "Epoch: 0471 loss_train: 0.2754 acc_train: 0.9643 loss_val: 0.7091 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0472 loss_train: 0.2209 acc_train: 0.9643 loss_val: 0.7075 acc_val: 0.8000 time: 0.0287s\n",
            "Epoch: 0473 loss_train: 0.2097 acc_train: 0.9714 loss_val: 0.7068 acc_val: 0.8000 time: 0.0251s\n",
            "Epoch: 0474 loss_train: 0.2477 acc_train: 0.9500 loss_val: 0.7059 acc_val: 0.8000 time: 0.0242s\n",
            "Epoch: 0475 loss_train: 0.2502 acc_train: 0.9500 loss_val: 0.7051 acc_val: 0.8000 time: 0.0238s\n",
            "Epoch: 0476 loss_train: 0.2414 acc_train: 0.9857 loss_val: 0.7051 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0477 loss_train: 0.2400 acc_train: 0.9571 loss_val: 0.7054 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0478 loss_train: 0.2379 acc_train: 0.9500 loss_val: 0.7054 acc_val: 0.7967 time: 0.0241s\n",
            "Epoch: 0479 loss_train: 0.2512 acc_train: 0.9500 loss_val: 0.7064 acc_val: 0.8000 time: 0.0242s\n",
            "Epoch: 0480 loss_train: 0.2079 acc_train: 0.9786 loss_val: 0.7075 acc_val: 0.8000 time: 0.0281s\n",
            "Epoch: 0481 loss_train: 0.2317 acc_train: 0.9786 loss_val: 0.7092 acc_val: 0.7967 time: 0.0300s\n",
            "Epoch: 0482 loss_train: 0.2272 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.7933 time: 0.0271s\n",
            "Epoch: 0483 loss_train: 0.2431 acc_train: 0.9500 loss_val: 0.7116 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0484 loss_train: 0.2399 acc_train: 0.9571 loss_val: 0.7111 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0485 loss_train: 0.2534 acc_train: 0.9500 loss_val: 0.7087 acc_val: 0.7933 time: 0.0240s\n",
            "Epoch: 0486 loss_train: 0.2441 acc_train: 0.9571 loss_val: 0.7075 acc_val: 0.7933 time: 0.0237s\n",
            "Epoch: 0487 loss_train: 0.2461 acc_train: 0.9857 loss_val: 0.7053 acc_val: 0.7933 time: 0.0252s\n",
            "Epoch: 0488 loss_train: 0.2211 acc_train: 0.9786 loss_val: 0.7040 acc_val: 0.7967 time: 0.0291s\n",
            "Epoch: 0489 loss_train: 0.2598 acc_train: 0.9429 loss_val: 0.7030 acc_val: 0.7967 time: 0.0327s\n",
            "Epoch: 0490 loss_train: 0.2805 acc_train: 0.9500 loss_val: 0.7036 acc_val: 0.7967 time: 0.0385s\n",
            "Epoch: 0491 loss_train: 0.2431 acc_train: 0.9500 loss_val: 0.7036 acc_val: 0.7967 time: 0.0325s\n",
            "Epoch: 0492 loss_train: 0.2209 acc_train: 0.9714 loss_val: 0.7041 acc_val: 0.7967 time: 0.0320s\n",
            "Epoch: 0493 loss_train: 0.2280 acc_train: 0.9571 loss_val: 0.7057 acc_val: 0.7967 time: 0.0306s\n",
            "Epoch: 0494 loss_train: 0.2342 acc_train: 0.9429 loss_val: 0.7067 acc_val: 0.8000 time: 0.0463s\n",
            "Epoch: 0495 loss_train: 0.2451 acc_train: 0.9429 loss_val: 0.7078 acc_val: 0.8000 time: 0.0340s\n",
            "Epoch: 0496 loss_train: 0.2504 acc_train: 0.9571 loss_val: 0.7090 acc_val: 0.8000 time: 0.0316s\n",
            "Epoch: 0497 loss_train: 0.2416 acc_train: 0.9500 loss_val: 0.7096 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0498 loss_train: 0.2515 acc_train: 0.9357 loss_val: 0.7105 acc_val: 0.8000 time: 0.0334s\n",
            "Epoch: 0499 loss_train: 0.2272 acc_train: 0.9643 loss_val: 0.7105 acc_val: 0.7933 time: 0.0312s\n",
            "Epoch: 0500 loss_train: 0.2439 acc_train: 0.9500 loss_val: 0.7114 acc_val: 0.7933 time: 0.0388s\n",
            "Epoch: 0501 loss_train: 0.2704 acc_train: 0.9500 loss_val: 0.7114 acc_val: 0.7933 time: 0.0324s\n",
            "Epoch: 0502 loss_train: 0.2784 acc_train: 0.9643 loss_val: 0.7103 acc_val: 0.7967 time: 0.0318s\n",
            "Epoch: 0503 loss_train: 0.2273 acc_train: 0.9500 loss_val: 0.7093 acc_val: 0.7933 time: 0.0311s\n",
            "Epoch: 0504 loss_train: 0.2191 acc_train: 0.9571 loss_val: 0.7084 acc_val: 0.7967 time: 0.0318s\n",
            "Epoch: 0505 loss_train: 0.1853 acc_train: 0.9857 loss_val: 0.7075 acc_val: 0.7933 time: 0.0316s\n",
            "Epoch: 0506 loss_train: 0.2159 acc_train: 0.9929 loss_val: 0.7070 acc_val: 0.7967 time: 0.0395s\n",
            "Epoch: 0507 loss_train: 0.1990 acc_train: 0.9714 loss_val: 0.7070 acc_val: 0.7967 time: 0.0316s\n",
            "Epoch: 0508 loss_train: 0.2131 acc_train: 0.9786 loss_val: 0.7072 acc_val: 0.8000 time: 0.0318s\n",
            "Epoch: 0509 loss_train: 0.2115 acc_train: 0.9643 loss_val: 0.7079 acc_val: 0.7967 time: 0.0314s\n",
            "Epoch: 0510 loss_train: 0.2508 acc_train: 0.9571 loss_val: 0.7074 acc_val: 0.7967 time: 0.0326s\n",
            "Epoch: 0511 loss_train: 0.2733 acc_train: 0.9429 loss_val: 0.7065 acc_val: 0.7967 time: 0.0354s\n",
            "Epoch: 0512 loss_train: 0.2240 acc_train: 0.9643 loss_val: 0.7064 acc_val: 0.7967 time: 0.0389s\n",
            "Epoch: 0513 loss_train: 0.2082 acc_train: 0.9786 loss_val: 0.7071 acc_val: 0.7967 time: 0.0323s\n",
            "Epoch: 0514 loss_train: 0.2277 acc_train: 0.9571 loss_val: 0.7089 acc_val: 0.7967 time: 0.0338s\n",
            "Epoch: 0515 loss_train: 0.2614 acc_train: 0.9571 loss_val: 0.7097 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0516 loss_train: 0.1805 acc_train: 0.9714 loss_val: 0.7097 acc_val: 0.8000 time: 0.0379s\n",
            "Epoch: 0517 loss_train: 0.2370 acc_train: 0.9786 loss_val: 0.7099 acc_val: 0.8000 time: 0.0325s\n",
            "Epoch: 0518 loss_train: 0.2228 acc_train: 0.9643 loss_val: 0.7072 acc_val: 0.8033 time: 0.0382s\n",
            "Epoch: 0519 loss_train: 0.2638 acc_train: 0.9429 loss_val: 0.7055 acc_val: 0.8033 time: 0.0323s\n",
            "Epoch: 0520 loss_train: 0.2568 acc_train: 0.9571 loss_val: 0.7037 acc_val: 0.8033 time: 0.0317s\n",
            "Epoch: 0521 loss_train: 0.2360 acc_train: 0.9500 loss_val: 0.7049 acc_val: 0.8033 time: 0.0347s\n",
            "Epoch: 0522 loss_train: 0.2571 acc_train: 0.9571 loss_val: 0.7055 acc_val: 0.8000 time: 0.0369s\n",
            "Epoch: 0523 loss_train: 0.2603 acc_train: 0.9500 loss_val: 0.7060 acc_val: 0.8000 time: 0.0315s\n",
            "Epoch: 0524 loss_train: 0.2668 acc_train: 0.9500 loss_val: 0.7047 acc_val: 0.8000 time: 0.0382s\n",
            "Epoch: 0525 loss_train: 0.2630 acc_train: 0.9500 loss_val: 0.7036 acc_val: 0.8000 time: 0.0311s\n",
            "Epoch: 0526 loss_train: 0.2411 acc_train: 0.9500 loss_val: 0.7031 acc_val: 0.8000 time: 0.0307s\n",
            "Epoch: 0527 loss_train: 0.2061 acc_train: 0.9786 loss_val: 0.7026 acc_val: 0.8000 time: 0.0308s\n",
            "Epoch: 0528 loss_train: 0.2318 acc_train: 0.9571 loss_val: 0.7028 acc_val: 0.7933 time: 0.0351s\n",
            "Epoch: 0529 loss_train: 0.2157 acc_train: 0.9857 loss_val: 0.7026 acc_val: 0.7900 time: 0.0325s\n",
            "Epoch: 0530 loss_train: 0.2600 acc_train: 0.9500 loss_val: 0.7026 acc_val: 0.7900 time: 0.0405s\n",
            "Epoch: 0531 loss_train: 0.2279 acc_train: 0.9714 loss_val: 0.7016 acc_val: 0.7933 time: 0.0319s\n",
            "Epoch: 0532 loss_train: 0.2213 acc_train: 0.9786 loss_val: 0.6998 acc_val: 0.7967 time: 0.0323s\n",
            "Epoch: 0533 loss_train: 0.2101 acc_train: 0.9714 loss_val: 0.6994 acc_val: 0.7967 time: 0.0338s\n",
            "Epoch: 0534 loss_train: 0.2429 acc_train: 0.9571 loss_val: 0.6995 acc_val: 0.7967 time: 0.0348s\n",
            "Epoch: 0535 loss_train: 0.2576 acc_train: 0.9500 loss_val: 0.7015 acc_val: 0.8033 time: 0.0319s\n",
            "Epoch: 0536 loss_train: 0.2384 acc_train: 0.9571 loss_val: 0.7033 acc_val: 0.8033 time: 0.0363s\n",
            "Epoch: 0537 loss_train: 0.2135 acc_train: 0.9500 loss_val: 0.7041 acc_val: 0.8033 time: 0.0376s\n",
            "Epoch: 0538 loss_train: 0.2029 acc_train: 0.9714 loss_val: 0.7042 acc_val: 0.8000 time: 0.0322s\n",
            "Epoch: 0539 loss_train: 0.2373 acc_train: 0.9500 loss_val: 0.7044 acc_val: 0.8000 time: 0.0435s\n",
            "Epoch: 0540 loss_train: 0.2713 acc_train: 0.9500 loss_val: 0.7048 acc_val: 0.8000 time: 0.0343s\n",
            "Epoch: 0541 loss_train: 0.2413 acc_train: 0.9571 loss_val: 0.7039 acc_val: 0.8000 time: 0.0374s\n",
            "Epoch: 0542 loss_train: 0.2318 acc_train: 0.9571 loss_val: 0.7030 acc_val: 0.8033 time: 0.0345s\n",
            "Epoch: 0543 loss_train: 0.2439 acc_train: 0.9500 loss_val: 0.7025 acc_val: 0.8033 time: 0.0366s\n",
            "Epoch: 0544 loss_train: 0.2085 acc_train: 0.9643 loss_val: 0.7021 acc_val: 0.8033 time: 0.0327s\n",
            "Epoch: 0545 loss_train: 0.2239 acc_train: 0.9786 loss_val: 0.7019 acc_val: 0.8067 time: 0.0330s\n",
            "Epoch: 0546 loss_train: 0.2430 acc_train: 0.9643 loss_val: 0.7022 acc_val: 0.8033 time: 0.0325s\n",
            "Epoch: 0547 loss_train: 0.2883 acc_train: 0.9286 loss_val: 0.7030 acc_val: 0.8000 time: 0.0484s\n",
            "Epoch: 0548 loss_train: 0.2314 acc_train: 0.9643 loss_val: 0.7043 acc_val: 0.8000 time: 0.0326s\n",
            "Epoch: 0549 loss_train: 0.2243 acc_train: 0.9786 loss_val: 0.7067 acc_val: 0.7967 time: 0.0341s\n",
            "Epoch: 0550 loss_train: 0.2349 acc_train: 0.9500 loss_val: 0.7086 acc_val: 0.7967 time: 0.0336s\n",
            "Epoch: 0551 loss_train: 0.2570 acc_train: 0.9429 loss_val: 0.7083 acc_val: 0.8000 time: 0.0320s\n",
            "Epoch: 0552 loss_train: 0.2254 acc_train: 0.9571 loss_val: 0.7089 acc_val: 0.8000 time: 0.0329s\n",
            "Epoch: 0553 loss_train: 0.2488 acc_train: 0.9571 loss_val: 0.7082 acc_val: 0.7967 time: 0.0393s\n",
            "Epoch: 0554 loss_train: 0.2127 acc_train: 0.9786 loss_val: 0.7072 acc_val: 0.7967 time: 0.0340s\n",
            "Epoch: 0555 loss_train: 0.2469 acc_train: 0.9571 loss_val: 0.7054 acc_val: 0.7933 time: 0.0324s\n",
            "Epoch: 0556 loss_train: 0.2129 acc_train: 0.9786 loss_val: 0.7037 acc_val: 0.7833 time: 0.0330s\n",
            "Epoch: 0557 loss_train: 0.1974 acc_train: 0.9714 loss_val: 0.7024 acc_val: 0.7833 time: 0.0325s\n",
            "Epoch: 0558 loss_train: 0.2161 acc_train: 0.9643 loss_val: 0.7022 acc_val: 0.7933 time: 0.0334s\n",
            "Epoch: 0559 loss_train: 0.2159 acc_train: 0.9786 loss_val: 0.7027 acc_val: 0.8000 time: 0.0407s\n",
            "Epoch: 0560 loss_train: 0.2726 acc_train: 0.9429 loss_val: 0.7040 acc_val: 0.8000 time: 0.0325s\n",
            "Epoch: 0561 loss_train: 0.2557 acc_train: 0.9643 loss_val: 0.7077 acc_val: 0.7967 time: 0.0316s\n",
            "Epoch: 0562 loss_train: 0.2482 acc_train: 0.9429 loss_val: 0.7117 acc_val: 0.7967 time: 0.0341s\n",
            "Epoch: 0563 loss_train: 0.2383 acc_train: 0.9500 loss_val: 0.7152 acc_val: 0.7967 time: 0.0359s\n",
            "Epoch: 0564 loss_train: 0.2094 acc_train: 0.9643 loss_val: 0.7180 acc_val: 0.7967 time: 0.0339s\n",
            "Epoch: 0565 loss_train: 0.2221 acc_train: 0.9500 loss_val: 0.7175 acc_val: 0.8000 time: 0.0396s\n",
            "Epoch: 0566 loss_train: 0.2456 acc_train: 0.9500 loss_val: 0.7151 acc_val: 0.7967 time: 0.0334s\n",
            "Epoch: 0567 loss_train: 0.2311 acc_train: 0.9714 loss_val: 0.7130 acc_val: 0.7967 time: 0.0350s\n",
            "Epoch: 0568 loss_train: 0.2472 acc_train: 0.9500 loss_val: 0.7117 acc_val: 0.7967 time: 0.0331s\n",
            "Epoch: 0569 loss_train: 0.2694 acc_train: 0.9214 loss_val: 0.7104 acc_val: 0.7967 time: 0.0317s\n",
            "Epoch: 0570 loss_train: 0.2485 acc_train: 0.9571 loss_val: 0.7096 acc_val: 0.7967 time: 0.0311s\n",
            "Epoch: 0571 loss_train: 0.2398 acc_train: 0.9500 loss_val: 0.7104 acc_val: 0.7967 time: 0.0414s\n",
            "Epoch: 0572 loss_train: 0.2378 acc_train: 0.9643 loss_val: 0.7107 acc_val: 0.7967 time: 0.0315s\n",
            "Epoch: 0573 loss_train: 0.1888 acc_train: 0.9857 loss_val: 0.7115 acc_val: 0.8000 time: 0.0316s\n",
            "Epoch: 0574 loss_train: 0.2198 acc_train: 0.9571 loss_val: 0.7127 acc_val: 0.8000 time: 0.0376s\n",
            "Epoch: 0575 loss_train: 0.2582 acc_train: 0.9643 loss_val: 0.7138 acc_val: 0.7967 time: 0.0351s\n",
            "Epoch: 0576 loss_train: 0.2066 acc_train: 0.9643 loss_val: 0.7170 acc_val: 0.7967 time: 0.0354s\n",
            "Epoch: 0577 loss_train: 0.2104 acc_train: 0.9714 loss_val: 0.7207 acc_val: 0.7933 time: 0.0381s\n",
            "Epoch: 0578 loss_train: 0.2220 acc_train: 0.9643 loss_val: 0.7226 acc_val: 0.7933 time: 0.0326s\n",
            "Epoch: 0579 loss_train: 0.2629 acc_train: 0.9571 loss_val: 0.7234 acc_val: 0.7967 time: 0.0394s\n",
            "Epoch: 0580 loss_train: 0.2412 acc_train: 0.9500 loss_val: 0.7231 acc_val: 0.7967 time: 0.0317s\n",
            "Epoch: 0581 loss_train: 0.2404 acc_train: 0.9500 loss_val: 0.7213 acc_val: 0.7967 time: 0.0323s\n",
            "Epoch: 0582 loss_train: 0.1974 acc_train: 0.9571 loss_val: 0.7180 acc_val: 0.7967 time: 0.0319s\n",
            "Epoch: 0583 loss_train: 0.2026 acc_train: 0.9786 loss_val: 0.7157 acc_val: 0.7933 time: 0.0379s\n",
            "Epoch: 0584 loss_train: 0.2427 acc_train: 0.9643 loss_val: 0.7130 acc_val: 0.7933 time: 0.0312s\n",
            "Epoch: 0585 loss_train: 0.2203 acc_train: 0.9643 loss_val: 0.7111 acc_val: 0.7933 time: 0.0324s\n",
            "Epoch: 0586 loss_train: 0.2312 acc_train: 0.9500 loss_val: 0.7094 acc_val: 0.7933 time: 0.0316s\n",
            "Epoch: 0587 loss_train: 0.2387 acc_train: 0.9357 loss_val: 0.7096 acc_val: 0.7967 time: 0.0318s\n",
            "Epoch: 0588 loss_train: 0.2574 acc_train: 0.9571 loss_val: 0.7119 acc_val: 0.7933 time: 0.0333s\n",
            "Epoch: 0589 loss_train: 0.2603 acc_train: 0.9500 loss_val: 0.7147 acc_val: 0.7967 time: 0.0412s\n",
            "Epoch: 0590 loss_train: 0.2255 acc_train: 0.9786 loss_val: 0.7183 acc_val: 0.7967 time: 0.0339s\n",
            "Epoch: 0591 loss_train: 0.2729 acc_train: 0.9500 loss_val: 0.7202 acc_val: 0.7967 time: 0.0320s\n",
            "Epoch: 0592 loss_train: 0.2686 acc_train: 0.9357 loss_val: 0.7199 acc_val: 0.8000 time: 0.0317s\n",
            "Epoch: 0593 loss_train: 0.2735 acc_train: 0.9571 loss_val: 0.7183 acc_val: 0.7967 time: 0.0341s\n",
            "Epoch: 0594 loss_train: 0.2149 acc_train: 0.9857 loss_val: 0.7161 acc_val: 0.8000 time: 0.0392s\n",
            "Epoch: 0595 loss_train: 0.2460 acc_train: 0.9357 loss_val: 0.7145 acc_val: 0.7967 time: 0.0449s\n",
            "Epoch: 0596 loss_train: 0.2195 acc_train: 0.9714 loss_val: 0.7123 acc_val: 0.7967 time: 0.0334s\n",
            "Epoch: 0597 loss_train: 0.2177 acc_train: 0.9643 loss_val: 0.7099 acc_val: 0.8000 time: 0.0320s\n",
            "Epoch: 0598 loss_train: 0.2590 acc_train: 0.9357 loss_val: 0.7084 acc_val: 0.8000 time: 0.0325s\n",
            "Epoch: 0599 loss_train: 0.2299 acc_train: 0.9714 loss_val: 0.7078 acc_val: 0.8000 time: 0.0323s\n",
            "Epoch: 0600 loss_train: 0.2324 acc_train: 0.9286 loss_val: 0.7081 acc_val: 0.8000 time: 0.0325s\n",
            "Epoch: 0601 loss_train: 0.2280 acc_train: 0.9786 loss_val: 0.7091 acc_val: 0.8000 time: 0.0471s\n",
            "Epoch: 0602 loss_train: 0.2103 acc_train: 0.9571 loss_val: 0.7107 acc_val: 0.7967 time: 0.0394s\n",
            "Epoch: 0603 loss_train: 0.2367 acc_train: 0.9500 loss_val: 0.7128 acc_val: 0.7967 time: 0.0385s\n",
            "Epoch: 0604 loss_train: 0.1935 acc_train: 0.9786 loss_val: 0.7140 acc_val: 0.7967 time: 0.0355s\n",
            "Epoch: 0605 loss_train: 0.2300 acc_train: 0.9571 loss_val: 0.7141 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0606 loss_train: 0.2109 acc_train: 0.9857 loss_val: 0.7142 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0607 loss_train: 0.2318 acc_train: 0.9357 loss_val: 0.7150 acc_val: 0.7967 time: 0.0277s\n",
            "Epoch: 0608 loss_train: 0.2379 acc_train: 0.9571 loss_val: 0.7151 acc_val: 0.8000 time: 0.0241s\n",
            "Epoch: 0609 loss_train: 0.2189 acc_train: 0.9643 loss_val: 0.7143 acc_val: 0.8000 time: 0.0240s\n",
            "Epoch: 0610 loss_train: 0.2431 acc_train: 0.9786 loss_val: 0.7131 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0611 loss_train: 0.2209 acc_train: 0.9643 loss_val: 0.7127 acc_val: 0.8000 time: 0.0247s\n",
            "Epoch: 0612 loss_train: 0.1868 acc_train: 0.9786 loss_val: 0.7126 acc_val: 0.7967 time: 0.0241s\n",
            "Epoch: 0613 loss_train: 0.2057 acc_train: 0.9714 loss_val: 0.7121 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0614 loss_train: 0.1996 acc_train: 0.9714 loss_val: 0.7129 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0615 loss_train: 0.2626 acc_train: 0.9286 loss_val: 0.7123 acc_val: 0.7967 time: 0.0314s\n",
            "Epoch: 0616 loss_train: 0.2066 acc_train: 0.9714 loss_val: 0.7120 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0617 loss_train: 0.2198 acc_train: 0.9500 loss_val: 0.7127 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0618 loss_train: 0.2197 acc_train: 0.9714 loss_val: 0.7124 acc_val: 0.8033 time: 0.0258s\n",
            "Epoch: 0619 loss_train: 0.1774 acc_train: 0.9643 loss_val: 0.7120 acc_val: 0.8033 time: 0.0258s\n",
            "Epoch: 0620 loss_train: 0.2226 acc_train: 0.9714 loss_val: 0.7124 acc_val: 0.8000 time: 0.0256s\n",
            "Epoch: 0621 loss_train: 0.1968 acc_train: 0.9786 loss_val: 0.7136 acc_val: 0.8000 time: 0.0258s\n",
            "Epoch: 0622 loss_train: 0.2355 acc_train: 0.9643 loss_val: 0.7150 acc_val: 0.7900 time: 0.0259s\n",
            "Epoch: 0623 loss_train: 0.2310 acc_train: 0.9357 loss_val: 0.7160 acc_val: 0.7900 time: 0.0282s\n",
            "Epoch: 0624 loss_train: 0.2121 acc_train: 0.9786 loss_val: 0.7159 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0625 loss_train: 0.2171 acc_train: 0.9571 loss_val: 0.7147 acc_val: 0.8000 time: 0.0255s\n",
            "Epoch: 0626 loss_train: 0.2532 acc_train: 0.9500 loss_val: 0.7124 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0627 loss_train: 0.2092 acc_train: 0.9786 loss_val: 0.7093 acc_val: 0.8000 time: 0.0288s\n",
            "Epoch: 0628 loss_train: 0.1966 acc_train: 0.9786 loss_val: 0.7060 acc_val: 0.8000 time: 0.0290s\n",
            "Epoch: 0629 loss_train: 0.2499 acc_train: 0.9500 loss_val: 0.7051 acc_val: 0.8000 time: 0.0279s\n",
            "Epoch: 0630 loss_train: 0.1914 acc_train: 0.9714 loss_val: 0.7055 acc_val: 0.7967 time: 0.0252s\n",
            "Epoch: 0631 loss_train: 0.2126 acc_train: 0.9714 loss_val: 0.7058 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0632 loss_train: 0.2287 acc_train: 0.9786 loss_val: 0.7066 acc_val: 0.8000 time: 0.0287s\n",
            "Epoch: 0633 loss_train: 0.2721 acc_train: 0.9571 loss_val: 0.7068 acc_val: 0.8033 time: 0.0247s\n",
            "Epoch: 0634 loss_train: 0.1773 acc_train: 0.9857 loss_val: 0.7083 acc_val: 0.8033 time: 0.0257s\n",
            "Epoch: 0635 loss_train: 0.2370 acc_train: 0.9500 loss_val: 0.7097 acc_val: 0.8000 time: 0.0273s\n",
            "Epoch: 0636 loss_train: 0.2236 acc_train: 0.9857 loss_val: 0.7095 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0637 loss_train: 0.1911 acc_train: 0.9571 loss_val: 0.7091 acc_val: 0.8033 time: 0.0321s\n",
            "Epoch: 0638 loss_train: 0.2287 acc_train: 0.9714 loss_val: 0.7089 acc_val: 0.8033 time: 0.0268s\n",
            "Epoch: 0639 loss_train: 0.2168 acc_train: 0.9714 loss_val: 0.7083 acc_val: 0.8033 time: 0.0260s\n",
            "Epoch: 0640 loss_train: 0.2501 acc_train: 0.9286 loss_val: 0.7078 acc_val: 0.8033 time: 0.0263s\n",
            "Epoch: 0641 loss_train: 0.2329 acc_train: 0.9643 loss_val: 0.7070 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0642 loss_train: 0.2462 acc_train: 0.9286 loss_val: 0.7069 acc_val: 0.7967 time: 0.0281s\n",
            "Epoch: 0643 loss_train: 0.2404 acc_train: 0.9429 loss_val: 0.7077 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0644 loss_train: 0.2415 acc_train: 0.9786 loss_val: 0.7092 acc_val: 0.7967 time: 0.0268s\n",
            "Epoch: 0645 loss_train: 0.1941 acc_train: 0.9857 loss_val: 0.7120 acc_val: 0.7967 time: 0.0319s\n",
            "Epoch: 0646 loss_train: 0.1690 acc_train: 0.9714 loss_val: 0.7139 acc_val: 0.7967 time: 0.0250s\n",
            "Epoch: 0647 loss_train: 0.2236 acc_train: 0.9429 loss_val: 0.7162 acc_val: 0.7967 time: 0.0252s\n",
            "Epoch: 0648 loss_train: 0.1817 acc_train: 0.9857 loss_val: 0.7170 acc_val: 0.7933 time: 0.0263s\n",
            "Epoch: 0649 loss_train: 0.2413 acc_train: 0.9571 loss_val: 0.7176 acc_val: 0.7933 time: 0.0260s\n",
            "Epoch: 0650 loss_train: 0.2235 acc_train: 0.9786 loss_val: 0.7170 acc_val: 0.7933 time: 0.0256s\n",
            "Epoch: 0651 loss_train: 0.2461 acc_train: 0.9500 loss_val: 0.7157 acc_val: 0.7933 time: 0.0253s\n",
            "Epoch: 0652 loss_train: 0.1984 acc_train: 0.9786 loss_val: 0.7135 acc_val: 0.7933 time: 0.0308s\n",
            "Epoch: 0653 loss_train: 0.2221 acc_train: 0.9571 loss_val: 0.7112 acc_val: 0.7967 time: 0.0271s\n",
            "Epoch: 0654 loss_train: 0.2287 acc_train: 0.9571 loss_val: 0.7095 acc_val: 0.7933 time: 0.0254s\n",
            "Epoch: 0655 loss_train: 0.2423 acc_train: 0.9571 loss_val: 0.7095 acc_val: 0.7933 time: 0.0288s\n",
            "Epoch: 0656 loss_train: 0.2321 acc_train: 0.9643 loss_val: 0.7093 acc_val: 0.7967 time: 0.0258s\n",
            "Epoch: 0657 loss_train: 0.2492 acc_train: 0.9429 loss_val: 0.7107 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0658 loss_train: 0.2395 acc_train: 0.9714 loss_val: 0.7111 acc_val: 0.7967 time: 0.0251s\n",
            "Epoch: 0659 loss_train: 0.2228 acc_train: 0.9714 loss_val: 0.7117 acc_val: 0.7967 time: 0.0278s\n",
            "Epoch: 0660 loss_train: 0.2419 acc_train: 0.9714 loss_val: 0.7136 acc_val: 0.7967 time: 0.0306s\n",
            "Epoch: 0661 loss_train: 0.2554 acc_train: 0.9571 loss_val: 0.7146 acc_val: 0.7967 time: 0.0243s\n",
            "Epoch: 0662 loss_train: 0.2110 acc_train: 0.9714 loss_val: 0.7139 acc_val: 0.7967 time: 0.0315s\n",
            "Epoch: 0663 loss_train: 0.2158 acc_train: 0.9643 loss_val: 0.7130 acc_val: 0.7933 time: 0.0275s\n",
            "Epoch: 0664 loss_train: 0.2335 acc_train: 0.9500 loss_val: 0.7121 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0665 loss_train: 0.1798 acc_train: 0.9643 loss_val: 0.7112 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0666 loss_train: 0.2127 acc_train: 0.9571 loss_val: 0.7109 acc_val: 0.8000 time: 0.0335s\n",
            "Epoch: 0667 loss_train: 0.2074 acc_train: 0.9786 loss_val: 0.7098 acc_val: 0.7967 time: 0.0269s\n",
            "Epoch: 0668 loss_train: 0.2164 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.7967 time: 0.0252s\n",
            "Epoch: 0669 loss_train: 0.2679 acc_train: 0.9286 loss_val: 0.7069 acc_val: 0.7933 time: 0.0271s\n",
            "Epoch: 0670 loss_train: 0.2081 acc_train: 0.9571 loss_val: 0.7059 acc_val: 0.7967 time: 0.0266s\n",
            "Epoch: 0671 loss_train: 0.1964 acc_train: 0.9857 loss_val: 0.7047 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0672 loss_train: 0.2327 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.7967 time: 0.0270s\n",
            "Epoch: 0673 loss_train: 0.2183 acc_train: 0.9571 loss_val: 0.7034 acc_val: 0.7967 time: 0.0296s\n",
            "Epoch: 0674 loss_train: 0.2349 acc_train: 0.9500 loss_val: 0.7029 acc_val: 0.7967 time: 0.0279s\n",
            "Epoch: 0675 loss_train: 0.2043 acc_train: 0.9714 loss_val: 0.7032 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0676 loss_train: 0.2061 acc_train: 0.9714 loss_val: 0.7041 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0677 loss_train: 0.1724 acc_train: 0.9786 loss_val: 0.7047 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0678 loss_train: 0.1791 acc_train: 0.9857 loss_val: 0.7062 acc_val: 0.8000 time: 0.0247s\n",
            "Epoch: 0679 loss_train: 0.1931 acc_train: 0.9500 loss_val: 0.7071 acc_val: 0.7967 time: 0.0250s\n",
            "Epoch: 0680 loss_train: 0.2034 acc_train: 0.9714 loss_val: 0.7085 acc_val: 0.8000 time: 0.0253s\n",
            "Epoch: 0681 loss_train: 0.2112 acc_train: 0.9714 loss_val: 0.7086 acc_val: 0.8000 time: 0.0304s\n",
            "Epoch: 0682 loss_train: 0.2544 acc_train: 0.9643 loss_val: 0.7086 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0683 loss_train: 0.1947 acc_train: 0.9714 loss_val: 0.7101 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0684 loss_train: 0.2340 acc_train: 0.9857 loss_val: 0.7103 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0685 loss_train: 0.2790 acc_train: 0.9286 loss_val: 0.7102 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0686 loss_train: 0.1918 acc_train: 0.9714 loss_val: 0.7116 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0687 loss_train: 0.2239 acc_train: 0.9500 loss_val: 0.7122 acc_val: 0.8000 time: 0.0261s\n",
            "Epoch: 0688 loss_train: 0.2010 acc_train: 0.9857 loss_val: 0.7135 acc_val: 0.8000 time: 0.0287s\n",
            "Epoch: 0689 loss_train: 0.2277 acc_train: 0.9643 loss_val: 0.7148 acc_val: 0.8033 time: 0.0343s\n",
            "Epoch: 0690 loss_train: 0.2119 acc_train: 0.9429 loss_val: 0.7159 acc_val: 0.8067 time: 0.0258s\n",
            "Epoch: 0691 loss_train: 0.2701 acc_train: 0.9500 loss_val: 0.7161 acc_val: 0.8100 time: 0.0260s\n",
            "Epoch: 0692 loss_train: 0.1827 acc_train: 0.9643 loss_val: 0.7155 acc_val: 0.8033 time: 0.0269s\n",
            "Epoch: 0693 loss_train: 0.2215 acc_train: 0.9786 loss_val: 0.7139 acc_val: 0.8033 time: 0.0255s\n",
            "Epoch: 0694 loss_train: 0.1973 acc_train: 0.9643 loss_val: 0.7118 acc_val: 0.7967 time: 0.0274s\n",
            "Epoch: 0695 loss_train: 0.2227 acc_train: 0.9429 loss_val: 0.7123 acc_val: 0.7967 time: 0.0274s\n",
            "Epoch: 0696 loss_train: 0.2042 acc_train: 0.9643 loss_val: 0.7133 acc_val: 0.8033 time: 0.0317s\n",
            "Epoch: 0697 loss_train: 0.2410 acc_train: 0.9571 loss_val: 0.7141 acc_val: 0.8000 time: 0.0335s\n",
            "Epoch: 0698 loss_train: 0.2292 acc_train: 0.9714 loss_val: 0.7151 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0699 loss_train: 0.2884 acc_train: 0.9429 loss_val: 0.7178 acc_val: 0.8000 time: 0.0261s\n",
            "Epoch: 0700 loss_train: 0.2093 acc_train: 0.9786 loss_val: 0.7181 acc_val: 0.8000 time: 0.0279s\n",
            "Epoch: 0701 loss_train: 0.2294 acc_train: 0.9571 loss_val: 0.7165 acc_val: 0.8000 time: 0.0261s\n",
            "Epoch: 0702 loss_train: 0.2370 acc_train: 0.9571 loss_val: 0.7133 acc_val: 0.8033 time: 0.0284s\n",
            "Epoch: 0703 loss_train: 0.2066 acc_train: 0.9714 loss_val: 0.7088 acc_val: 0.8033 time: 0.0297s\n",
            "Epoch: 0704 loss_train: 0.2111 acc_train: 0.9714 loss_val: 0.7059 acc_val: 0.8033 time: 0.0281s\n",
            "Epoch: 0705 loss_train: 0.2144 acc_train: 0.9429 loss_val: 0.7043 acc_val: 0.8000 time: 0.0272s\n",
            "Epoch: 0706 loss_train: 0.1612 acc_train: 0.9929 loss_val: 0.7043 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0707 loss_train: 0.1973 acc_train: 0.9714 loss_val: 0.7053 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0708 loss_train: 0.1678 acc_train: 0.9857 loss_val: 0.7061 acc_val: 0.7967 time: 0.0275s\n",
            "Epoch: 0709 loss_train: 0.2307 acc_train: 0.9643 loss_val: 0.7083 acc_val: 0.7967 time: 0.0295s\n",
            "Epoch: 0710 loss_train: 0.1972 acc_train: 0.9714 loss_val: 0.7102 acc_val: 0.7967 time: 0.0281s\n",
            "Epoch: 0711 loss_train: 0.1942 acc_train: 0.9929 loss_val: 0.7108 acc_val: 0.7933 time: 0.0249s\n",
            "Epoch: 0712 loss_train: 0.2487 acc_train: 0.9571 loss_val: 0.7110 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0713 loss_train: 0.2087 acc_train: 0.9429 loss_val: 0.7115 acc_val: 0.8000 time: 0.0265s\n",
            "Epoch: 0714 loss_train: 0.1863 acc_train: 0.9786 loss_val: 0.7113 acc_val: 0.7967 time: 0.0283s\n",
            "Epoch: 0715 loss_train: 0.2280 acc_train: 0.9500 loss_val: 0.7096 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0716 loss_train: 0.2000 acc_train: 0.9786 loss_val: 0.7061 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0717 loss_train: 0.2619 acc_train: 0.9500 loss_val: 0.7025 acc_val: 0.7967 time: 0.0282s\n",
            "Epoch: 0718 loss_train: 0.1968 acc_train: 0.9857 loss_val: 0.7003 acc_val: 0.7900 time: 0.0241s\n",
            "Epoch: 0719 loss_train: 0.2191 acc_train: 0.9643 loss_val: 0.6984 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0720 loss_train: 0.2196 acc_train: 0.9643 loss_val: 0.6983 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0721 loss_train: 0.3108 acc_train: 0.9286 loss_val: 0.6976 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0722 loss_train: 0.2383 acc_train: 0.9429 loss_val: 0.6987 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0723 loss_train: 0.2353 acc_train: 0.9500 loss_val: 0.7010 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0724 loss_train: 0.2379 acc_train: 0.9429 loss_val: 0.7042 acc_val: 0.7967 time: 0.0300s\n",
            "Epoch: 0725 loss_train: 0.2322 acc_train: 0.9500 loss_val: 0.7089 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0726 loss_train: 0.2274 acc_train: 0.9643 loss_val: 0.7117 acc_val: 0.7933 time: 0.0247s\n",
            "Epoch: 0727 loss_train: 0.2183 acc_train: 0.9643 loss_val: 0.7118 acc_val: 0.7933 time: 0.0242s\n",
            "Epoch: 0728 loss_train: 0.2049 acc_train: 0.9857 loss_val: 0.7092 acc_val: 0.7967 time: 0.0244s\n",
            "Epoch: 0729 loss_train: 0.1765 acc_train: 0.9714 loss_val: 0.7070 acc_val: 0.8000 time: 0.0243s\n",
            "Epoch: 0730 loss_train: 0.2064 acc_train: 0.9714 loss_val: 0.7057 acc_val: 0.8000 time: 0.0250s\n",
            "Epoch: 0731 loss_train: 0.2667 acc_train: 0.9500 loss_val: 0.7045 acc_val: 0.7933 time: 0.0243s\n",
            "Epoch: 0732 loss_train: 0.1912 acc_train: 0.9714 loss_val: 0.7034 acc_val: 0.7933 time: 0.0295s\n",
            "Epoch: 0733 loss_train: 0.2006 acc_train: 0.9500 loss_val: 0.7016 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0734 loss_train: 0.2167 acc_train: 0.9571 loss_val: 0.7023 acc_val: 0.7867 time: 0.0269s\n",
            "Epoch: 0735 loss_train: 0.1994 acc_train: 0.9571 loss_val: 0.7036 acc_val: 0.7867 time: 0.0270s\n",
            "Epoch: 0736 loss_train: 0.2361 acc_train: 0.9571 loss_val: 0.7050 acc_val: 0.7933 time: 0.0249s\n",
            "Epoch: 0737 loss_train: 0.2209 acc_train: 0.9714 loss_val: 0.7069 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0738 loss_train: 0.1967 acc_train: 0.9714 loss_val: 0.7106 acc_val: 0.8000 time: 0.0254s\n",
            "Epoch: 0739 loss_train: 0.2545 acc_train: 0.9357 loss_val: 0.7147 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0740 loss_train: 0.2787 acc_train: 0.9429 loss_val: 0.7186 acc_val: 0.7933 time: 0.0245s\n",
            "Epoch: 0741 loss_train: 0.2041 acc_train: 0.9714 loss_val: 0.7198 acc_val: 0.8000 time: 0.0290s\n",
            "Epoch: 0742 loss_train: 0.2339 acc_train: 0.9643 loss_val: 0.7162 acc_val: 0.8000 time: 0.0244s\n",
            "Epoch: 0743 loss_train: 0.2503 acc_train: 0.9643 loss_val: 0.7134 acc_val: 0.8000 time: 0.0254s\n",
            "Epoch: 0744 loss_train: 0.1752 acc_train: 0.9929 loss_val: 0.7099 acc_val: 0.8033 time: 0.0242s\n",
            "Epoch: 0745 loss_train: 0.2159 acc_train: 0.9571 loss_val: 0.7062 acc_val: 0.8033 time: 0.0244s\n",
            "Epoch: 0746 loss_train: 0.2347 acc_train: 0.9643 loss_val: 0.7037 acc_val: 0.8033 time: 0.0241s\n",
            "Epoch: 0747 loss_train: 0.2288 acc_train: 0.9643 loss_val: 0.7019 acc_val: 0.8033 time: 0.0585s\n",
            "Epoch: 0748 loss_train: 0.2220 acc_train: 0.9643 loss_val: 0.7007 acc_val: 0.8000 time: 0.1140s\n",
            "Epoch: 0749 loss_train: 0.2182 acc_train: 0.9857 loss_val: 0.7008 acc_val: 0.8000 time: 0.0349s\n",
            "Epoch: 0750 loss_train: 0.2036 acc_train: 0.9500 loss_val: 0.7020 acc_val: 0.7967 time: 0.0256s\n",
            "Epoch: 0751 loss_train: 0.2106 acc_train: 0.9643 loss_val: 0.7037 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0752 loss_train: 0.1656 acc_train: 0.9857 loss_val: 0.7062 acc_val: 0.7967 time: 0.0270s\n",
            "Epoch: 0753 loss_train: 0.2392 acc_train: 0.9571 loss_val: 0.7089 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0754 loss_train: 0.2383 acc_train: 0.9571 loss_val: 0.7118 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0755 loss_train: 0.2314 acc_train: 0.9571 loss_val: 0.7150 acc_val: 0.7967 time: 0.0271s\n",
            "Epoch: 0756 loss_train: 0.2142 acc_train: 0.9643 loss_val: 0.7159 acc_val: 0.8000 time: 0.0281s\n",
            "Epoch: 0757 loss_train: 0.2303 acc_train: 0.9500 loss_val: 0.7145 acc_val: 0.8000 time: 0.0253s\n",
            "Epoch: 0758 loss_train: 0.2430 acc_train: 0.9429 loss_val: 0.7115 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0759 loss_train: 0.2038 acc_train: 0.9643 loss_val: 0.7084 acc_val: 0.8000 time: 0.0285s\n",
            "Epoch: 0760 loss_train: 0.2164 acc_train: 0.9714 loss_val: 0.7061 acc_val: 0.8033 time: 0.0252s\n",
            "Epoch: 0761 loss_train: 0.2023 acc_train: 0.9500 loss_val: 0.7042 acc_val: 0.8033 time: 0.0249s\n",
            "Epoch: 0762 loss_train: 0.2377 acc_train: 0.9143 loss_val: 0.7035 acc_val: 0.8067 time: 0.0245s\n",
            "Epoch: 0763 loss_train: 0.2146 acc_train: 0.9643 loss_val: 0.7035 acc_val: 0.8067 time: 0.0343s\n",
            "Epoch: 0764 loss_train: 0.2470 acc_train: 0.9357 loss_val: 0.7050 acc_val: 0.8067 time: 0.0308s\n",
            "Epoch: 0765 loss_train: 0.2091 acc_train: 0.9786 loss_val: 0.7078 acc_val: 0.8033 time: 0.0252s\n",
            "Epoch: 0766 loss_train: 0.2337 acc_train: 0.9643 loss_val: 0.7105 acc_val: 0.8033 time: 0.0251s\n",
            "Epoch: 0767 loss_train: 0.2263 acc_train: 0.9571 loss_val: 0.7119 acc_val: 0.8033 time: 0.0247s\n",
            "Epoch: 0768 loss_train: 0.2188 acc_train: 0.9786 loss_val: 0.7139 acc_val: 0.8067 time: 0.0550s\n",
            "Epoch: 0769 loss_train: 0.2072 acc_train: 0.9929 loss_val: 0.7156 acc_val: 0.8067 time: 0.0665s\n",
            "Epoch: 0770 loss_train: 0.2091 acc_train: 0.9714 loss_val: 0.7160 acc_val: 0.8067 time: 0.0667s\n",
            "Epoch: 0771 loss_train: 0.2228 acc_train: 0.9786 loss_val: 0.7140 acc_val: 0.8033 time: 0.0316s\n",
            "Epoch: 0772 loss_train: 0.2078 acc_train: 0.9643 loss_val: 0.7115 acc_val: 0.8067 time: 0.0283s\n",
            "Epoch: 0773 loss_train: 0.2292 acc_train: 0.9500 loss_val: 0.7099 acc_val: 0.8000 time: 0.0241s\n",
            "Epoch: 0774 loss_train: 0.1657 acc_train: 0.9857 loss_val: 0.7088 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0775 loss_train: 0.2402 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.8033 time: 0.0273s\n",
            "Epoch: 0776 loss_train: 0.2446 acc_train: 0.9286 loss_val: 0.7082 acc_val: 0.8000 time: 0.0275s\n",
            "Epoch: 0777 loss_train: 0.2476 acc_train: 0.9643 loss_val: 0.7087 acc_val: 0.8033 time: 0.0238s\n",
            "Epoch: 0778 loss_train: 0.2078 acc_train: 0.9786 loss_val: 0.7110 acc_val: 0.8033 time: 0.0244s\n",
            "Epoch: 0779 loss_train: 0.2350 acc_train: 0.9714 loss_val: 0.7137 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0780 loss_train: 0.2022 acc_train: 0.9857 loss_val: 0.7147 acc_val: 0.8000 time: 0.0244s\n",
            "Epoch: 0781 loss_train: 0.2163 acc_train: 0.9643 loss_val: 0.7151 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0782 loss_train: 0.1996 acc_train: 0.9571 loss_val: 0.7142 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0783 loss_train: 0.2126 acc_train: 0.9571 loss_val: 0.7136 acc_val: 0.8000 time: 0.0295s\n",
            "Epoch: 0784 loss_train: 0.2428 acc_train: 0.9786 loss_val: 0.7126 acc_val: 0.7967 time: 0.0251s\n",
            "Epoch: 0785 loss_train: 0.2873 acc_train: 0.9286 loss_val: 0.7107 acc_val: 0.8000 time: 0.0250s\n",
            "Epoch: 0786 loss_train: 0.2427 acc_train: 0.9500 loss_val: 0.7089 acc_val: 0.8000 time: 0.0250s\n",
            "Epoch: 0787 loss_train: 0.2198 acc_train: 0.9714 loss_val: 0.7067 acc_val: 0.7933 time: 0.0245s\n",
            "Epoch: 0788 loss_train: 0.1939 acc_train: 0.9857 loss_val: 0.7049 acc_val: 0.7933 time: 0.0257s\n",
            "Epoch: 0789 loss_train: 0.2203 acc_train: 0.9500 loss_val: 0.7030 acc_val: 0.7933 time: 0.0267s\n",
            "Epoch: 0790 loss_train: 0.2032 acc_train: 0.9500 loss_val: 0.7026 acc_val: 0.7933 time: 0.0247s\n",
            "Epoch: 0791 loss_train: 0.2276 acc_train: 0.9643 loss_val: 0.7040 acc_val: 0.8000 time: 0.0301s\n",
            "Epoch: 0792 loss_train: 0.2033 acc_train: 0.9571 loss_val: 0.7054 acc_val: 0.8000 time: 0.0297s\n",
            "Epoch: 0793 loss_train: 0.2539 acc_train: 0.9571 loss_val: 0.7068 acc_val: 0.8033 time: 0.0310s\n",
            "Epoch: 0794 loss_train: 0.2227 acc_train: 0.9714 loss_val: 0.7080 acc_val: 0.8033 time: 0.0609s\n",
            "Epoch: 0795 loss_train: 0.1714 acc_train: 0.9929 loss_val: 0.7090 acc_val: 0.8033 time: 0.0315s\n",
            "Epoch: 0796 loss_train: 0.1955 acc_train: 0.9929 loss_val: 0.7107 acc_val: 0.8000 time: 0.0308s\n",
            "Epoch: 0797 loss_train: 0.2174 acc_train: 0.9357 loss_val: 0.7121 acc_val: 0.7967 time: 0.0463s\n",
            "Epoch: 0798 loss_train: 0.2175 acc_train: 0.9286 loss_val: 0.7128 acc_val: 0.8000 time: 0.0329s\n",
            "Epoch: 0799 loss_train: 0.2482 acc_train: 0.9429 loss_val: 0.7119 acc_val: 0.7967 time: 0.0270s\n",
            "Epoch: 0800 loss_train: 0.2283 acc_train: 0.9857 loss_val: 0.7093 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0801 loss_train: 0.2109 acc_train: 0.9857 loss_val: 0.7074 acc_val: 0.8000 time: 0.0284s\n",
            "Epoch: 0802 loss_train: 0.2120 acc_train: 0.9500 loss_val: 0.7065 acc_val: 0.8000 time: 0.0270s\n",
            "Epoch: 0803 loss_train: 0.2399 acc_train: 0.9429 loss_val: 0.7058 acc_val: 0.7967 time: 0.0290s\n",
            "Epoch: 0804 loss_train: 0.2225 acc_train: 0.9500 loss_val: 0.7054 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0805 loss_train: 0.2111 acc_train: 0.9643 loss_val: 0.7068 acc_val: 0.8000 time: 0.0251s\n",
            "Epoch: 0806 loss_train: 0.2208 acc_train: 0.9571 loss_val: 0.7086 acc_val: 0.8067 time: 0.0247s\n",
            "Epoch: 0807 loss_train: 0.1791 acc_train: 0.9714 loss_val: 0.7106 acc_val: 0.8067 time: 0.0247s\n",
            "Epoch: 0808 loss_train: 0.1955 acc_train: 0.9857 loss_val: 0.7123 acc_val: 0.8067 time: 0.0257s\n",
            "Epoch: 0809 loss_train: 0.1873 acc_train: 0.9786 loss_val: 0.7138 acc_val: 0.8067 time: 0.0247s\n",
            "Epoch: 0810 loss_train: 0.2233 acc_train: 0.9714 loss_val: 0.7153 acc_val: 0.8100 time: 0.0286s\n",
            "Epoch: 0811 loss_train: 0.1876 acc_train: 0.9786 loss_val: 0.7163 acc_val: 0.8067 time: 0.0247s\n",
            "Epoch: 0812 loss_train: 0.1940 acc_train: 0.9929 loss_val: 0.7170 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0813 loss_train: 0.2290 acc_train: 0.9357 loss_val: 0.7179 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0814 loss_train: 0.2313 acc_train: 0.9571 loss_val: 0.7185 acc_val: 0.7967 time: 0.0297s\n",
            "Epoch: 0815 loss_train: 0.1834 acc_train: 1.0000 loss_val: 0.7182 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0816 loss_train: 0.2400 acc_train: 0.9357 loss_val: 0.7178 acc_val: 0.7967 time: 0.0287s\n",
            "Epoch: 0817 loss_train: 0.2775 acc_train: 0.9429 loss_val: 0.7166 acc_val: 0.8000 time: 0.0298s\n",
            "Epoch: 0818 loss_train: 0.1694 acc_train: 0.9857 loss_val: 0.7152 acc_val: 0.8000 time: 0.0243s\n",
            "Epoch: 0819 loss_train: 0.2365 acc_train: 0.9429 loss_val: 0.7139 acc_val: 0.7967 time: 0.0259s\n",
            "Epoch: 0820 loss_train: 0.2178 acc_train: 0.9500 loss_val: 0.7133 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0821 loss_train: 0.2434 acc_train: 0.9571 loss_val: 0.7128 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0822 loss_train: 0.2386 acc_train: 0.9500 loss_val: 0.7132 acc_val: 0.8000 time: 0.0251s\n",
            "Epoch: 0823 loss_train: 0.2054 acc_train: 0.9571 loss_val: 0.7144 acc_val: 0.8000 time: 0.0254s\n",
            "Epoch: 0824 loss_train: 0.1832 acc_train: 0.9643 loss_val: 0.7159 acc_val: 0.8033 time: 0.0264s\n",
            "Epoch: 0825 loss_train: 0.2201 acc_train: 0.9571 loss_val: 0.7171 acc_val: 0.8033 time: 0.0288s\n",
            "Epoch: 0826 loss_train: 0.2178 acc_train: 0.9500 loss_val: 0.7156 acc_val: 0.8033 time: 0.0257s\n",
            "Epoch: 0827 loss_train: 0.2092 acc_train: 0.9571 loss_val: 0.7162 acc_val: 0.8000 time: 0.0281s\n",
            "Epoch: 0828 loss_train: 0.1980 acc_train: 0.9643 loss_val: 0.7166 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0829 loss_train: 0.1842 acc_train: 0.9643 loss_val: 0.7170 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0830 loss_train: 0.1803 acc_train: 0.9786 loss_val: 0.7175 acc_val: 0.7967 time: 0.0271s\n",
            "Epoch: 0831 loss_train: 0.2114 acc_train: 0.9500 loss_val: 0.7174 acc_val: 0.7933 time: 0.0261s\n",
            "Epoch: 0832 loss_train: 0.2070 acc_train: 0.9500 loss_val: 0.7168 acc_val: 0.7967 time: 0.0308s\n",
            "Epoch: 0833 loss_train: 0.2357 acc_train: 0.9714 loss_val: 0.7155 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0834 loss_train: 0.1960 acc_train: 0.9643 loss_val: 0.7141 acc_val: 0.7933 time: 0.0240s\n",
            "Epoch: 0835 loss_train: 0.2008 acc_train: 0.9714 loss_val: 0.7115 acc_val: 0.7967 time: 0.0241s\n",
            "Epoch: 0836 loss_train: 0.1815 acc_train: 0.9857 loss_val: 0.7098 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0837 loss_train: 0.2149 acc_train: 0.9500 loss_val: 0.7076 acc_val: 0.7967 time: 0.0250s\n",
            "Epoch: 0838 loss_train: 0.2484 acc_train: 0.9500 loss_val: 0.7042 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0839 loss_train: 0.2213 acc_train: 0.9500 loss_val: 0.7015 acc_val: 0.8033 time: 0.0252s\n",
            "Epoch: 0840 loss_train: 0.2182 acc_train: 0.9786 loss_val: 0.7012 acc_val: 0.8033 time: 0.0302s\n",
            "Epoch: 0841 loss_train: 0.2269 acc_train: 0.9571 loss_val: 0.7030 acc_val: 0.8033 time: 0.0288s\n",
            "Epoch: 0842 loss_train: 0.2426 acc_train: 0.9429 loss_val: 0.7052 acc_val: 0.8000 time: 0.0246s\n",
            "Epoch: 0843 loss_train: 0.2011 acc_train: 0.9714 loss_val: 0.7085 acc_val: 0.8000 time: 0.0244s\n",
            "Epoch: 0844 loss_train: 0.2051 acc_train: 0.9786 loss_val: 0.7120 acc_val: 0.7967 time: 0.0244s\n",
            "Epoch: 0845 loss_train: 0.2217 acc_train: 0.9500 loss_val: 0.7155 acc_val: 0.7967 time: 0.0248s\n",
            "Epoch: 0846 loss_train: 0.1914 acc_train: 0.9714 loss_val: 0.7187 acc_val: 0.7967 time: 0.0243s\n",
            "Epoch: 0847 loss_train: 0.1918 acc_train: 0.9786 loss_val: 0.7197 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0848 loss_train: 0.2459 acc_train: 0.9500 loss_val: 0.7179 acc_val: 0.7967 time: 0.0326s\n",
            "Epoch: 0849 loss_train: 0.2295 acc_train: 0.9714 loss_val: 0.7118 acc_val: 0.7967 time: 0.0253s\n",
            "Epoch: 0850 loss_train: 0.2213 acc_train: 0.9500 loss_val: 0.7051 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0851 loss_train: 0.2081 acc_train: 0.9500 loss_val: 0.7016 acc_val: 0.8000 time: 0.0264s\n",
            "Epoch: 0852 loss_train: 0.1904 acc_train: 0.9929 loss_val: 0.7000 acc_val: 0.8000 time: 0.0272s\n",
            "Epoch: 0853 loss_train: 0.2192 acc_train: 0.9714 loss_val: 0.6987 acc_val: 0.8000 time: 0.0258s\n",
            "Epoch: 0854 loss_train: 0.2322 acc_train: 0.9500 loss_val: 0.6986 acc_val: 0.8033 time: 0.0259s\n",
            "Epoch: 0855 loss_train: 0.2174 acc_train: 0.9500 loss_val: 0.7004 acc_val: 0.8000 time: 0.0281s\n",
            "Epoch: 0856 loss_train: 0.1872 acc_train: 0.9643 loss_val: 0.7041 acc_val: 0.8000 time: 0.0247s\n",
            "Epoch: 0857 loss_train: 0.2327 acc_train: 0.9357 loss_val: 0.7075 acc_val: 0.7967 time: 0.0245s\n",
            "Epoch: 0858 loss_train: 0.2176 acc_train: 0.9500 loss_val: 0.7115 acc_val: 0.7967 time: 0.0305s\n",
            "Epoch: 0859 loss_train: 0.2214 acc_train: 0.9429 loss_val: 0.7136 acc_val: 0.8033 time: 0.0245s\n",
            "Epoch: 0860 loss_train: 0.2398 acc_train: 0.9357 loss_val: 0.7149 acc_val: 0.8000 time: 0.0242s\n",
            "Epoch: 0861 loss_train: 0.1847 acc_train: 0.9786 loss_val: 0.7153 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0862 loss_train: 0.2117 acc_train: 0.9643 loss_val: 0.7147 acc_val: 0.8033 time: 0.0244s\n",
            "Epoch: 0863 loss_train: 0.2035 acc_train: 0.9714 loss_val: 0.7145 acc_val: 0.8000 time: 0.0354s\n",
            "Epoch: 0864 loss_train: 0.2375 acc_train: 0.9500 loss_val: 0.7134 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0865 loss_train: 0.2138 acc_train: 0.9714 loss_val: 0.7115 acc_val: 0.7967 time: 0.0283s\n",
            "Epoch: 0866 loss_train: 0.2340 acc_train: 0.9643 loss_val: 0.7090 acc_val: 0.7967 time: 0.0254s\n",
            "Epoch: 0867 loss_train: 0.2176 acc_train: 0.9786 loss_val: 0.7055 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0868 loss_train: 0.2280 acc_train: 0.9571 loss_val: 0.7025 acc_val: 0.7967 time: 0.0247s\n",
            "Epoch: 0869 loss_train: 0.2434 acc_train: 0.9500 loss_val: 0.7006 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0870 loss_train: 0.2108 acc_train: 0.9571 loss_val: 0.6992 acc_val: 0.7967 time: 0.0257s\n",
            "Epoch: 0871 loss_train: 0.2328 acc_train: 0.9429 loss_val: 0.6986 acc_val: 0.7967 time: 0.0308s\n",
            "Epoch: 0872 loss_train: 0.2080 acc_train: 0.9571 loss_val: 0.6996 acc_val: 0.8000 time: 0.0291s\n",
            "Epoch: 0873 loss_train: 0.2314 acc_train: 0.9286 loss_val: 0.7018 acc_val: 0.8000 time: 0.0248s\n",
            "Epoch: 0874 loss_train: 0.2353 acc_train: 0.9714 loss_val: 0.7048 acc_val: 0.8000 time: 0.0255s\n",
            "Epoch: 0875 loss_train: 0.1974 acc_train: 0.9500 loss_val: 0.7061 acc_val: 0.8033 time: 0.0251s\n",
            "Epoch: 0876 loss_train: 0.2393 acc_train: 0.9357 loss_val: 0.7064 acc_val: 0.8033 time: 0.0255s\n",
            "Epoch: 0877 loss_train: 0.1871 acc_train: 0.9714 loss_val: 0.7074 acc_val: 0.8033 time: 0.0257s\n",
            "Epoch: 0878 loss_train: 0.2554 acc_train: 0.9357 loss_val: 0.7094 acc_val: 0.8000 time: 0.0310s\n",
            "Epoch: 0879 loss_train: 0.1743 acc_train: 0.9786 loss_val: 0.7117 acc_val: 0.8000 time: 0.0282s\n",
            "Epoch: 0880 loss_train: 0.2483 acc_train: 0.9714 loss_val: 0.7136 acc_val: 0.7967 time: 0.0269s\n",
            "Epoch: 0881 loss_train: 0.1759 acc_train: 0.9714 loss_val: 0.7139 acc_val: 0.7967 time: 0.0264s\n",
            "Epoch: 0882 loss_train: 0.2302 acc_train: 0.9786 loss_val: 0.7128 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0883 loss_train: 0.1907 acc_train: 0.9786 loss_val: 0.7116 acc_val: 0.7967 time: 0.0267s\n",
            "Epoch: 0884 loss_train: 0.2125 acc_train: 0.9643 loss_val: 0.7088 acc_val: 0.8000 time: 0.0245s\n",
            "Epoch: 0885 loss_train: 0.2233 acc_train: 0.9571 loss_val: 0.7081 acc_val: 0.7967 time: 0.0249s\n",
            "Epoch: 0886 loss_train: 0.2187 acc_train: 0.9714 loss_val: 0.7064 acc_val: 0.7967 time: 0.0302s\n",
            "Epoch: 0887 loss_train: 0.2124 acc_train: 0.9500 loss_val: 0.7060 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0888 loss_train: 0.1756 acc_train: 0.9786 loss_val: 0.7041 acc_val: 0.8000 time: 0.0268s\n",
            "Epoch: 0889 loss_train: 0.2149 acc_train: 0.9714 loss_val: 0.7027 acc_val: 0.8000 time: 0.0301s\n",
            "Epoch: 0890 loss_train: 0.2134 acc_train: 0.9643 loss_val: 0.7028 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0891 loss_train: 0.2703 acc_train: 0.9357 loss_val: 0.7053 acc_val: 0.8000 time: 0.0269s\n",
            "Epoch: 0892 loss_train: 0.2131 acc_train: 0.9429 loss_val: 0.7074 acc_val: 0.8000 time: 0.0252s\n",
            "Epoch: 0893 loss_train: 0.2248 acc_train: 0.9357 loss_val: 0.7090 acc_val: 0.7967 time: 0.0273s\n",
            "Epoch: 0894 loss_train: 0.2308 acc_train: 0.9714 loss_val: 0.7104 acc_val: 0.7967 time: 0.0274s\n",
            "Epoch: 0895 loss_train: 0.1758 acc_train: 0.9714 loss_val: 0.7116 acc_val: 0.7967 time: 0.0260s\n",
            "Epoch: 0896 loss_train: 0.2091 acc_train: 0.9643 loss_val: 0.7107 acc_val: 0.7967 time: 0.0250s\n",
            "Epoch: 0897 loss_train: 0.2078 acc_train: 0.9643 loss_val: 0.7106 acc_val: 0.7967 time: 0.0243s\n",
            "Epoch: 0898 loss_train: 0.2256 acc_train: 0.9714 loss_val: 0.7106 acc_val: 0.8000 time: 0.0289s\n",
            "Epoch: 0899 loss_train: 0.1901 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.8000 time: 0.0283s\n",
            "Epoch: 0900 loss_train: 0.2181 acc_train: 0.9286 loss_val: 0.7125 acc_val: 0.8000 time: 0.0257s\n",
            "Epoch: 0901 loss_train: 0.2348 acc_train: 0.9500 loss_val: 0.7155 acc_val: 0.7967 time: 0.0288s\n",
            "Epoch: 0902 loss_train: 0.2318 acc_train: 0.9429 loss_val: 0.7204 acc_val: 0.7933 time: 0.0245s\n",
            "Epoch: 0903 loss_train: 0.2279 acc_train: 0.9571 loss_val: 0.7242 acc_val: 0.7933 time: 0.0252s\n",
            "Epoch: 0904 loss_train: 0.1918 acc_train: 0.9786 loss_val: 0.7266 acc_val: 0.7900 time: 0.0286s\n",
            "Epoch: 0905 loss_train: 0.2164 acc_train: 0.9571 loss_val: 0.7272 acc_val: 0.7933 time: 0.0245s\n",
            "Epoch: 0906 loss_train: 0.2238 acc_train: 0.9714 loss_val: 0.7239 acc_val: 0.7967 time: 0.0246s\n",
            "Epoch: 0907 loss_train: 0.2033 acc_train: 0.9643 loss_val: 0.7209 acc_val: 0.7967 time: 0.0255s\n",
            "Epoch: 0908 loss_train: 0.2255 acc_train: 0.9571 loss_val: 0.7192 acc_val: 0.7933 time: 0.0291s\n",
            "Epoch: 0909 loss_train: 0.2033 acc_train: 0.9929 loss_val: 0.7176 acc_val: 0.7933 time: 0.0248s\n",
            "Epoch: 0910 loss_train: 0.2069 acc_train: 0.9643 loss_val: 0.7159 acc_val: 0.7967 time: 0.0261s\n",
            "Epoch: 0911 loss_train: 0.2205 acc_train: 0.9500 loss_val: 0.7153 acc_val: 0.7933 time: 0.0269s\n",
            "Epoch: 0912 loss_train: 0.2005 acc_train: 0.9786 loss_val: 0.7146 acc_val: 0.7967 time: 0.0262s\n",
            "Epoch: 0913 loss_train: 0.1726 acc_train: 0.9786 loss_val: 0.7138 acc_val: 0.7967 time: 0.0313s\n",
            "Epoch: 0914 loss_train: 0.2196 acc_train: 0.9643 loss_val: 0.7142 acc_val: 0.8000 time: 0.0249s\n",
            "Epoch: 0915 loss_train: 0.2342 acc_train: 0.9571 loss_val: 0.7146 acc_val: 0.8033 time: 0.0290s\n",
            "Epoch: 0916 loss_train: 0.2118 acc_train: 0.9571 loss_val: 0.7158 acc_val: 0.8033 time: 0.0269s\n",
            "Epoch: 0917 loss_train: 0.2197 acc_train: 0.9643 loss_val: 0.7167 acc_val: 0.8033 time: 0.0250s\n",
            "Epoch: 0918 loss_train: 0.2048 acc_train: 0.9571 loss_val: 0.7181 acc_val: 0.8033 time: 0.0290s\n",
            "Epoch: 0919 loss_train: 0.1896 acc_train: 0.9571 loss_val: 0.7184 acc_val: 0.8033 time: 0.0303s\n",
            "Epoch: 0920 loss_train: 0.1977 acc_train: 0.9643 loss_val: 0.7167 acc_val: 0.8033 time: 0.0281s\n",
            "Epoch: 0921 loss_train: 0.1872 acc_train: 0.9786 loss_val: 0.7141 acc_val: 0.8033 time: 0.0251s\n",
            "Epoch: 0922 loss_train: 0.2341 acc_train: 0.9857 loss_val: 0.7118 acc_val: 0.8033 time: 0.0272s\n",
            "Epoch: 0923 loss_train: 0.2094 acc_train: 0.9571 loss_val: 0.7098 acc_val: 0.8033 time: 0.0308s\n",
            "Epoch: 0924 loss_train: 0.1744 acc_train: 0.9643 loss_val: 0.7090 acc_val: 0.8067 time: 0.0256s\n",
            "Epoch: 0925 loss_train: 0.2273 acc_train: 0.9857 loss_val: 0.7091 acc_val: 0.8067 time: 0.0265s\n",
            "Epoch: 0926 loss_train: 0.2428 acc_train: 0.9571 loss_val: 0.7104 acc_val: 0.8067 time: 0.0255s\n",
            "Epoch: 0927 loss_train: 0.2320 acc_train: 0.9500 loss_val: 0.7104 acc_val: 0.8067 time: 0.0255s\n",
            "Epoch: 0928 loss_train: 0.2303 acc_train: 0.9500 loss_val: 0.7108 acc_val: 0.8067 time: 0.0277s\n",
            "Epoch: 0929 loss_train: 0.2166 acc_train: 0.9643 loss_val: 0.7111 acc_val: 0.8067 time: 0.0289s\n",
            "Epoch: 0930 loss_train: 0.2334 acc_train: 0.9643 loss_val: 0.7113 acc_val: 0.8067 time: 0.0317s\n",
            "Epoch: 0931 loss_train: 0.2079 acc_train: 0.9357 loss_val: 0.7112 acc_val: 0.8067 time: 0.0260s\n",
            "Epoch: 0932 loss_train: 0.1997 acc_train: 0.9714 loss_val: 0.7109 acc_val: 0.8067 time: 0.0256s\n",
            "Epoch: 0933 loss_train: 0.2051 acc_train: 0.9857 loss_val: 0.7106 acc_val: 0.8067 time: 0.0367s\n",
            "Epoch: 0934 loss_train: 0.1789 acc_train: 0.9714 loss_val: 0.7104 acc_val: 0.8067 time: 0.0261s\n",
            "Epoch: 0935 loss_train: 0.1979 acc_train: 0.9929 loss_val: 0.7096 acc_val: 0.8033 time: 0.0251s\n",
            "Epoch: 0936 loss_train: 0.2234 acc_train: 0.9500 loss_val: 0.7092 acc_val: 0.8067 time: 0.0356s\n",
            "Epoch: 0937 loss_train: 0.2122 acc_train: 0.9643 loss_val: 0.7081 acc_val: 0.8033 time: 0.0341s\n",
            "Epoch: 0938 loss_train: 0.1707 acc_train: 0.9643 loss_val: 0.7076 acc_val: 0.8000 time: 0.0344s\n",
            "Epoch: 0939 loss_train: 0.2132 acc_train: 0.9786 loss_val: 0.7056 acc_val: 0.8067 time: 0.0339s\n",
            "Epoch: 0940 loss_train: 0.2146 acc_train: 0.9429 loss_val: 0.7037 acc_val: 0.8067 time: 0.0351s\n",
            "Epoch: 0941 loss_train: 0.2205 acc_train: 0.9500 loss_val: 0.7029 acc_val: 0.8067 time: 0.0317s\n",
            "Epoch: 0942 loss_train: 0.2270 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.8033 time: 0.0345s\n",
            "Epoch: 0943 loss_train: 0.1765 acc_train: 0.9786 loss_val: 0.7074 acc_val: 0.8000 time: 0.0318s\n",
            "Epoch: 0944 loss_train: 0.2091 acc_train: 0.9500 loss_val: 0.7099 acc_val: 0.8000 time: 0.0313s\n",
            "Epoch: 0945 loss_train: 0.2277 acc_train: 0.9500 loss_val: 0.7120 acc_val: 0.8000 time: 0.0326s\n",
            "Epoch: 0946 loss_train: 0.2569 acc_train: 0.9500 loss_val: 0.7124 acc_val: 0.8000 time: 0.0354s\n",
            "Epoch: 0947 loss_train: 0.2093 acc_train: 0.9857 loss_val: 0.7115 acc_val: 0.8000 time: 0.0350s\n",
            "Epoch: 0948 loss_train: 0.1716 acc_train: 0.9714 loss_val: 0.7096 acc_val: 0.8000 time: 0.0333s\n",
            "Epoch: 0949 loss_train: 0.2173 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0950 loss_train: 0.2119 acc_train: 0.9857 loss_val: 0.7081 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0951 loss_train: 0.2057 acc_train: 0.9714 loss_val: 0.7076 acc_val: 0.8000 time: 0.0315s\n",
            "Epoch: 0952 loss_train: 0.1796 acc_train: 0.9786 loss_val: 0.7049 acc_val: 0.8067 time: 0.0319s\n",
            "Epoch: 0953 loss_train: 0.2145 acc_train: 0.9571 loss_val: 0.7032 acc_val: 0.8067 time: 0.0320s\n",
            "Epoch: 0954 loss_train: 0.1962 acc_train: 0.9643 loss_val: 0.7024 acc_val: 0.8067 time: 0.0334s\n",
            "Epoch: 0955 loss_train: 0.1976 acc_train: 0.9714 loss_val: 0.7019 acc_val: 0.8067 time: 0.0319s\n",
            "Epoch: 0956 loss_train: 0.2084 acc_train: 0.9571 loss_val: 0.7016 acc_val: 0.8067 time: 0.0311s\n",
            "Epoch: 0957 loss_train: 0.2176 acc_train: 0.9286 loss_val: 0.7021 acc_val: 0.8033 time: 0.0358s\n",
            "Epoch: 0958 loss_train: 0.1727 acc_train: 0.9857 loss_val: 0.7033 acc_val: 0.8000 time: 0.0318s\n",
            "Epoch: 0959 loss_train: 0.2314 acc_train: 0.9643 loss_val: 0.7070 acc_val: 0.8000 time: 0.0319s\n",
            "Epoch: 0960 loss_train: 0.2083 acc_train: 0.9714 loss_val: 0.7088 acc_val: 0.8000 time: 0.0347s\n",
            "Epoch: 0961 loss_train: 0.2106 acc_train: 0.9571 loss_val: 0.7100 acc_val: 0.8000 time: 0.0344s\n",
            "Epoch: 0962 loss_train: 0.2066 acc_train: 0.9571 loss_val: 0.7100 acc_val: 0.8000 time: 0.0443s\n",
            "Epoch: 0963 loss_train: 0.2358 acc_train: 0.9500 loss_val: 0.7078 acc_val: 0.8033 time: 0.0331s\n",
            "Epoch: 0964 loss_train: 0.1986 acc_train: 0.9714 loss_val: 0.7064 acc_val: 0.8067 time: 0.0337s\n",
            "Epoch: 0965 loss_train: 0.2110 acc_train: 0.9643 loss_val: 0.7037 acc_val: 0.8067 time: 0.0334s\n",
            "Epoch: 0966 loss_train: 0.1999 acc_train: 0.9786 loss_val: 0.7010 acc_val: 0.8067 time: 0.0328s\n",
            "Epoch: 0967 loss_train: 0.1960 acc_train: 0.9643 loss_val: 0.6981 acc_val: 0.8033 time: 0.0322s\n",
            "Epoch: 0968 loss_train: 0.1955 acc_train: 0.9714 loss_val: 0.6963 acc_val: 0.8000 time: 0.0334s\n",
            "Epoch: 0969 loss_train: 0.2062 acc_train: 0.9500 loss_val: 0.6963 acc_val: 0.8000 time: 0.0327s\n",
            "Epoch: 0970 loss_train: 0.2199 acc_train: 0.9429 loss_val: 0.6982 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0971 loss_train: 0.2355 acc_train: 0.9500 loss_val: 0.7001 acc_val: 0.8033 time: 0.0345s\n",
            "Epoch: 0972 loss_train: 0.2344 acc_train: 0.9286 loss_val: 0.7012 acc_val: 0.8000 time: 0.0443s\n",
            "Epoch: 0973 loss_train: 0.1738 acc_train: 0.9786 loss_val: 0.7015 acc_val: 0.8000 time: 0.0324s\n",
            "Epoch: 0974 loss_train: 0.2314 acc_train: 0.9571 loss_val: 0.7024 acc_val: 0.8000 time: 0.0313s\n",
            "Epoch: 0975 loss_train: 0.2103 acc_train: 0.9714 loss_val: 0.7029 acc_val: 0.8000 time: 0.0316s\n",
            "Epoch: 0976 loss_train: 0.2001 acc_train: 0.9857 loss_val: 0.7033 acc_val: 0.8000 time: 0.0312s\n",
            "Epoch: 0977 loss_train: 0.2034 acc_train: 0.9643 loss_val: 0.7022 acc_val: 0.8000 time: 0.0318s\n",
            "Epoch: 0978 loss_train: 0.2233 acc_train: 0.9571 loss_val: 0.7004 acc_val: 0.8033 time: 0.0408s\n",
            "Epoch: 0979 loss_train: 0.2155 acc_train: 0.9571 loss_val: 0.6998 acc_val: 0.8033 time: 0.0330s\n",
            "Epoch: 0980 loss_train: 0.1790 acc_train: 0.9714 loss_val: 0.6989 acc_val: 0.8000 time: 0.0321s\n",
            "Epoch: 0981 loss_train: 0.2013 acc_train: 0.9857 loss_val: 0.6989 acc_val: 0.8033 time: 0.0339s\n",
            "Epoch: 0982 loss_train: 0.1823 acc_train: 0.9714 loss_val: 0.7002 acc_val: 0.8033 time: 0.0329s\n",
            "Epoch: 0983 loss_train: 0.1784 acc_train: 0.9857 loss_val: 0.7014 acc_val: 0.8067 time: 0.0320s\n",
            "Epoch: 0984 loss_train: 0.2361 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.8000 time: 0.0439s\n",
            "Epoch: 0985 loss_train: 0.1976 acc_train: 0.9571 loss_val: 0.7063 acc_val: 0.8000 time: 0.0340s\n",
            "Epoch: 0986 loss_train: 0.2139 acc_train: 0.9714 loss_val: 0.7078 acc_val: 0.8033 time: 0.0315s\n",
            "Epoch: 0987 loss_train: 0.1914 acc_train: 0.9571 loss_val: 0.7088 acc_val: 0.8033 time: 0.0328s\n",
            "Epoch: 0988 loss_train: 0.2456 acc_train: 0.9429 loss_val: 0.7097 acc_val: 0.8033 time: 0.0323s\n",
            "Epoch: 0989 loss_train: 0.2181 acc_train: 0.9643 loss_val: 0.7095 acc_val: 0.8033 time: 0.0354s\n",
            "Epoch: 0990 loss_train: 0.2132 acc_train: 0.9643 loss_val: 0.7084 acc_val: 0.8000 time: 0.0418s\n",
            "Epoch: 0991 loss_train: 0.1875 acc_train: 0.9786 loss_val: 0.7078 acc_val: 0.8033 time: 0.0378s\n",
            "Epoch: 0992 loss_train: 0.1727 acc_train: 0.9857 loss_val: 0.7073 acc_val: 0.8000 time: 0.0351s\n",
            "Epoch: 0993 loss_train: 0.2160 acc_train: 0.9500 loss_val: 0.7066 acc_val: 0.8000 time: 0.0337s\n",
            "Epoch: 0994 loss_train: 0.2255 acc_train: 0.9571 loss_val: 0.7066 acc_val: 0.7967 time: 0.0318s\n",
            "Epoch: 0995 loss_train: 0.2069 acc_train: 0.9714 loss_val: 0.7078 acc_val: 0.8000 time: 0.0319s\n",
            "Epoch: 0996 loss_train: 0.2266 acc_train: 0.9500 loss_val: 0.7096 acc_val: 0.8000 time: 0.0425s\n",
            "Epoch: 0997 loss_train: 0.2020 acc_train: 0.9714 loss_val: 0.7125 acc_val: 0.8033 time: 0.0323s\n",
            "Epoch: 0998 loss_train: 0.1939 acc_train: 0.9929 loss_val: 0.7146 acc_val: 0.8033 time: 0.0313s\n",
            "Epoch: 0999 loss_train: 0.2219 acc_train: 0.9643 loss_val: 0.7152 acc_val: 0.8000 time: 0.0310s\n",
            "Epoch: 1000 loss_train: 0.2162 acc_train: 0.9714 loss_val: 0.7148 acc_val: 0.8000 time: 0.0319s\n",
            "Optimization Finished!\n",
            "Total time slapsed: 32.2433s\n",
            "Test set results loss= 0.7869 accuracy= 0.7740\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}