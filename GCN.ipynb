{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tae-Hwanii/SEMI-SUPERVISED-CLASSIFICATION-WITH-GRAPH-CONVOLUTIONAL-NETWORKS/blob/main/GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "    # Module 클래스를 상속받는 GraphConvolution 클래스의 생성자\n",
        "    # in_feauters : 입력 특징의 수, out_features : 출력 특성의 수, bias : 편향 사용 여부\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # 상위 클래스(Module)의 생성자를 호출하여 초기화\n",
        "        self.in_features = in_features\n",
        "        # 입력 특성 수를 클래스 속성으로 저장\n",
        "        self.out_features = out_features\n",
        "        # 출력 특성의 수를 클래스 속성으로 저장\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        # 레이어의 가중치 행렬을 정의하고 Parameter로 감싸서 모델 파라미터로 만듦\n",
        "        # 크기는 (입력 특성의 수, 출력 특성의 수)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "            # 편향 사용 여부에 따라 편향 항을 정의하고 Parameter로 감싸서 모델 파라미터로 만듦\n",
        "            # 크기는 (출력 특성의 수)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            # 편향을 사용하지 않는 경우, bias를 None으로 등록하여 레이어에서 사용되지 않도록 힘\n",
        "        self.reset_parameters()\n",
        "        # reset_parameters() 함수를 호출하여 가중치와 편향을 초기화\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        # 가중치 초기화에 사용할 표준 편차를 계산\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # 가중치 행렬을 -stdv에서 stdv 사이의 균일한 랜덤 값으로 초기화\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "            # 편향이 존재하는 경우, 편향을 -stdv에서 stdv 사이의 균일한 랜덤 값으로 초기화\n",
        "\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # 그래프 합성곱 레이어의 순방향 연산을 정의하는 메서드\n",
        "        # input : 입력 데이터 (노드의 특성 행렬)\n",
        "        # adj : 그래프의 연결성 정보를 나타내는 희소 행렬 (인접 행렬)\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # 입력 데이터와 가중치 행렬을 곱하여 support를 계산\n",
        "        # support는 입력 데이터에 가중치를 적용한 결과로, 각 노드에 대한 새로운 특성 행렬\n",
        "        output = torch.spmm(adj, support)\n",
        "        # 희소 행렬과 support를 곱하여 그래프 합성곱 연산을 수행하고 output을 얻음\n",
        "        # output은 그래프에 따라 연결된 이웃 노드의 정보를 사용한 결과로, 각 노드에 대한 새로운 특성 행렬\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "            # 편향(bias)이 존재하는 경우, 결과에 편향을 더하고 반환\n",
        "        else:\n",
        "            return output\n",
        "            # 편향이 존재하지 않는 경우, 그래프 합성곱 연산 결과만 반환\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        # 객체를 문자열로 표현하는 메서드 정의\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "                + str(self.in_features) + ' -> ' \\\n",
        "                + str(self.out_features) + ')'\n",
        "        # 객체의 클래스 이름과 입력 및 출력 특성의 수를 포함한 문자열 반환"
      ],
      "metadata": {
        "id": "UquzPzJKgyHG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        # GCN 모델의 생성자\n",
        "        # nfeat : 입력 특성의 수, nhid : 은닉층의 특성 수, nclass : 출력 클래스 수, dropout : 드롭아웃 확률\n",
        "        super(GCN, self).__init__()\n",
        "        # 상위 클래스(nn.Moudle)의 생성자 호출\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        # 첫 번째 그래프 합성곱 레이어 (입력 특성 -> 은닉층)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        # 두 번째 그래프 합성곱 레이어 (은닉층 -> 출력 클래스)\n",
        "        self.dropout = dropout\n",
        "        # 드롭아웃 확률\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # 순방향 연산 메서드\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        # 첫 번째 그래프 합성곱 레이어를 통과한 후 ReLU 활성화 함수 적용\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # 드롭아웃 적용\n",
        "        x = self.gc2(x, adj)\n",
        "        # 두 번째 그래프 합성곱 레이어를 통과\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        # 소프트맥스 함수를 사용하여 출력을 확률 분포로 변환하고, 로그 확률값 반환"
      ],
      "metadata": {
        "id": "Le0wKbChotFs"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    # 레이블을 원-핫 인코딩하는 함수 정의\n",
        "    # labels : 입력으로 주어진 레이블 리스트\n",
        "    classes = set(labels)\n",
        "    # 주어진 레이블 리스트에서 고유한 클래스(레이블)를 추출하여 집합(set)으로 만듦\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    # 각 클래스(레이블)를 원-핫 인코딩으로 나타내는 딕셔너리(class_dict) 생성\n",
        "    # np.identity(len(classes))는 크기가 클래스 수와 같은 단위 행렬을 생성\n",
        "    # enumerate(classes)를 통해 클래스(레이블)를 순회하며 해당 클래스에 해당하는 단위 행렬 행을 추출\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    # labels 리스트의 각 레이블을 classes_dict에서 찾아 원-핫 인코딩으로 반환\n",
        "    # 반환된 결과를 Numpy 배열로 변환하고 데이터 타입을 int32로 설정\n",
        "    return labels_onehot\n",
        "    # 원-핫 인코딩된 레이블을 반환\n",
        "\n",
        "def load_data(path=\"/content/drive/MyDrive/cora/\", dataset=\"cora\"):\n",
        "    # 데이터를 로드하고 전처리하는 함수 정의\n",
        "    # path : 데이터 파일이 위치한 경로, dataset : 데이터셋 이름\n",
        "    print('Loading {} dataset. . .'.format(dataset))\n",
        "    # 데이터셋 로딩 메시지 출력\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    # 데이터셋 파일에서 데이터를 읽어와서 numpy 배열로 저장\n",
        "    # 데이터는 공백으로 구분되어 있고, dtype(str)을 사용하여 문자열로 읽음\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    # 데이터에서 특성(features) 부분을 추출하여 희소 특성 행렬(csr_matrix)로 변환\n",
        "    # 1번 열부터 마지막에서 1번 열까지를 선택하며, 데이터 타입을 float32로 설정\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "    # 데이터에서 레이블(Label) 부분을 추출하여 원-핫 인코딩된 레이블로 변환\n",
        "    # encode_onehot 함수를 사용하여 레이블을 원-핫 인코딩\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    # 데이터에서 인덱스 정보를 추출하여 NumPy 배열로 저장\n",
        "    # 인덱스는 정수형(int32)으로 저장됨\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    # 인덱스를 매핑하는 딕셔너리(idx_map) 생성\n",
        "    # 기존 인덱스를 새로운 인덱스로 매핑하는 역할을 함\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    # 그래프의 엣지 정보를 포함한 텍스트 파일을 읽어와서 NumPy 배열로 저장\n",
        "    # 데이터 타입은 정수형(int32)으로 설정\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    # 엣지 정보의 인덱스를 새로운 인덱스로 변환\n",
        "    # idx_map을 사용하여 엣지 정보의 인덱스를 새로운 인덱스로 매핑\n",
        "    # 엣지 정보의 배열을 펼친 다음, 새로운 인덱스로 변환하고 다시 원래 형태로 변환\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]),dtype=np.float32)\n",
        "    # 희소 고유값(coo) 행렬을 생성하여 그래프의 인접 행렬(adjacenct matrix)을 나타냄\n",
        "    # (edges.shape[0])은 엣지의 수를 나타냄\n",
        "    # (edges[:, 0], edges[:, i])은 각 엣지의 연결된 노드 인덱스를 나타냄\n",
        "    # shape=(labels.shape[0], labels.shape[0])은 인접 행렬의 크기를 설정\n",
        "    # dtype=np.float32은 데이터 타입을 부동소수점(float32)으로 설정\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    # 인접 행렬(adj)을 대칭으로 만듦\n",
        "    # adj.T는 인접 행렬을 전치한 것이며, '>' 연산자로 대칭성을 검사\n",
        "    # multiply 함수를 사용하여 대칭된 부분만 남기고, 대칭되지 않은 부분을 0으로 만듦\n",
        "\n",
        "    features = normalize(features)\n",
        "    # 특성 행렬(features)을 정규화(normalize)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    # 대칭 인접 행렬(adj)에 단위 행렬(eye)을 더하고 다시 정규화\n",
        "    # 이렇게 함으로써 자기 루프(self-loop)를 고려한 인접 행렬 생성\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "    # 훈련, 검증, 테스트 데이터의 인덱스를 설정\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    # 특성 행렬을 NumPy 배열로 변환하고, PyTorch의 Float Tensor로 변환\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    # 레이블을 NumPy 배열로 변환하고, PyTorch의 LongTensor로 변환\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    # 인접 행렬을 PyTorch의 희소 텐서(Sparse Tensor)로 변환\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "    # 인덱스들을 PyTorch의 LongTensor로 변환\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "    # 준비된 데이터를 반환\n",
        "\n",
        "def normalize(mx):\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    # 행렬의 각 행(row)의 합을 계산하여 배열(rowsum)로 저장\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    # 각 행의 합에 역수를 취하고, 0으로 나누는 경우(무한대 역수)에 대한 처리를 수행\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    # 대각 행렬(diagonal matrix)을 생성하고, 대각 성분에 역수 값을 포함\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    # 입력 행렬(mx)에 대각 행렬을 왼쪽에서 곱하여 행렬을 정규화\n",
        "    return mx\n",
        "    # 정규화된 행렬을 반환\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    # 모델의 출력(output)에서 가장 큰 값의 인덱스(max(1)[1])를 선택하여 예측(preds)을 구함\n",
        "    # 예측(preds)을 실제 레이블(labels)의 데이터 타입(type_as)으로 변환\n",
        "    correct = preds.eq(labels).double()\n",
        "    # 예측(preds)과 실제 레이블(labels)을 비교하여 일치하는 경우 1, 아닌 경우 0인 이진 값으로 변환\n",
        "    correct = correct.sum()\n",
        "    # 일치하는 값을 합산하여 정확한 예측의 수를 계산\n",
        "    return correct / len(labels)\n",
        "    # 정확한 예측의 수를 전체 레이블(labels)의 수로 나누어 정확도(accuracy)를 계산\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "     # 희소 행렬(sparse_mx)을 COO(coordinate List) 형식으로 변환하고 데이터 타입을 float32로 변환\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "     # COO 형식의 희소 행렬을 PyTorch 텐서로 변환하기 위해 인덱스(indices), 값(values), 크기(shape)를 추출\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "    # PyTorch의 희소 텐서(Sparse Tensor)를 생성하여 변환"
      ],
      "metadata": {
        "id": "jSkjVNdLsQ73"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#training setting\n",
        "\n",
        "no_cuda = False\n",
        "fastmode = False\n",
        "seed = 42\n",
        "epochs = 1000\n",
        "lr = 0.01\n",
        "weight_decay = 5e-4\n",
        "hidden = 16\n",
        "dropout = 0.5\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# 실험의 재현성을 위해 난수 시드 설정\n",
        "# NumPy 및 PyTorch의 난수 생성을 제어하기 위한 시드 설정\n",
        "\n",
        "#Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "#Model and optimizer\n",
        "model = GCN(nfeat = features.shape[1],                    # 입력 특성의 크기를 모델에 전달\n",
        "            nhid = hidden,                           # 은닉층의 크기 설정 (hidden units)\n",
        "            nclass = labels.max().item() + 1,             # 클래스 수를 레이블에서 결정\n",
        "            dropout = dropout)                       # 드롭아웃 확률 설정\n",
        "optimizer = optim.Adam(model.parameters(),              # Adam 최적화 알고리즘 사용\n",
        "                       lr = lr,                      # 학습률(learning rate) 설정\n",
        "                       weight_decay = weight_decay)  # 가중치 감쇠(L2 손실) 설정\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    # 현재 학습 epoch의 시작 시간 기록\n",
        "    model.train()\n",
        "    # 모델 학습 모드로 설정\n",
        "    optimizer.zero_grad()\n",
        "    # 기울기 초기화\n",
        "    output = model(features, adj)\n",
        "    # 모델에 입력 데이터와 인접 행렬을 전달하여 출력 예측 계산\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    # 학습 데이터에 대한 손실(loss) 및 정확도(accuracy) 계산\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    # 역전파 및 모델 파라미터 업데이트\n",
        "\n",
        "    if not fastmode:\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        # 만약 'fastmode'가 비활성화된 경우, 검증 데이터에 대한 평가 수행\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    # 검증 데이터에 대한 손실(loss) 및 정확도(accuracy) 계산\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    # 모델을 평가 모드로 설정\n",
        "    output = model(features, adj)\n",
        "    # 모델을 사용하여 출력 예측 계산\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    # 테스트 데이터에 대한 손실(loss)과 정확도(accuracy) 계산\n",
        "    print(\"Test set results\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "t_total = time.time()\n",
        "# 학습 시간 측정을 위한 변수 설정\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time slapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "DIX0dvSOC9oG",
        "outputId": "964a81c0-5d5b-4076-f581-427f27bce962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset. . .\n",
            "Epoch: 0001 loss_train: 1.9766 acc_train: 0.0786 loss_val: 1.9628 acc_val: 0.1233 time: 0.0248s\n",
            "Epoch: 0002 loss_train: 1.9648 acc_train: 0.1357 loss_val: 1.9517 acc_val: 0.1267 time: 0.0251s\n",
            "Epoch: 0003 loss_train: 1.9509 acc_train: 0.1500 loss_val: 1.9409 acc_val: 0.1267 time: 0.0264s\n",
            "Epoch: 0004 loss_train: 1.9393 acc_train: 0.1571 loss_val: 1.9302 acc_val: 0.1267 time: 0.0250s\n",
            "Epoch: 0005 loss_train: 1.9343 acc_train: 0.1286 loss_val: 1.9194 acc_val: 0.1267 time: 0.0258s\n",
            "Epoch: 0006 loss_train: 1.9101 acc_train: 0.2714 loss_val: 1.9080 acc_val: 0.1867 time: 0.0247s\n",
            "Epoch: 0007 loss_train: 1.9078 acc_train: 0.3071 loss_val: 1.8965 acc_val: 0.3433 time: 0.0252s\n",
            "Epoch: 0008 loss_train: 1.8821 acc_train: 0.3643 loss_val: 1.8845 acc_val: 0.3533 time: 0.0260s\n",
            "Epoch: 0009 loss_train: 1.8785 acc_train: 0.3214 loss_val: 1.8723 acc_val: 0.3567 time: 0.0332s\n",
            "Epoch: 0010 loss_train: 1.8662 acc_train: 0.3429 loss_val: 1.8597 acc_val: 0.3567 time: 0.0247s\n",
            "Epoch: 0011 loss_train: 1.8477 acc_train: 0.3214 loss_val: 1.8470 acc_val: 0.3567 time: 0.0260s\n",
            "Epoch: 0012 loss_train: 1.8578 acc_train: 0.2929 loss_val: 1.8345 acc_val: 0.3567 time: 0.0258s\n",
            "Epoch: 0013 loss_train: 1.8167 acc_train: 0.3643 loss_val: 1.8220 acc_val: 0.3567 time: 0.0283s\n",
            "Epoch: 0014 loss_train: 1.7891 acc_train: 0.4000 loss_val: 1.8094 acc_val: 0.3533 time: 0.0291s\n",
            "Epoch: 0015 loss_train: 1.8084 acc_train: 0.3500 loss_val: 1.7971 acc_val: 0.3533 time: 0.0261s\n",
            "Epoch: 0016 loss_train: 1.7778 acc_train: 0.4000 loss_val: 1.7851 acc_val: 0.3533 time: 0.0264s\n",
            "Epoch: 0017 loss_train: 1.7623 acc_train: 0.3500 loss_val: 1.7733 acc_val: 0.3533 time: 0.0282s\n",
            "Epoch: 0018 loss_train: 1.7657 acc_train: 0.3714 loss_val: 1.7620 acc_val: 0.3533 time: 0.0244s\n",
            "Epoch: 0019 loss_train: 1.7358 acc_train: 0.4214 loss_val: 1.7512 acc_val: 0.3567 time: 0.0259s\n",
            "Epoch: 0020 loss_train: 1.7336 acc_train: 0.4071 loss_val: 1.7408 acc_val: 0.3567 time: 0.0253s\n",
            "Epoch: 0021 loss_train: 1.7226 acc_train: 0.3571 loss_val: 1.7308 acc_val: 0.3567 time: 0.0256s\n",
            "Epoch: 0022 loss_train: 1.6868 acc_train: 0.3786 loss_val: 1.7213 acc_val: 0.3600 time: 0.0265s\n",
            "Epoch: 0023 loss_train: 1.6888 acc_train: 0.3786 loss_val: 1.7121 acc_val: 0.3667 time: 0.0296s\n",
            "Epoch: 0024 loss_train: 1.6883 acc_train: 0.4214 loss_val: 1.7035 acc_val: 0.3667 time: 0.0255s\n",
            "Epoch: 0025 loss_train: 1.6530 acc_train: 0.4000 loss_val: 1.6951 acc_val: 0.3667 time: 0.0241s\n",
            "Epoch: 0026 loss_train: 1.6434 acc_train: 0.4357 loss_val: 1.6870 acc_val: 0.3733 time: 0.0245s\n",
            "Epoch: 0027 loss_train: 1.6548 acc_train: 0.4071 loss_val: 1.6789 acc_val: 0.3833 time: 0.0255s\n",
            "Epoch: 0028 loss_train: 1.6562 acc_train: 0.4000 loss_val: 1.6708 acc_val: 0.3833 time: 0.0239s\n",
            "Epoch: 0029 loss_train: 1.6353 acc_train: 0.3643 loss_val: 1.6626 acc_val: 0.3833 time: 0.0246s\n",
            "Epoch: 0030 loss_train: 1.6189 acc_train: 0.4000 loss_val: 1.6544 acc_val: 0.3867 time: 0.0278s\n",
            "Epoch: 0031 loss_train: 1.6134 acc_train: 0.3571 loss_val: 1.6460 acc_val: 0.3867 time: 0.0240s\n",
            "Epoch: 0032 loss_train: 1.5751 acc_train: 0.4286 loss_val: 1.6376 acc_val: 0.3900 time: 0.0268s\n",
            "Epoch: 0033 loss_train: 1.5753 acc_train: 0.4286 loss_val: 1.6291 acc_val: 0.3967 time: 0.0236s\n",
            "Epoch: 0034 loss_train: 1.5389 acc_train: 0.4429 loss_val: 1.6204 acc_val: 0.4133 time: 0.0245s\n",
            "Epoch: 0035 loss_train: 1.5539 acc_train: 0.4143 loss_val: 1.6114 acc_val: 0.4133 time: 0.0245s\n",
            "Epoch: 0036 loss_train: 1.5293 acc_train: 0.4071 loss_val: 1.6023 acc_val: 0.4167 time: 0.0254s\n",
            "Epoch: 0037 loss_train: 1.5321 acc_train: 0.4571 loss_val: 1.5932 acc_val: 0.4167 time: 0.0300s\n",
            "Epoch: 0038 loss_train: 1.4988 acc_train: 0.4214 loss_val: 1.5839 acc_val: 0.4200 time: 0.0289s\n",
            "Epoch: 0039 loss_train: 1.4602 acc_train: 0.4500 loss_val: 1.5745 acc_val: 0.4267 time: 0.0267s\n",
            "Epoch: 0040 loss_train: 1.4657 acc_train: 0.4500 loss_val: 1.5649 acc_val: 0.4333 time: 0.0265s\n",
            "Epoch: 0041 loss_train: 1.4917 acc_train: 0.4571 loss_val: 1.5552 acc_val: 0.4333 time: 0.0238s\n",
            "Epoch: 0042 loss_train: 1.5082 acc_train: 0.4571 loss_val: 1.5455 acc_val: 0.4300 time: 0.0230s\n",
            "Epoch: 0043 loss_train: 1.4009 acc_train: 0.4857 loss_val: 1.5358 acc_val: 0.4300 time: 0.0237s\n",
            "Epoch: 0044 loss_train: 1.4239 acc_train: 0.4500 loss_val: 1.5260 acc_val: 0.4333 time: 0.0242s\n",
            "Epoch: 0045 loss_train: 1.4240 acc_train: 0.4714 loss_val: 1.5163 acc_val: 0.4333 time: 0.0248s\n",
            "Epoch: 0046 loss_train: 1.4383 acc_train: 0.4857 loss_val: 1.5066 acc_val: 0.4367 time: 0.0271s\n",
            "Epoch: 0047 loss_train: 1.3993 acc_train: 0.4929 loss_val: 1.4965 acc_val: 0.4433 time: 0.0277s\n",
            "Epoch: 0048 loss_train: 1.4028 acc_train: 0.4643 loss_val: 1.4862 acc_val: 0.4433 time: 0.0267s\n",
            "Epoch: 0049 loss_train: 1.3479 acc_train: 0.4929 loss_val: 1.4758 acc_val: 0.4500 time: 0.0258s\n",
            "Epoch: 0050 loss_train: 1.3268 acc_train: 0.5429 loss_val: 1.4649 acc_val: 0.4600 time: 0.0318s\n",
            "Epoch: 0051 loss_train: 1.2824 acc_train: 0.5214 loss_val: 1.4538 acc_val: 0.4633 time: 0.0248s\n",
            "Epoch: 0052 loss_train: 1.2999 acc_train: 0.4857 loss_val: 1.4425 acc_val: 0.4700 time: 0.0247s\n",
            "Epoch: 0053 loss_train: 1.2953 acc_train: 0.5786 loss_val: 1.4309 acc_val: 0.4767 time: 0.0227s\n",
            "Epoch: 0054 loss_train: 1.2916 acc_train: 0.5071 loss_val: 1.4194 acc_val: 0.4933 time: 0.0281s\n",
            "Epoch: 0055 loss_train: 1.2630 acc_train: 0.5500 loss_val: 1.4078 acc_val: 0.5133 time: 0.0251s\n",
            "Epoch: 0056 loss_train: 1.2342 acc_train: 0.5786 loss_val: 1.3957 acc_val: 0.5200 time: 0.0345s\n",
            "Epoch: 0057 loss_train: 1.2534 acc_train: 0.5286 loss_val: 1.3836 acc_val: 0.5200 time: 0.0389s\n",
            "Epoch: 0058 loss_train: 1.2133 acc_train: 0.5929 loss_val: 1.3716 acc_val: 0.5200 time: 0.0382s\n",
            "Epoch: 0059 loss_train: 1.2047 acc_train: 0.6429 loss_val: 1.3594 acc_val: 0.5333 time: 0.0348s\n",
            "Epoch: 0060 loss_train: 1.1803 acc_train: 0.6286 loss_val: 1.3474 acc_val: 0.5333 time: 0.0332s\n",
            "Epoch: 0061 loss_train: 1.2104 acc_train: 0.5714 loss_val: 1.3353 acc_val: 0.5367 time: 0.0325s\n",
            "Epoch: 0062 loss_train: 1.1532 acc_train: 0.6357 loss_val: 1.3234 acc_val: 0.5600 time: 0.0330s\n",
            "Epoch: 0063 loss_train: 1.1368 acc_train: 0.6500 loss_val: 1.3117 acc_val: 0.5767 time: 0.0318s\n",
            "Epoch: 0064 loss_train: 1.1611 acc_train: 0.5929 loss_val: 1.3006 acc_val: 0.5933 time: 0.0362s\n",
            "Epoch: 0065 loss_train: 1.1624 acc_train: 0.6500 loss_val: 1.2896 acc_val: 0.6100 time: 0.0392s\n",
            "Epoch: 0066 loss_train: 1.1170 acc_train: 0.6643 loss_val: 1.2791 acc_val: 0.6400 time: 0.0324s\n",
            "Epoch: 0067 loss_train: 1.0858 acc_train: 0.7000 loss_val: 1.2689 acc_val: 0.6567 time: 0.0313s\n",
            "Epoch: 0068 loss_train: 1.0818 acc_train: 0.7286 loss_val: 1.2586 acc_val: 0.6600 time: 0.0314s\n",
            "Epoch: 0069 loss_train: 1.0438 acc_train: 0.7071 loss_val: 1.2484 acc_val: 0.6600 time: 0.0304s\n",
            "Epoch: 0070 loss_train: 1.0223 acc_train: 0.7214 loss_val: 1.2383 acc_val: 0.6767 time: 0.0312s\n",
            "Epoch: 0071 loss_train: 1.0066 acc_train: 0.6857 loss_val: 1.2282 acc_val: 0.6933 time: 0.0351s\n",
            "Epoch: 0072 loss_train: 1.0157 acc_train: 0.7714 loss_val: 1.2181 acc_val: 0.7033 time: 0.0320s\n",
            "Epoch: 0073 loss_train: 1.0326 acc_train: 0.7357 loss_val: 1.2083 acc_val: 0.7067 time: 0.0316s\n",
            "Epoch: 0074 loss_train: 0.9627 acc_train: 0.7571 loss_val: 1.1985 acc_val: 0.7233 time: 0.0315s\n",
            "Epoch: 0075 loss_train: 0.9923 acc_train: 0.7357 loss_val: 1.1891 acc_val: 0.7233 time: 0.0321s\n",
            "Epoch: 0076 loss_train: 1.0257 acc_train: 0.7643 loss_val: 1.1787 acc_val: 0.7200 time: 0.0311s\n",
            "Epoch: 0077 loss_train: 0.9246 acc_train: 0.8143 loss_val: 1.1681 acc_val: 0.7267 time: 0.0354s\n",
            "Epoch: 0078 loss_train: 0.9754 acc_train: 0.7714 loss_val: 1.1577 acc_val: 0.7300 time: 0.0314s\n",
            "Epoch: 0079 loss_train: 0.9960 acc_train: 0.7357 loss_val: 1.1475 acc_val: 0.7300 time: 0.0383s\n",
            "Epoch: 0080 loss_train: 0.9115 acc_train: 0.8000 loss_val: 1.1371 acc_val: 0.7300 time: 0.0346s\n",
            "Epoch: 0081 loss_train: 1.0368 acc_train: 0.6929 loss_val: 1.1271 acc_val: 0.7333 time: 0.0319s\n",
            "Epoch: 0082 loss_train: 0.8728 acc_train: 0.7929 loss_val: 1.1178 acc_val: 0.7400 time: 0.0316s\n",
            "Epoch: 0083 loss_train: 0.9331 acc_train: 0.7643 loss_val: 1.1089 acc_val: 0.7367 time: 0.0436s\n",
            "Epoch: 0084 loss_train: 0.9038 acc_train: 0.8000 loss_val: 1.1005 acc_val: 0.7367 time: 0.0343s\n",
            "Epoch: 0085 loss_train: 0.8600 acc_train: 0.7857 loss_val: 1.0924 acc_val: 0.7500 time: 0.0320s\n",
            "Epoch: 0086 loss_train: 0.8725 acc_train: 0.8000 loss_val: 1.0843 acc_val: 0.7500 time: 0.0314s\n",
            "Epoch: 0087 loss_train: 0.8871 acc_train: 0.7643 loss_val: 1.0764 acc_val: 0.7533 time: 0.0318s\n",
            "Epoch: 0088 loss_train: 0.8286 acc_train: 0.7857 loss_val: 1.0686 acc_val: 0.7533 time: 0.0326s\n",
            "Epoch: 0089 loss_train: 0.8589 acc_train: 0.8429 loss_val: 1.0611 acc_val: 0.7567 time: 0.0386s\n",
            "Epoch: 0090 loss_train: 0.8003 acc_train: 0.8000 loss_val: 1.0537 acc_val: 0.7567 time: 0.0318s\n",
            "Epoch: 0091 loss_train: 0.7707 acc_train: 0.8714 loss_val: 1.0465 acc_val: 0.7633 time: 0.0394s\n",
            "Epoch: 0092 loss_train: 0.8968 acc_train: 0.8000 loss_val: 1.0398 acc_val: 0.7733 time: 0.0329s\n",
            "Epoch: 0093 loss_train: 0.8003 acc_train: 0.8286 loss_val: 1.0325 acc_val: 0.7700 time: 0.0330s\n",
            "Epoch: 0094 loss_train: 0.7893 acc_train: 0.8500 loss_val: 1.0253 acc_val: 0.7700 time: 0.0318s\n",
            "Epoch: 0095 loss_train: 0.7868 acc_train: 0.8429 loss_val: 1.0174 acc_val: 0.7700 time: 0.0372s\n",
            "Epoch: 0096 loss_train: 0.8644 acc_train: 0.8071 loss_val: 1.0093 acc_val: 0.7633 time: 0.0321s\n",
            "Epoch: 0097 loss_train: 0.7500 acc_train: 0.8357 loss_val: 1.0012 acc_val: 0.7700 time: 0.0330s\n",
            "Epoch: 0098 loss_train: 0.8255 acc_train: 0.7929 loss_val: 0.9936 acc_val: 0.7667 time: 0.0325s\n",
            "Epoch: 0099 loss_train: 0.7880 acc_train: 0.7929 loss_val: 0.9870 acc_val: 0.7633 time: 0.0312s\n",
            "Epoch: 0100 loss_train: 0.7519 acc_train: 0.7929 loss_val: 0.9813 acc_val: 0.7600 time: 0.0305s\n",
            "Epoch: 0101 loss_train: 0.7384 acc_train: 0.7929 loss_val: 0.9760 acc_val: 0.7667 time: 0.0379s\n",
            "Epoch: 0102 loss_train: 0.6859 acc_train: 0.8571 loss_val: 0.9709 acc_val: 0.7667 time: 0.0328s\n",
            "Epoch: 0103 loss_train: 0.7382 acc_train: 0.8286 loss_val: 0.9667 acc_val: 0.7700 time: 0.0335s\n",
            "Epoch: 0104 loss_train: 0.7200 acc_train: 0.8286 loss_val: 0.9629 acc_val: 0.7700 time: 0.0350s\n",
            "Epoch: 0105 loss_train: 0.7073 acc_train: 0.8429 loss_val: 0.9593 acc_val: 0.7667 time: 0.0323s\n",
            "Epoch: 0106 loss_train: 0.7261 acc_train: 0.8143 loss_val: 0.9548 acc_val: 0.7700 time: 0.0322s\n",
            "Epoch: 0107 loss_train: 0.7200 acc_train: 0.8071 loss_val: 0.9482 acc_val: 0.7700 time: 0.0431s\n",
            "Epoch: 0108 loss_train: 0.6836 acc_train: 0.8214 loss_val: 0.9405 acc_val: 0.7700 time: 0.0357s\n",
            "Epoch: 0109 loss_train: 0.7426 acc_train: 0.7929 loss_val: 0.9338 acc_val: 0.7733 time: 0.0331s\n",
            "Epoch: 0110 loss_train: 0.6783 acc_train: 0.8143 loss_val: 0.9282 acc_val: 0.7700 time: 0.0313s\n",
            "Epoch: 0111 loss_train: 0.7010 acc_train: 0.8214 loss_val: 0.9241 acc_val: 0.7700 time: 0.0322s\n",
            "Epoch: 0112 loss_train: 0.6423 acc_train: 0.8500 loss_val: 0.9200 acc_val: 0.7767 time: 0.0328s\n",
            "Epoch: 0113 loss_train: 0.6895 acc_train: 0.8286 loss_val: 0.9164 acc_val: 0.7733 time: 0.0348s\n",
            "Epoch: 0114 loss_train: 0.6976 acc_train: 0.8357 loss_val: 0.9135 acc_val: 0.7733 time: 0.0347s\n",
            "Epoch: 0115 loss_train: 0.6738 acc_train: 0.8286 loss_val: 0.9110 acc_val: 0.7733 time: 0.0341s\n",
            "Epoch: 0116 loss_train: 0.6924 acc_train: 0.8357 loss_val: 0.9084 acc_val: 0.7700 time: 0.0324s\n",
            "Epoch: 0117 loss_train: 0.6308 acc_train: 0.8357 loss_val: 0.9056 acc_val: 0.7767 time: 0.0336s\n",
            "Epoch: 0118 loss_train: 0.6098 acc_train: 0.8857 loss_val: 0.9019 acc_val: 0.7733 time: 0.0332s\n",
            "Epoch: 0119 loss_train: 0.6517 acc_train: 0.8357 loss_val: 0.8984 acc_val: 0.7767 time: 0.0409s\n",
            "Epoch: 0120 loss_train: 0.6700 acc_train: 0.8286 loss_val: 0.8943 acc_val: 0.7733 time: 0.0351s\n",
            "Epoch: 0121 loss_train: 0.5923 acc_train: 0.8929 loss_val: 0.8899 acc_val: 0.7700 time: 0.0327s\n",
            "Epoch: 0122 loss_train: 0.6101 acc_train: 0.8429 loss_val: 0.8856 acc_val: 0.7733 time: 0.0321s\n",
            "Epoch: 0123 loss_train: 0.5588 acc_train: 0.8857 loss_val: 0.8814 acc_val: 0.7767 time: 0.0310s\n",
            "Epoch: 0124 loss_train: 0.5868 acc_train: 0.8857 loss_val: 0.8786 acc_val: 0.7733 time: 0.0311s\n",
            "Epoch: 0125 loss_train: 0.6204 acc_train: 0.8500 loss_val: 0.8743 acc_val: 0.7733 time: 0.0386s\n",
            "Epoch: 0126 loss_train: 0.5911 acc_train: 0.8357 loss_val: 0.8700 acc_val: 0.7733 time: 0.0306s\n",
            "Epoch: 0127 loss_train: 0.6002 acc_train: 0.8571 loss_val: 0.8656 acc_val: 0.7833 time: 0.0329s\n",
            "Epoch: 0128 loss_train: 0.6422 acc_train: 0.8143 loss_val: 0.8607 acc_val: 0.7833 time: 0.0319s\n",
            "Epoch: 0129 loss_train: 0.5937 acc_train: 0.8857 loss_val: 0.8571 acc_val: 0.7900 time: 0.0327s\n",
            "Epoch: 0130 loss_train: 0.5488 acc_train: 0.9071 loss_val: 0.8549 acc_val: 0.7900 time: 0.0323s\n",
            "Epoch: 0131 loss_train: 0.5797 acc_train: 0.8857 loss_val: 0.8545 acc_val: 0.7967 time: 0.0380s\n",
            "Epoch: 0132 loss_train: 0.5357 acc_train: 0.8786 loss_val: 0.8544 acc_val: 0.7967 time: 0.0359s\n",
            "Epoch: 0133 loss_train: 0.5692 acc_train: 0.8786 loss_val: 0.8530 acc_val: 0.7900 time: 0.0323s\n",
            "Epoch: 0134 loss_train: 0.5415 acc_train: 0.8643 loss_val: 0.8521 acc_val: 0.7900 time: 0.0316s\n",
            "Epoch: 0135 loss_train: 0.5448 acc_train: 0.8714 loss_val: 0.8505 acc_val: 0.7933 time: 0.0318s\n",
            "Epoch: 0136 loss_train: 0.5432 acc_train: 0.9000 loss_val: 0.8466 acc_val: 0.7933 time: 0.0362s\n",
            "Epoch: 0137 loss_train: 0.5216 acc_train: 0.9071 loss_val: 0.8432 acc_val: 0.7967 time: 0.0369s\n",
            "Epoch: 0138 loss_train: 0.5351 acc_train: 0.9071 loss_val: 0.8379 acc_val: 0.8000 time: 0.0317s\n",
            "Epoch: 0139 loss_train: 0.5063 acc_train: 0.9143 loss_val: 0.8316 acc_val: 0.8000 time: 0.0339s\n",
            "Epoch: 0140 loss_train: 0.5446 acc_train: 0.8714 loss_val: 0.8262 acc_val: 0.7967 time: 0.0341s\n",
            "Epoch: 0141 loss_train: 0.4742 acc_train: 0.9000 loss_val: 0.8224 acc_val: 0.7967 time: 0.0335s\n",
            "Epoch: 0142 loss_train: 0.5230 acc_train: 0.8786 loss_val: 0.8202 acc_val: 0.8000 time: 0.0315s\n",
            "Epoch: 0143 loss_train: 0.5130 acc_train: 0.8429 loss_val: 0.8188 acc_val: 0.8033 time: 0.0326s\n",
            "Epoch: 0144 loss_train: 0.5089 acc_train: 0.9143 loss_val: 0.8186 acc_val: 0.8000 time: 0.0314s\n",
            "Epoch: 0145 loss_train: 0.4906 acc_train: 0.8929 loss_val: 0.8200 acc_val: 0.8067 time: 0.0356s\n",
            "Epoch: 0146 loss_train: 0.5338 acc_train: 0.8714 loss_val: 0.8233 acc_val: 0.8033 time: 0.0327s\n",
            "Epoch: 0147 loss_train: 0.4850 acc_train: 0.9357 loss_val: 0.8257 acc_val: 0.7933 time: 0.0318s\n",
            "Epoch: 0148 loss_train: 0.4995 acc_train: 0.8786 loss_val: 0.8266 acc_val: 0.7933 time: 0.0318s\n",
            "Epoch: 0149 loss_train: 0.4154 acc_train: 0.9500 loss_val: 0.8260 acc_val: 0.7933 time: 0.0357s\n",
            "Epoch: 0150 loss_train: 0.5234 acc_train: 0.8929 loss_val: 0.8240 acc_val: 0.7933 time: 0.0311s\n",
            "Epoch: 0151 loss_train: 0.5414 acc_train: 0.8786 loss_val: 0.8205 acc_val: 0.7933 time: 0.0356s\n",
            "Epoch: 0152 loss_train: 0.5392 acc_train: 0.8571 loss_val: 0.8179 acc_val: 0.7900 time: 0.0324s\n",
            "Epoch: 0153 loss_train: 0.4533 acc_train: 0.9000 loss_val: 0.8162 acc_val: 0.7933 time: 0.0310s\n",
            "Epoch: 0154 loss_train: 0.4744 acc_train: 0.9000 loss_val: 0.8145 acc_val: 0.7967 time: 0.0321s\n",
            "Epoch: 0155 loss_train: 0.4703 acc_train: 0.9214 loss_val: 0.8112 acc_val: 0.7933 time: 0.0350s\n",
            "Epoch: 0156 loss_train: 0.4882 acc_train: 0.8929 loss_val: 0.8090 acc_val: 0.7933 time: 0.0316s\n",
            "Epoch: 0157 loss_train: 0.5326 acc_train: 0.8929 loss_val: 0.8071 acc_val: 0.7933 time: 0.0323s\n",
            "Epoch: 0158 loss_train: 0.4737 acc_train: 0.9000 loss_val: 0.8053 acc_val: 0.7933 time: 0.0323s\n",
            "Epoch: 0159 loss_train: 0.4897 acc_train: 0.8929 loss_val: 0.8027 acc_val: 0.7967 time: 0.0317s\n",
            "Epoch: 0160 loss_train: 0.4705 acc_train: 0.9000 loss_val: 0.8007 acc_val: 0.8000 time: 0.0353s\n",
            "Epoch: 0161 loss_train: 0.4829 acc_train: 0.9000 loss_val: 0.7995 acc_val: 0.8000 time: 0.0405s\n",
            "Epoch: 0162 loss_train: 0.4834 acc_train: 0.9071 loss_val: 0.7994 acc_val: 0.8000 time: 0.0312s\n",
            "Epoch: 0163 loss_train: 0.4467 acc_train: 0.9214 loss_val: 0.7999 acc_val: 0.8000 time: 0.0316s\n",
            "Epoch: 0164 loss_train: 0.4471 acc_train: 0.9143 loss_val: 0.8016 acc_val: 0.8000 time: 0.0412s\n",
            "Epoch: 0165 loss_train: 0.4976 acc_train: 0.8857 loss_val: 0.8035 acc_val: 0.7867 time: 0.0410s\n",
            "Epoch: 0166 loss_train: 0.4659 acc_train: 0.9071 loss_val: 0.8078 acc_val: 0.7800 time: 0.0338s\n",
            "Epoch: 0167 loss_train: 0.4213 acc_train: 0.9071 loss_val: 0.8095 acc_val: 0.7800 time: 0.0364s\n",
            "Epoch: 0168 loss_train: 0.4234 acc_train: 0.9000 loss_val: 0.8095 acc_val: 0.7767 time: 0.0316s\n",
            "Epoch: 0169 loss_train: 0.4846 acc_train: 0.8929 loss_val: 0.8077 acc_val: 0.7800 time: 0.0344s\n",
            "Epoch: 0170 loss_train: 0.5143 acc_train: 0.8786 loss_val: 0.8028 acc_val: 0.7800 time: 0.0325s\n",
            "Epoch: 0171 loss_train: 0.4804 acc_train: 0.8929 loss_val: 0.7995 acc_val: 0.7833 time: 0.0327s\n",
            "Epoch: 0172 loss_train: 0.4214 acc_train: 0.9214 loss_val: 0.7956 acc_val: 0.7867 time: 0.0394s\n",
            "Epoch: 0173 loss_train: 0.3942 acc_train: 0.9429 loss_val: 0.7923 acc_val: 0.7900 time: 0.0391s\n",
            "Epoch: 0174 loss_train: 0.4305 acc_train: 0.9214 loss_val: 0.7901 acc_val: 0.7900 time: 0.0368s\n",
            "Epoch: 0175 loss_train: 0.4422 acc_train: 0.9214 loss_val: 0.7891 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0176 loss_train: 0.4483 acc_train: 0.9071 loss_val: 0.7886 acc_val: 0.7900 time: 0.0254s\n",
            "Epoch: 0177 loss_train: 0.4463 acc_train: 0.9286 loss_val: 0.7882 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0178 loss_train: 0.4684 acc_train: 0.9143 loss_val: 0.7871 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0179 loss_train: 0.4185 acc_train: 0.9429 loss_val: 0.7869 acc_val: 0.7900 time: 0.0248s\n",
            "Epoch: 0180 loss_train: 0.3948 acc_train: 0.9000 loss_val: 0.7876 acc_val: 0.7900 time: 0.0280s\n",
            "Epoch: 0181 loss_train: 0.4038 acc_train: 0.9286 loss_val: 0.7894 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0182 loss_train: 0.4073 acc_train: 0.8929 loss_val: 0.7911 acc_val: 0.7900 time: 0.0254s\n",
            "Epoch: 0183 loss_train: 0.4308 acc_train: 0.9000 loss_val: 0.7924 acc_val: 0.7900 time: 0.0281s\n",
            "Epoch: 0184 loss_train: 0.4109 acc_train: 0.9071 loss_val: 0.7929 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0185 loss_train: 0.3894 acc_train: 0.9286 loss_val: 0.7910 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0186 loss_train: 0.4798 acc_train: 0.9357 loss_val: 0.7873 acc_val: 0.7900 time: 0.0275s\n",
            "Epoch: 0187 loss_train: 0.3809 acc_train: 0.9214 loss_val: 0.7827 acc_val: 0.7900 time: 0.0286s\n",
            "Epoch: 0188 loss_train: 0.3828 acc_train: 0.9357 loss_val: 0.7785 acc_val: 0.7900 time: 0.0278s\n",
            "Epoch: 0189 loss_train: 0.3675 acc_train: 0.9429 loss_val: 0.7769 acc_val: 0.7900 time: 0.0252s\n",
            "Epoch: 0190 loss_train: 0.3661 acc_train: 0.9429 loss_val: 0.7756 acc_val: 0.7900 time: 0.0254s\n",
            "Epoch: 0191 loss_train: 0.4311 acc_train: 0.8786 loss_val: 0.7744 acc_val: 0.7900 time: 0.0247s\n",
            "Epoch: 0192 loss_train: 0.4893 acc_train: 0.9071 loss_val: 0.7728 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0193 loss_train: 0.3679 acc_train: 0.9214 loss_val: 0.7721 acc_val: 0.7867 time: 0.0296s\n",
            "Epoch: 0194 loss_train: 0.3825 acc_train: 0.9429 loss_val: 0.7717 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0195 loss_train: 0.3728 acc_train: 0.9500 loss_val: 0.7720 acc_val: 0.7867 time: 0.0310s\n",
            "Epoch: 0196 loss_train: 0.4514 acc_train: 0.8786 loss_val: 0.7728 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0197 loss_train: 0.4185 acc_train: 0.9000 loss_val: 0.7744 acc_val: 0.7867 time: 0.0282s\n",
            "Epoch: 0198 loss_train: 0.3750 acc_train: 0.9214 loss_val: 0.7758 acc_val: 0.7867 time: 0.0289s\n",
            "Epoch: 0199 loss_train: 0.3900 acc_train: 0.9000 loss_val: 0.7769 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0200 loss_train: 0.4802 acc_train: 0.8643 loss_val: 0.7772 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0201 loss_train: 0.4197 acc_train: 0.9357 loss_val: 0.7765 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0202 loss_train: 0.3863 acc_train: 0.9214 loss_val: 0.7737 acc_val: 0.7867 time: 0.0296s\n",
            "Epoch: 0203 loss_train: 0.4083 acc_train: 0.9143 loss_val: 0.7709 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0204 loss_train: 0.3563 acc_train: 0.9214 loss_val: 0.7682 acc_val: 0.7833 time: 0.0271s\n",
            "Epoch: 0205 loss_train: 0.3614 acc_train: 0.9214 loss_val: 0.7669 acc_val: 0.7833 time: 0.0289s\n",
            "Epoch: 0206 loss_train: 0.3820 acc_train: 0.9214 loss_val: 0.7654 acc_val: 0.7833 time: 0.0251s\n",
            "Epoch: 0207 loss_train: 0.3712 acc_train: 0.9071 loss_val: 0.7653 acc_val: 0.7833 time: 0.0259s\n",
            "Epoch: 0208 loss_train: 0.4342 acc_train: 0.9286 loss_val: 0.7665 acc_val: 0.7833 time: 0.0251s\n",
            "Epoch: 0209 loss_train: 0.3741 acc_train: 0.9357 loss_val: 0.7671 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0210 loss_train: 0.4140 acc_train: 0.9071 loss_val: 0.7667 acc_val: 0.7867 time: 0.0308s\n",
            "Epoch: 0211 loss_train: 0.4112 acc_train: 0.9357 loss_val: 0.7662 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0212 loss_train: 0.3672 acc_train: 0.9429 loss_val: 0.7669 acc_val: 0.7900 time: 0.0255s\n",
            "Epoch: 0213 loss_train: 0.4052 acc_train: 0.9071 loss_val: 0.7696 acc_val: 0.7867 time: 0.0265s\n",
            "Epoch: 0214 loss_train: 0.3183 acc_train: 0.9643 loss_val: 0.7720 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0215 loss_train: 0.3993 acc_train: 0.9286 loss_val: 0.7727 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0216 loss_train: 0.3671 acc_train: 0.9143 loss_val: 0.7746 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0217 loss_train: 0.3584 acc_train: 0.9286 loss_val: 0.7745 acc_val: 0.7867 time: 0.0273s\n",
            "Epoch: 0218 loss_train: 0.3738 acc_train: 0.9143 loss_val: 0.7716 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0219 loss_train: 0.3953 acc_train: 0.9143 loss_val: 0.7676 acc_val: 0.7833 time: 0.0268s\n",
            "Epoch: 0220 loss_train: 0.4013 acc_train: 0.8929 loss_val: 0.7660 acc_val: 0.7833 time: 0.0249s\n",
            "Epoch: 0221 loss_train: 0.3814 acc_train: 0.9214 loss_val: 0.7658 acc_val: 0.7833 time: 0.0250s\n",
            "Epoch: 0222 loss_train: 0.3479 acc_train: 0.9286 loss_val: 0.7665 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0223 loss_train: 0.3727 acc_train: 0.9571 loss_val: 0.7669 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0224 loss_train: 0.3752 acc_train: 0.9357 loss_val: 0.7681 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0225 loss_train: 0.3761 acc_train: 0.9429 loss_val: 0.7691 acc_val: 0.7833 time: 0.0297s\n",
            "Epoch: 0226 loss_train: 0.3479 acc_train: 0.9214 loss_val: 0.7702 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0227 loss_train: 0.3827 acc_train: 0.9143 loss_val: 0.7715 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0228 loss_train: 0.3772 acc_train: 0.9357 loss_val: 0.7719 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0229 loss_train: 0.3785 acc_train: 0.9214 loss_val: 0.7705 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0230 loss_train: 0.3761 acc_train: 0.9357 loss_val: 0.7695 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0231 loss_train: 0.3948 acc_train: 0.9214 loss_val: 0.7704 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0232 loss_train: 0.3853 acc_train: 0.8929 loss_val: 0.7721 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0233 loss_train: 0.3345 acc_train: 0.9429 loss_val: 0.7725 acc_val: 0.7867 time: 0.0334s\n",
            "Epoch: 0234 loss_train: 0.3956 acc_train: 0.9071 loss_val: 0.7705 acc_val: 0.7867 time: 0.0294s\n",
            "Epoch: 0235 loss_train: 0.3406 acc_train: 0.9286 loss_val: 0.7692 acc_val: 0.7867 time: 0.0254s\n",
            "Epoch: 0236 loss_train: 0.3809 acc_train: 0.9500 loss_val: 0.7693 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0237 loss_train: 0.3141 acc_train: 0.9429 loss_val: 0.7698 acc_val: 0.7867 time: 0.0234s\n",
            "Epoch: 0238 loss_train: 0.3537 acc_train: 0.9571 loss_val: 0.7707 acc_val: 0.7867 time: 0.0234s\n",
            "Epoch: 0239 loss_train: 0.4313 acc_train: 0.9357 loss_val: 0.7715 acc_val: 0.7867 time: 0.0234s\n",
            "Epoch: 0240 loss_train: 0.3100 acc_train: 0.9429 loss_val: 0.7726 acc_val: 0.7900 time: 0.0259s\n",
            "Epoch: 0241 loss_train: 0.3575 acc_train: 0.9286 loss_val: 0.7738 acc_val: 0.7900 time: 0.0296s\n",
            "Epoch: 0242 loss_train: 0.3863 acc_train: 0.9071 loss_val: 0.7742 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0243 loss_train: 0.3965 acc_train: 0.9286 loss_val: 0.7712 acc_val: 0.7867 time: 0.0288s\n",
            "Epoch: 0244 loss_train: 0.3220 acc_train: 0.9643 loss_val: 0.7682 acc_val: 0.7867 time: 0.0725s\n",
            "Epoch: 0245 loss_train: 0.3567 acc_train: 0.9143 loss_val: 0.7653 acc_val: 0.7867 time: 0.0997s\n",
            "Epoch: 0246 loss_train: 0.3699 acc_train: 0.9429 loss_val: 0.7639 acc_val: 0.7867 time: 0.0757s\n",
            "Epoch: 0247 loss_train: 0.3687 acc_train: 0.8786 loss_val: 0.7641 acc_val: 0.7867 time: 0.0614s\n",
            "Epoch: 0248 loss_train: 0.3726 acc_train: 0.9429 loss_val: 0.7667 acc_val: 0.7867 time: 0.0868s\n",
            "Epoch: 0249 loss_train: 0.3477 acc_train: 0.9357 loss_val: 0.7701 acc_val: 0.7867 time: 0.0262s\n",
            "Epoch: 0250 loss_train: 0.3539 acc_train: 0.9571 loss_val: 0.7714 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0251 loss_train: 0.3428 acc_train: 0.9357 loss_val: 0.7713 acc_val: 0.7833 time: 0.0249s\n",
            "Epoch: 0252 loss_train: 0.3526 acc_train: 0.9143 loss_val: 0.7695 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0253 loss_train: 0.3553 acc_train: 0.9429 loss_val: 0.7685 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0254 loss_train: 0.3599 acc_train: 0.9357 loss_val: 0.7684 acc_val: 0.7867 time: 0.0264s\n",
            "Epoch: 0255 loss_train: 0.3149 acc_train: 0.9214 loss_val: 0.7672 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0256 loss_train: 0.4049 acc_train: 0.9143 loss_val: 0.7645 acc_val: 0.7867 time: 0.0294s\n",
            "Epoch: 0257 loss_train: 0.3457 acc_train: 0.9357 loss_val: 0.7616 acc_val: 0.7833 time: 0.0257s\n",
            "Epoch: 0258 loss_train: 0.3366 acc_train: 0.9143 loss_val: 0.7609 acc_val: 0.7833 time: 0.0250s\n",
            "Epoch: 0259 loss_train: 0.3804 acc_train: 0.9143 loss_val: 0.7605 acc_val: 0.7833 time: 0.0296s\n",
            "Epoch: 0260 loss_train: 0.3395 acc_train: 0.9286 loss_val: 0.7605 acc_val: 0.7833 time: 0.0262s\n",
            "Epoch: 0261 loss_train: 0.3403 acc_train: 0.9571 loss_val: 0.7596 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0262 loss_train: 0.3287 acc_train: 0.9286 loss_val: 0.7590 acc_val: 0.7833 time: 0.0229s\n",
            "Epoch: 0263 loss_train: 0.3934 acc_train: 0.9214 loss_val: 0.7581 acc_val: 0.7833 time: 0.0280s\n",
            "Epoch: 0264 loss_train: 0.3480 acc_train: 0.9286 loss_val: 0.7588 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0265 loss_train: 0.2914 acc_train: 0.9643 loss_val: 0.7608 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0266 loss_train: 0.3597 acc_train: 0.9071 loss_val: 0.7633 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0267 loss_train: 0.3575 acc_train: 0.9214 loss_val: 0.7653 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0268 loss_train: 0.3052 acc_train: 0.9429 loss_val: 0.7665 acc_val: 0.7900 time: 0.0254s\n",
            "Epoch: 0269 loss_train: 0.3735 acc_train: 0.9286 loss_val: 0.7645 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0270 loss_train: 0.3430 acc_train: 0.9429 loss_val: 0.7608 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0271 loss_train: 0.3477 acc_train: 0.9000 loss_val: 0.7583 acc_val: 0.7867 time: 0.0341s\n",
            "Epoch: 0272 loss_train: 0.2759 acc_train: 0.9571 loss_val: 0.7568 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0273 loss_train: 0.3604 acc_train: 0.8929 loss_val: 0.7573 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0274 loss_train: 0.3385 acc_train: 0.9429 loss_val: 0.7589 acc_val: 0.7867 time: 0.0253s\n",
            "Epoch: 0275 loss_train: 0.3343 acc_train: 0.9571 loss_val: 0.7600 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0276 loss_train: 0.3563 acc_train: 0.9214 loss_val: 0.7603 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0277 loss_train: 0.3535 acc_train: 0.9357 loss_val: 0.7607 acc_val: 0.7867 time: 0.0232s\n",
            "Epoch: 0278 loss_train: 0.3468 acc_train: 0.9143 loss_val: 0.7598 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0279 loss_train: 0.3586 acc_train: 0.9214 loss_val: 0.7592 acc_val: 0.7867 time: 0.0266s\n",
            "Epoch: 0280 loss_train: 0.3252 acc_train: 0.9429 loss_val: 0.7577 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0281 loss_train: 0.3699 acc_train: 0.9357 loss_val: 0.7559 acc_val: 0.7867 time: 0.0267s\n",
            "Epoch: 0282 loss_train: 0.3720 acc_train: 0.9286 loss_val: 0.7544 acc_val: 0.7867 time: 0.0293s\n",
            "Epoch: 0283 loss_train: 0.2936 acc_train: 0.9500 loss_val: 0.7559 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0284 loss_train: 0.3043 acc_train: 0.9571 loss_val: 0.7579 acc_val: 0.7867 time: 0.0310s\n",
            "Epoch: 0285 loss_train: 0.3029 acc_train: 0.9500 loss_val: 0.7607 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0286 loss_train: 0.3605 acc_train: 0.9000 loss_val: 0.7615 acc_val: 0.7867 time: 0.0264s\n",
            "Epoch: 0287 loss_train: 0.3143 acc_train: 0.9357 loss_val: 0.7621 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0288 loss_train: 0.3562 acc_train: 0.9286 loss_val: 0.7609 acc_val: 0.7867 time: 0.0262s\n",
            "Epoch: 0289 loss_train: 0.3359 acc_train: 0.9357 loss_val: 0.7598 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0290 loss_train: 0.2878 acc_train: 0.9500 loss_val: 0.7593 acc_val: 0.7867 time: 0.0253s\n",
            "Epoch: 0291 loss_train: 0.3441 acc_train: 0.9357 loss_val: 0.7596 acc_val: 0.7867 time: 0.0254s\n",
            "Epoch: 0292 loss_train: 0.3114 acc_train: 0.9429 loss_val: 0.7602 acc_val: 0.7867 time: 0.0262s\n",
            "Epoch: 0293 loss_train: 0.3322 acc_train: 0.9143 loss_val: 0.7609 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0294 loss_train: 0.3731 acc_train: 0.9214 loss_val: 0.7611 acc_val: 0.7867 time: 0.0329s\n",
            "Epoch: 0295 loss_train: 0.3416 acc_train: 0.9357 loss_val: 0.7603 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0296 loss_train: 0.3523 acc_train: 0.9143 loss_val: 0.7583 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0297 loss_train: 0.3139 acc_train: 0.9500 loss_val: 0.7582 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0298 loss_train: 0.3561 acc_train: 0.9143 loss_val: 0.7589 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0299 loss_train: 0.3231 acc_train: 0.9500 loss_val: 0.7592 acc_val: 0.7933 time: 0.0267s\n",
            "Epoch: 0300 loss_train: 0.3292 acc_train: 0.9286 loss_val: 0.7597 acc_val: 0.7933 time: 0.0265s\n",
            "Epoch: 0301 loss_train: 0.3372 acc_train: 0.9500 loss_val: 0.7585 acc_val: 0.7933 time: 0.0305s\n",
            "Epoch: 0302 loss_train: 0.2988 acc_train: 0.9429 loss_val: 0.7560 acc_val: 0.7933 time: 0.0241s\n",
            "Epoch: 0303 loss_train: 0.3339 acc_train: 0.9214 loss_val: 0.7545 acc_val: 0.7867 time: 0.0272s\n",
            "Epoch: 0304 loss_train: 0.3735 acc_train: 0.9429 loss_val: 0.7542 acc_val: 0.7867 time: 0.0266s\n",
            "Epoch: 0305 loss_train: 0.3026 acc_train: 0.9571 loss_val: 0.7560 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0306 loss_train: 0.3198 acc_train: 0.9500 loss_val: 0.7574 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0307 loss_train: 0.3431 acc_train: 0.9357 loss_val: 0.7595 acc_val: 0.7867 time: 0.0280s\n",
            "Epoch: 0308 loss_train: 0.3119 acc_train: 0.9571 loss_val: 0.7608 acc_val: 0.7867 time: 0.0304s\n",
            "Epoch: 0309 loss_train: 0.3475 acc_train: 0.9286 loss_val: 0.7611 acc_val: 0.7900 time: 0.0235s\n",
            "Epoch: 0310 loss_train: 0.2968 acc_train: 0.9643 loss_val: 0.7606 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0311 loss_train: 0.3493 acc_train: 0.9357 loss_val: 0.7587 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0312 loss_train: 0.3128 acc_train: 0.9571 loss_val: 0.7562 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0313 loss_train: 0.3492 acc_train: 0.9429 loss_val: 0.7548 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0314 loss_train: 0.3031 acc_train: 0.9500 loss_val: 0.7538 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0315 loss_train: 0.3456 acc_train: 0.9071 loss_val: 0.7558 acc_val: 0.7900 time: 0.0263s\n",
            "Epoch: 0316 loss_train: 0.3342 acc_train: 0.9286 loss_val: 0.7589 acc_val: 0.7900 time: 0.0288s\n",
            "Epoch: 0317 loss_train: 0.2863 acc_train: 0.9571 loss_val: 0.7608 acc_val: 0.7900 time: 0.0254s\n",
            "Epoch: 0318 loss_train: 0.3220 acc_train: 0.9500 loss_val: 0.7622 acc_val: 0.7933 time: 0.0255s\n",
            "Epoch: 0319 loss_train: 0.3561 acc_train: 0.9214 loss_val: 0.7620 acc_val: 0.7900 time: 0.0309s\n",
            "Epoch: 0320 loss_train: 0.3523 acc_train: 0.9286 loss_val: 0.7617 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0321 loss_train: 0.2893 acc_train: 0.9429 loss_val: 0.7605 acc_val: 0.7867 time: 0.0251s\n",
            "Epoch: 0322 loss_train: 0.2867 acc_train: 0.9429 loss_val: 0.7591 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0323 loss_train: 0.2423 acc_train: 0.9500 loss_val: 0.7590 acc_val: 0.7867 time: 0.0306s\n",
            "Epoch: 0324 loss_train: 0.3010 acc_train: 0.9357 loss_val: 0.7590 acc_val: 0.7867 time: 0.0325s\n",
            "Epoch: 0325 loss_train: 0.3010 acc_train: 0.9643 loss_val: 0.7577 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0326 loss_train: 0.3042 acc_train: 0.9357 loss_val: 0.7555 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0327 loss_train: 0.3311 acc_train: 0.9071 loss_val: 0.7541 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0328 loss_train: 0.3010 acc_train: 0.9643 loss_val: 0.7521 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0329 loss_train: 0.3102 acc_train: 0.9214 loss_val: 0.7511 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0330 loss_train: 0.2933 acc_train: 0.9571 loss_val: 0.7514 acc_val: 0.7867 time: 0.0343s\n",
            "Epoch: 0331 loss_train: 0.3046 acc_train: 0.9286 loss_val: 0.7535 acc_val: 0.7867 time: 0.0293s\n",
            "Epoch: 0332 loss_train: 0.3120 acc_train: 0.9214 loss_val: 0.7551 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0333 loss_train: 0.2584 acc_train: 0.9714 loss_val: 0.7569 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0334 loss_train: 0.3076 acc_train: 0.9357 loss_val: 0.7593 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0335 loss_train: 0.3484 acc_train: 0.9214 loss_val: 0.7596 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0336 loss_train: 0.3194 acc_train: 0.9286 loss_val: 0.7600 acc_val: 0.7900 time: 0.0241s\n",
            "Epoch: 0337 loss_train: 0.2884 acc_train: 0.9500 loss_val: 0.7606 acc_val: 0.7900 time: 0.0255s\n",
            "Epoch: 0338 loss_train: 0.3368 acc_train: 0.9143 loss_val: 0.7598 acc_val: 0.7900 time: 0.0282s\n",
            "Epoch: 0339 loss_train: 0.3185 acc_train: 0.9286 loss_val: 0.7609 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0340 loss_train: 0.3068 acc_train: 0.9500 loss_val: 0.7626 acc_val: 0.7867 time: 0.0273s\n",
            "Epoch: 0341 loss_train: 0.3147 acc_train: 0.9500 loss_val: 0.7628 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0342 loss_train: 0.2640 acc_train: 0.9500 loss_val: 0.7640 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0343 loss_train: 0.2999 acc_train: 0.9214 loss_val: 0.7666 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0344 loss_train: 0.2868 acc_train: 0.9571 loss_val: 0.7668 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0345 loss_train: 0.3484 acc_train: 0.9357 loss_val: 0.7669 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0346 loss_train: 0.3075 acc_train: 0.9429 loss_val: 0.7658 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0347 loss_train: 0.2968 acc_train: 0.9357 loss_val: 0.7639 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0348 loss_train: 0.3127 acc_train: 0.9429 loss_val: 0.7605 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0349 loss_train: 0.3073 acc_train: 0.9286 loss_val: 0.7583 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0350 loss_train: 0.3158 acc_train: 0.9500 loss_val: 0.7547 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0351 loss_train: 0.2716 acc_train: 0.9714 loss_val: 0.7517 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0352 loss_train: 0.3389 acc_train: 0.9357 loss_val: 0.7495 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0353 loss_train: 0.3222 acc_train: 0.9357 loss_val: 0.7496 acc_val: 0.7867 time: 0.0236s\n",
            "Epoch: 0354 loss_train: 0.3188 acc_train: 0.9143 loss_val: 0.7507 acc_val: 0.7867 time: 0.0286s\n",
            "Epoch: 0355 loss_train: 0.3060 acc_train: 0.9500 loss_val: 0.7500 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0356 loss_train: 0.2776 acc_train: 0.9357 loss_val: 0.7509 acc_val: 0.7867 time: 0.0270s\n",
            "Epoch: 0357 loss_train: 0.2933 acc_train: 0.9286 loss_val: 0.7530 acc_val: 0.7867 time: 0.0251s\n",
            "Epoch: 0358 loss_train: 0.3085 acc_train: 0.9286 loss_val: 0.7547 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0359 loss_train: 0.3068 acc_train: 0.9571 loss_val: 0.7555 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0360 loss_train: 0.2998 acc_train: 0.9571 loss_val: 0.7555 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0361 loss_train: 0.2738 acc_train: 0.9429 loss_val: 0.7556 acc_val: 0.7867 time: 0.0259s\n",
            "Epoch: 0362 loss_train: 0.3606 acc_train: 0.9286 loss_val: 0.7565 acc_val: 0.7867 time: 0.0275s\n",
            "Epoch: 0363 loss_train: 0.3214 acc_train: 0.9286 loss_val: 0.7567 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0364 loss_train: 0.3115 acc_train: 0.9357 loss_val: 0.7569 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0365 loss_train: 0.2870 acc_train: 0.9500 loss_val: 0.7571 acc_val: 0.7833 time: 0.0239s\n",
            "Epoch: 0366 loss_train: 0.3026 acc_train: 0.9500 loss_val: 0.7568 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0367 loss_train: 0.2838 acc_train: 0.9500 loss_val: 0.7559 acc_val: 0.7833 time: 0.0293s\n",
            "Epoch: 0368 loss_train: 0.3068 acc_train: 0.9429 loss_val: 0.7549 acc_val: 0.7833 time: 0.0291s\n",
            "Epoch: 0369 loss_train: 0.2920 acc_train: 0.9500 loss_val: 0.7535 acc_val: 0.7867 time: 0.0267s\n",
            "Epoch: 0370 loss_train: 0.3341 acc_train: 0.9214 loss_val: 0.7525 acc_val: 0.7867 time: 0.0286s\n",
            "Epoch: 0371 loss_train: 0.3307 acc_train: 0.9214 loss_val: 0.7527 acc_val: 0.7867 time: 0.0284s\n",
            "Epoch: 0372 loss_train: 0.3043 acc_train: 0.9214 loss_val: 0.7528 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0373 loss_train: 0.2639 acc_train: 0.9429 loss_val: 0.7535 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0374 loss_train: 0.3298 acc_train: 0.9143 loss_val: 0.7554 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0375 loss_train: 0.3156 acc_train: 0.9429 loss_val: 0.7551 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0376 loss_train: 0.3244 acc_train: 0.9214 loss_val: 0.7544 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0377 loss_train: 0.2984 acc_train: 0.9143 loss_val: 0.7537 acc_val: 0.7867 time: 0.0302s\n",
            "Epoch: 0378 loss_train: 0.2989 acc_train: 0.9429 loss_val: 0.7544 acc_val: 0.7867 time: 0.0292s\n",
            "Epoch: 0379 loss_train: 0.3249 acc_train: 0.9571 loss_val: 0.7558 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0380 loss_train: 0.2707 acc_train: 0.9643 loss_val: 0.7577 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0381 loss_train: 0.2455 acc_train: 0.9786 loss_val: 0.7592 acc_val: 0.7867 time: 0.0259s\n",
            "Epoch: 0382 loss_train: 0.2850 acc_train: 0.9571 loss_val: 0.7600 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0383 loss_train: 0.2368 acc_train: 0.9643 loss_val: 0.7597 acc_val: 0.7833 time: 0.0246s\n",
            "Epoch: 0384 loss_train: 0.2920 acc_train: 0.9286 loss_val: 0.7575 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0385 loss_train: 0.3391 acc_train: 0.9143 loss_val: 0.7567 acc_val: 0.7867 time: 0.0293s\n",
            "Epoch: 0386 loss_train: 0.3133 acc_train: 0.9500 loss_val: 0.7552 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0387 loss_train: 0.2896 acc_train: 0.9429 loss_val: 0.7560 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0388 loss_train: 0.3323 acc_train: 0.8857 loss_val: 0.7591 acc_val: 0.7833 time: 0.0248s\n",
            "Epoch: 0389 loss_train: 0.2470 acc_train: 0.9571 loss_val: 0.7608 acc_val: 0.7833 time: 0.0263s\n",
            "Epoch: 0390 loss_train: 0.2823 acc_train: 0.9286 loss_val: 0.7605 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0391 loss_train: 0.2723 acc_train: 0.9429 loss_val: 0.7599 acc_val: 0.7900 time: 0.0281s\n",
            "Epoch: 0392 loss_train: 0.2851 acc_train: 0.9357 loss_val: 0.7584 acc_val: 0.7867 time: 0.0278s\n",
            "Epoch: 0393 loss_train: 0.2892 acc_train: 0.9571 loss_val: 0.7574 acc_val: 0.7867 time: 0.0304s\n",
            "Epoch: 0394 loss_train: 0.3080 acc_train: 0.9357 loss_val: 0.7550 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0395 loss_train: 0.3024 acc_train: 0.9429 loss_val: 0.7528 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0396 loss_train: 0.2902 acc_train: 0.9500 loss_val: 0.7506 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0397 loss_train: 0.3082 acc_train: 0.9429 loss_val: 0.7476 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0398 loss_train: 0.2617 acc_train: 0.9500 loss_val: 0.7474 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0399 loss_train: 0.2824 acc_train: 0.9071 loss_val: 0.7492 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0400 loss_train: 0.3012 acc_train: 0.9643 loss_val: 0.7503 acc_val: 0.7867 time: 0.0284s\n",
            "Epoch: 0401 loss_train: 0.3312 acc_train: 0.9071 loss_val: 0.7527 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0402 loss_train: 0.2403 acc_train: 0.9571 loss_val: 0.7566 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0403 loss_train: 0.2329 acc_train: 0.9571 loss_val: 0.7607 acc_val: 0.7833 time: 0.0281s\n",
            "Epoch: 0404 loss_train: 0.3103 acc_train: 0.9500 loss_val: 0.7641 acc_val: 0.7867 time: 0.0294s\n",
            "Epoch: 0405 loss_train: 0.2628 acc_train: 0.9714 loss_val: 0.7629 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0406 loss_train: 0.2764 acc_train: 0.9429 loss_val: 0.7601 acc_val: 0.7867 time: 0.0277s\n",
            "Epoch: 0407 loss_train: 0.2455 acc_train: 0.9571 loss_val: 0.7547 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0408 loss_train: 0.3093 acc_train: 0.9429 loss_val: 0.7500 acc_val: 0.7867 time: 0.0299s\n",
            "Epoch: 0409 loss_train: 0.2540 acc_train: 0.9714 loss_val: 0.7473 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0410 loss_train: 0.3283 acc_train: 0.9357 loss_val: 0.7466 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0411 loss_train: 0.3128 acc_train: 0.9429 loss_val: 0.7480 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0412 loss_train: 0.2892 acc_train: 0.9500 loss_val: 0.7500 acc_val: 0.7867 time: 0.0311s\n",
            "Epoch: 0413 loss_train: 0.2573 acc_train: 0.9500 loss_val: 0.7522 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0414 loss_train: 0.2858 acc_train: 0.9643 loss_val: 0.7542 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0415 loss_train: 0.2982 acc_train: 0.9643 loss_val: 0.7547 acc_val: 0.7867 time: 0.0286s\n",
            "Epoch: 0416 loss_train: 0.2900 acc_train: 0.9429 loss_val: 0.7553 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0417 loss_train: 0.2977 acc_train: 0.9500 loss_val: 0.7555 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0418 loss_train: 0.2829 acc_train: 0.9429 loss_val: 0.7565 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0419 loss_train: 0.2790 acc_train: 0.9571 loss_val: 0.7568 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0420 loss_train: 0.2584 acc_train: 0.9714 loss_val: 0.7567 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0421 loss_train: 0.3132 acc_train: 0.9214 loss_val: 0.7551 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0422 loss_train: 0.3050 acc_train: 0.9214 loss_val: 0.7545 acc_val: 0.7867 time: 0.0289s\n",
            "Epoch: 0423 loss_train: 0.2897 acc_train: 0.9357 loss_val: 0.7553 acc_val: 0.7867 time: 0.0341s\n",
            "Epoch: 0424 loss_train: 0.2981 acc_train: 0.9714 loss_val: 0.7534 acc_val: 0.7867 time: 0.0277s\n",
            "Epoch: 0425 loss_train: 0.2676 acc_train: 0.9429 loss_val: 0.7533 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0426 loss_train: 0.3077 acc_train: 0.9143 loss_val: 0.7552 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0427 loss_train: 0.2454 acc_train: 0.9571 loss_val: 0.7572 acc_val: 0.7833 time: 0.0252s\n",
            "Epoch: 0428 loss_train: 0.2951 acc_train: 0.9214 loss_val: 0.7602 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0429 loss_train: 0.2486 acc_train: 0.9357 loss_val: 0.7612 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0430 loss_train: 0.3253 acc_train: 0.9000 loss_val: 0.7601 acc_val: 0.7833 time: 0.0267s\n",
            "Epoch: 0431 loss_train: 0.2711 acc_train: 0.9357 loss_val: 0.7592 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0432 loss_train: 0.2653 acc_train: 0.9643 loss_val: 0.7572 acc_val: 0.7800 time: 0.0236s\n",
            "Epoch: 0433 loss_train: 0.2841 acc_train: 0.9357 loss_val: 0.7556 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0434 loss_train: 0.2973 acc_train: 0.9500 loss_val: 0.7553 acc_val: 0.7833 time: 0.0247s\n",
            "Epoch: 0435 loss_train: 0.3262 acc_train: 0.9429 loss_val: 0.7556 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0436 loss_train: 0.2686 acc_train: 0.9571 loss_val: 0.7535 acc_val: 0.7833 time: 0.0238s\n",
            "Epoch: 0437 loss_train: 0.3004 acc_train: 0.9571 loss_val: 0.7507 acc_val: 0.7867 time: 0.0273s\n",
            "Epoch: 0438 loss_train: 0.2811 acc_train: 0.9429 loss_val: 0.7489 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0439 loss_train: 0.2675 acc_train: 0.9571 loss_val: 0.7469 acc_val: 0.7867 time: 0.0351s\n",
            "Epoch: 0440 loss_train: 0.2672 acc_train: 0.9500 loss_val: 0.7446 acc_val: 0.7867 time: 0.0265s\n",
            "Epoch: 0441 loss_train: 0.3105 acc_train: 0.9000 loss_val: 0.7447 acc_val: 0.7867 time: 0.0258s\n",
            "Epoch: 0442 loss_train: 0.2444 acc_train: 0.9500 loss_val: 0.7450 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0443 loss_train: 0.3167 acc_train: 0.9214 loss_val: 0.7473 acc_val: 0.7900 time: 0.0284s\n",
            "Epoch: 0444 loss_train: 0.2532 acc_train: 0.9643 loss_val: 0.7490 acc_val: 0.7900 time: 0.0272s\n",
            "Epoch: 0445 loss_train: 0.2940 acc_train: 0.9429 loss_val: 0.7499 acc_val: 0.7900 time: 0.0243s\n",
            "Epoch: 0446 loss_train: 0.3133 acc_train: 0.9286 loss_val: 0.7508 acc_val: 0.7900 time: 0.0234s\n",
            "Epoch: 0447 loss_train: 0.2740 acc_train: 0.9500 loss_val: 0.7509 acc_val: 0.7900 time: 0.0236s\n",
            "Epoch: 0448 loss_train: 0.3083 acc_train: 0.9357 loss_val: 0.7527 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0449 loss_train: 0.2674 acc_train: 0.9714 loss_val: 0.7547 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0450 loss_train: 0.2929 acc_train: 0.9214 loss_val: 0.7572 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0451 loss_train: 0.3066 acc_train: 0.9571 loss_val: 0.7587 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0452 loss_train: 0.2617 acc_train: 0.9571 loss_val: 0.7585 acc_val: 0.7867 time: 0.0368s\n",
            "Epoch: 0453 loss_train: 0.2570 acc_train: 0.9643 loss_val: 0.7587 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0454 loss_train: 0.2862 acc_train: 0.9500 loss_val: 0.7592 acc_val: 0.7867 time: 0.0253s\n",
            "Epoch: 0455 loss_train: 0.2206 acc_train: 0.9857 loss_val: 0.7568 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0456 loss_train: 0.2843 acc_train: 0.9500 loss_val: 0.7540 acc_val: 0.7900 time: 0.0240s\n",
            "Epoch: 0457 loss_train: 0.2897 acc_train: 0.9286 loss_val: 0.7509 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0458 loss_train: 0.2225 acc_train: 0.9643 loss_val: 0.7501 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0459 loss_train: 0.2906 acc_train: 0.9500 loss_val: 0.7501 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0460 loss_train: 0.3053 acc_train: 0.9286 loss_val: 0.7494 acc_val: 0.7867 time: 0.0283s\n",
            "Epoch: 0461 loss_train: 0.2968 acc_train: 0.9357 loss_val: 0.7489 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0462 loss_train: 0.2983 acc_train: 0.9286 loss_val: 0.7513 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0463 loss_train: 0.2549 acc_train: 0.9500 loss_val: 0.7546 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0464 loss_train: 0.2808 acc_train: 0.9500 loss_val: 0.7572 acc_val: 0.7867 time: 0.0263s\n",
            "Epoch: 0465 loss_train: 0.2511 acc_train: 0.9643 loss_val: 0.7588 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0466 loss_train: 0.2314 acc_train: 0.9857 loss_val: 0.7591 acc_val: 0.7867 time: 0.0277s\n",
            "Epoch: 0467 loss_train: 0.2599 acc_train: 0.9571 loss_val: 0.7582 acc_val: 0.7900 time: 0.0257s\n",
            "Epoch: 0468 loss_train: 0.2654 acc_train: 0.9357 loss_val: 0.7577 acc_val: 0.7867 time: 0.0301s\n",
            "Epoch: 0469 loss_train: 0.2702 acc_train: 0.9714 loss_val: 0.7560 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0470 loss_train: 0.2969 acc_train: 0.9357 loss_val: 0.7547 acc_val: 0.7867 time: 0.0266s\n",
            "Epoch: 0471 loss_train: 0.2164 acc_train: 0.9714 loss_val: 0.7521 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0472 loss_train: 0.2603 acc_train: 0.9286 loss_val: 0.7496 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0473 loss_train: 0.2513 acc_train: 0.9571 loss_val: 0.7479 acc_val: 0.7867 time: 0.0266s\n",
            "Epoch: 0474 loss_train: 0.2741 acc_train: 0.9286 loss_val: 0.7480 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0475 loss_train: 0.2803 acc_train: 0.9500 loss_val: 0.7481 acc_val: 0.7900 time: 0.0286s\n",
            "Epoch: 0476 loss_train: 0.3120 acc_train: 0.9357 loss_val: 0.7495 acc_val: 0.7900 time: 0.0320s\n",
            "Epoch: 0477 loss_train: 0.2952 acc_train: 0.9429 loss_val: 0.7503 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0478 loss_train: 0.3086 acc_train: 0.9429 loss_val: 0.7494 acc_val: 0.7900 time: 0.0255s\n",
            "Epoch: 0479 loss_train: 0.2452 acc_train: 0.9714 loss_val: 0.7485 acc_val: 0.7900 time: 0.0272s\n",
            "Epoch: 0480 loss_train: 0.2542 acc_train: 0.9571 loss_val: 0.7472 acc_val: 0.7867 time: 0.0232s\n",
            "Epoch: 0481 loss_train: 0.2641 acc_train: 0.9286 loss_val: 0.7469 acc_val: 0.7867 time: 0.0263s\n",
            "Epoch: 0482 loss_train: 0.2587 acc_train: 0.9429 loss_val: 0.7481 acc_val: 0.7867 time: 0.0231s\n",
            "Epoch: 0483 loss_train: 0.2465 acc_train: 0.9643 loss_val: 0.7493 acc_val: 0.7867 time: 0.0266s\n",
            "Epoch: 0484 loss_train: 0.2857 acc_train: 0.9429 loss_val: 0.7512 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0485 loss_train: 0.2568 acc_train: 0.9643 loss_val: 0.7520 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0486 loss_train: 0.3232 acc_train: 0.9214 loss_val: 0.7526 acc_val: 0.7867 time: 0.0270s\n",
            "Epoch: 0487 loss_train: 0.2920 acc_train: 0.9286 loss_val: 0.7525 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0488 loss_train: 0.2229 acc_train: 0.9571 loss_val: 0.7517 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0489 loss_train: 0.2315 acc_train: 0.9429 loss_val: 0.7509 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0490 loss_train: 0.2765 acc_train: 0.9500 loss_val: 0.7509 acc_val: 0.7867 time: 0.0264s\n",
            "Epoch: 0491 loss_train: 0.2997 acc_train: 0.9429 loss_val: 0.7508 acc_val: 0.7867 time: 0.0289s\n",
            "Epoch: 0492 loss_train: 0.2661 acc_train: 0.9643 loss_val: 0.7491 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0493 loss_train: 0.2594 acc_train: 0.9571 loss_val: 0.7492 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0494 loss_train: 0.2920 acc_train: 0.9357 loss_val: 0.7491 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0495 loss_train: 0.2949 acc_train: 0.9286 loss_val: 0.7516 acc_val: 0.7867 time: 0.0258s\n",
            "Epoch: 0496 loss_train: 0.2955 acc_train: 0.9357 loss_val: 0.7535 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0497 loss_train: 0.2899 acc_train: 0.9429 loss_val: 0.7548 acc_val: 0.7867 time: 0.0233s\n",
            "Epoch: 0498 loss_train: 0.3263 acc_train: 0.9357 loss_val: 0.7567 acc_val: 0.7833 time: 0.0247s\n",
            "Epoch: 0499 loss_train: 0.2653 acc_train: 0.9643 loss_val: 0.7574 acc_val: 0.7833 time: 0.0278s\n",
            "Epoch: 0500 loss_train: 0.2843 acc_train: 0.9500 loss_val: 0.7581 acc_val: 0.7833 time: 0.0246s\n",
            "Epoch: 0501 loss_train: 0.2767 acc_train: 0.9357 loss_val: 0.7577 acc_val: 0.7867 time: 0.0257s\n",
            "Epoch: 0502 loss_train: 0.2620 acc_train: 0.9429 loss_val: 0.7570 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0503 loss_train: 0.2232 acc_train: 0.9714 loss_val: 0.7581 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0504 loss_train: 0.2579 acc_train: 0.9643 loss_val: 0.7583 acc_val: 0.7833 time: 0.0271s\n",
            "Epoch: 0505 loss_train: 0.3195 acc_train: 0.9357 loss_val: 0.7570 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0506 loss_train: 0.2719 acc_train: 0.9500 loss_val: 0.7565 acc_val: 0.7833 time: 0.0252s\n",
            "Epoch: 0507 loss_train: 0.2478 acc_train: 0.9643 loss_val: 0.7560 acc_val: 0.7867 time: 0.0284s\n",
            "Epoch: 0508 loss_train: 0.2994 acc_train: 0.9000 loss_val: 0.7558 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0509 loss_train: 0.2674 acc_train: 0.9500 loss_val: 0.7523 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0510 loss_train: 0.2643 acc_train: 0.9429 loss_val: 0.7485 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0511 loss_train: 0.2522 acc_train: 0.9643 loss_val: 0.7459 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0512 loss_train: 0.2682 acc_train: 0.9357 loss_val: 0.7468 acc_val: 0.7867 time: 0.0306s\n",
            "Epoch: 0513 loss_train: 0.2380 acc_train: 0.9714 loss_val: 0.7495 acc_val: 0.7867 time: 0.0280s\n",
            "Epoch: 0514 loss_train: 0.3177 acc_train: 0.9357 loss_val: 0.7531 acc_val: 0.7867 time: 0.0374s\n",
            "Epoch: 0515 loss_train: 0.3199 acc_train: 0.9357 loss_val: 0.7586 acc_val: 0.7800 time: 0.0349s\n",
            "Epoch: 0516 loss_train: 0.2647 acc_train: 0.9500 loss_val: 0.7640 acc_val: 0.7800 time: 0.0366s\n",
            "Epoch: 0517 loss_train: 0.2738 acc_train: 0.9429 loss_val: 0.7672 acc_val: 0.7800 time: 0.0358s\n",
            "Epoch: 0518 loss_train: 0.2836 acc_train: 0.9214 loss_val: 0.7697 acc_val: 0.7800 time: 0.0310s\n",
            "Epoch: 0519 loss_train: 0.2858 acc_train: 0.9786 loss_val: 0.7695 acc_val: 0.7767 time: 0.0316s\n",
            "Epoch: 0520 loss_train: 0.2718 acc_train: 0.9286 loss_val: 0.7674 acc_val: 0.7800 time: 0.0400s\n",
            "Epoch: 0521 loss_train: 0.2309 acc_train: 0.9643 loss_val: 0.7636 acc_val: 0.7800 time: 0.0313s\n",
            "Epoch: 0522 loss_train: 0.2266 acc_train: 0.9643 loss_val: 0.7593 acc_val: 0.7800 time: 0.0328s\n",
            "Epoch: 0523 loss_train: 0.2855 acc_train: 0.9571 loss_val: 0.7571 acc_val: 0.7800 time: 0.0334s\n",
            "Epoch: 0524 loss_train: 0.2971 acc_train: 0.9143 loss_val: 0.7549 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0525 loss_train: 0.2467 acc_train: 0.9643 loss_val: 0.7515 acc_val: 0.7833 time: 0.0316s\n",
            "Epoch: 0526 loss_train: 0.2369 acc_train: 0.9571 loss_val: 0.7495 acc_val: 0.7833 time: 0.0420s\n",
            "Epoch: 0527 loss_train: 0.2123 acc_train: 0.9714 loss_val: 0.7494 acc_val: 0.7833 time: 0.0306s\n",
            "Epoch: 0528 loss_train: 0.2455 acc_train: 0.9571 loss_val: 0.7494 acc_val: 0.7833 time: 0.0314s\n",
            "Epoch: 0529 loss_train: 0.2344 acc_train: 0.9357 loss_val: 0.7510 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0530 loss_train: 0.2278 acc_train: 0.9714 loss_val: 0.7533 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0531 loss_train: 0.3209 acc_train: 0.9357 loss_val: 0.7520 acc_val: 0.7867 time: 0.0316s\n",
            "Epoch: 0532 loss_train: 0.2593 acc_train: 0.9429 loss_val: 0.7517 acc_val: 0.7867 time: 0.0345s\n",
            "Epoch: 0533 loss_train: 0.2341 acc_train: 0.9571 loss_val: 0.7524 acc_val: 0.7867 time: 0.0308s\n",
            "Epoch: 0534 loss_train: 0.3043 acc_train: 0.9286 loss_val: 0.7535 acc_val: 0.7867 time: 0.0323s\n",
            "Epoch: 0535 loss_train: 0.2598 acc_train: 0.9286 loss_val: 0.7560 acc_val: 0.7833 time: 0.0309s\n",
            "Epoch: 0536 loss_train: 0.2718 acc_train: 0.9571 loss_val: 0.7590 acc_val: 0.7833 time: 0.0317s\n",
            "Epoch: 0537 loss_train: 0.2973 acc_train: 0.9286 loss_val: 0.7595 acc_val: 0.7833 time: 0.0343s\n",
            "Epoch: 0538 loss_train: 0.2632 acc_train: 0.9429 loss_val: 0.7591 acc_val: 0.7833 time: 0.0389s\n",
            "Epoch: 0539 loss_train: 0.2131 acc_train: 0.9714 loss_val: 0.7568 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0540 loss_train: 0.2406 acc_train: 0.9714 loss_val: 0.7548 acc_val: 0.7833 time: 0.0383s\n",
            "Epoch: 0541 loss_train: 0.2578 acc_train: 0.9571 loss_val: 0.7537 acc_val: 0.7867 time: 0.0392s\n",
            "Epoch: 0542 loss_train: 0.2496 acc_train: 0.9571 loss_val: 0.7516 acc_val: 0.7867 time: 0.0325s\n",
            "Epoch: 0543 loss_train: 0.2620 acc_train: 0.9429 loss_val: 0.7495 acc_val: 0.7867 time: 0.0311s\n",
            "Epoch: 0544 loss_train: 0.2400 acc_train: 0.9429 loss_val: 0.7493 acc_val: 0.7867 time: 0.0364s\n",
            "Epoch: 0545 loss_train: 0.2582 acc_train: 0.9500 loss_val: 0.7497 acc_val: 0.7867 time: 0.0306s\n",
            "Epoch: 0546 loss_train: 0.3075 acc_train: 0.9214 loss_val: 0.7512 acc_val: 0.7867 time: 0.0311s\n",
            "Epoch: 0547 loss_train: 0.2568 acc_train: 0.9714 loss_val: 0.7516 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0548 loss_train: 0.2647 acc_train: 0.9357 loss_val: 0.7536 acc_val: 0.7833 time: 0.0319s\n",
            "Epoch: 0549 loss_train: 0.2474 acc_train: 0.9714 loss_val: 0.7565 acc_val: 0.7833 time: 0.0313s\n",
            "Epoch: 0550 loss_train: 0.2718 acc_train: 0.9429 loss_val: 0.7586 acc_val: 0.7833 time: 0.0330s\n",
            "Epoch: 0551 loss_train: 0.2669 acc_train: 0.9357 loss_val: 0.7566 acc_val: 0.7833 time: 0.0313s\n",
            "Epoch: 0552 loss_train: 0.2976 acc_train: 0.9214 loss_val: 0.7540 acc_val: 0.7833 time: 0.0305s\n",
            "Epoch: 0553 loss_train: 0.2724 acc_train: 0.9571 loss_val: 0.7496 acc_val: 0.7867 time: 0.0312s\n",
            "Epoch: 0554 loss_train: 0.2322 acc_train: 0.9714 loss_val: 0.7472 acc_val: 0.7867 time: 0.0346s\n",
            "Epoch: 0555 loss_train: 0.2200 acc_train: 0.9714 loss_val: 0.7454 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0556 loss_train: 0.2694 acc_train: 0.9286 loss_val: 0.7462 acc_val: 0.7867 time: 0.0346s\n",
            "Epoch: 0557 loss_train: 0.2481 acc_train: 0.9714 loss_val: 0.7471 acc_val: 0.7867 time: 0.0336s\n",
            "Epoch: 0558 loss_train: 0.3021 acc_train: 0.9429 loss_val: 0.7493 acc_val: 0.7867 time: 0.0318s\n",
            "Epoch: 0559 loss_train: 0.2213 acc_train: 0.9857 loss_val: 0.7523 acc_val: 0.7867 time: 0.0312s\n",
            "Epoch: 0560 loss_train: 0.2428 acc_train: 0.9500 loss_val: 0.7569 acc_val: 0.7833 time: 0.0322s\n",
            "Epoch: 0561 loss_train: 0.2612 acc_train: 0.9571 loss_val: 0.7605 acc_val: 0.7833 time: 0.0317s\n",
            "Epoch: 0562 loss_train: 0.2720 acc_train: 0.9500 loss_val: 0.7614 acc_val: 0.7833 time: 0.0343s\n",
            "Epoch: 0563 loss_train: 0.2802 acc_train: 0.9429 loss_val: 0.7585 acc_val: 0.7833 time: 0.0327s\n",
            "Epoch: 0564 loss_train: 0.2513 acc_train: 0.9714 loss_val: 0.7534 acc_val: 0.7833 time: 0.0361s\n",
            "Epoch: 0565 loss_train: 0.2222 acc_train: 0.9714 loss_val: 0.7493 acc_val: 0.7833 time: 0.0312s\n",
            "Epoch: 0566 loss_train: 0.2450 acc_train: 0.9643 loss_val: 0.7455 acc_val: 0.7867 time: 0.0322s\n",
            "Epoch: 0567 loss_train: 0.2911 acc_train: 0.9286 loss_val: 0.7424 acc_val: 0.7867 time: 0.0308s\n",
            "Epoch: 0568 loss_train: 0.2512 acc_train: 0.9500 loss_val: 0.7403 acc_val: 0.7867 time: 0.0344s\n",
            "Epoch: 0569 loss_train: 0.2588 acc_train: 0.9357 loss_val: 0.7401 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0570 loss_train: 0.2279 acc_train: 0.9500 loss_val: 0.7411 acc_val: 0.7867 time: 0.0401s\n",
            "Epoch: 0571 loss_train: 0.2906 acc_train: 0.9286 loss_val: 0.7433 acc_val: 0.7867 time: 0.0320s\n",
            "Epoch: 0572 loss_train: 0.2357 acc_train: 0.9714 loss_val: 0.7435 acc_val: 0.7867 time: 0.0312s\n",
            "Epoch: 0573 loss_train: 0.2287 acc_train: 0.9714 loss_val: 0.7447 acc_val: 0.7867 time: 0.0311s\n",
            "Epoch: 0574 loss_train: 0.2463 acc_train: 0.9500 loss_val: 0.7476 acc_val: 0.7867 time: 0.0385s\n",
            "Epoch: 0575 loss_train: 0.2612 acc_train: 0.9571 loss_val: 0.7490 acc_val: 0.7833 time: 0.0323s\n",
            "Epoch: 0576 loss_train: 0.2059 acc_train: 0.9786 loss_val: 0.7481 acc_val: 0.7833 time: 0.0320s\n",
            "Epoch: 0577 loss_train: 0.2135 acc_train: 0.9643 loss_val: 0.7455 acc_val: 0.7833 time: 0.0322s\n",
            "Epoch: 0578 loss_train: 0.2482 acc_train: 0.9357 loss_val: 0.7413 acc_val: 0.7833 time: 0.0317s\n",
            "Epoch: 0579 loss_train: 0.2759 acc_train: 0.9500 loss_val: 0.7382 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0580 loss_train: 0.2525 acc_train: 0.9500 loss_val: 0.7362 acc_val: 0.7867 time: 0.0394s\n",
            "Epoch: 0581 loss_train: 0.2398 acc_train: 0.9429 loss_val: 0.7367 acc_val: 0.7867 time: 0.0323s\n",
            "Epoch: 0582 loss_train: 0.2637 acc_train: 0.9429 loss_val: 0.7372 acc_val: 0.7867 time: 0.0360s\n",
            "Epoch: 0583 loss_train: 0.2258 acc_train: 0.9643 loss_val: 0.7378 acc_val: 0.7867 time: 0.0379s\n",
            "Epoch: 0584 loss_train: 0.2209 acc_train: 0.9643 loss_val: 0.7399 acc_val: 0.7867 time: 0.0320s\n",
            "Epoch: 0585 loss_train: 0.3032 acc_train: 0.9214 loss_val: 0.7423 acc_val: 0.7833 time: 0.0311s\n",
            "Epoch: 0586 loss_train: 0.2406 acc_train: 0.9571 loss_val: 0.7425 acc_val: 0.7867 time: 0.0368s\n",
            "Epoch: 0587 loss_train: 0.2032 acc_train: 0.9714 loss_val: 0.7411 acc_val: 0.7867 time: 0.0311s\n",
            "Epoch: 0588 loss_train: 0.2330 acc_train: 0.9571 loss_val: 0.7380 acc_val: 0.7867 time: 0.0365s\n",
            "Epoch: 0589 loss_train: 0.2448 acc_train: 0.9429 loss_val: 0.7356 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0590 loss_train: 0.2307 acc_train: 0.9500 loss_val: 0.7348 acc_val: 0.7867 time: 0.0316s\n",
            "Epoch: 0591 loss_train: 0.2031 acc_train: 0.9643 loss_val: 0.7354 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0592 loss_train: 0.2880 acc_train: 0.9071 loss_val: 0.7376 acc_val: 0.7867 time: 0.0419s\n",
            "Epoch: 0593 loss_train: 0.2928 acc_train: 0.9214 loss_val: 0.7394 acc_val: 0.7833 time: 0.0330s\n",
            "Epoch: 0594 loss_train: 0.2403 acc_train: 0.9429 loss_val: 0.7404 acc_val: 0.7800 time: 0.0315s\n",
            "Epoch: 0595 loss_train: 0.2234 acc_train: 0.9643 loss_val: 0.7410 acc_val: 0.7833 time: 0.0316s\n",
            "Epoch: 0596 loss_train: 0.2292 acc_train: 0.9571 loss_val: 0.7415 acc_val: 0.7867 time: 0.0320s\n",
            "Epoch: 0597 loss_train: 0.2432 acc_train: 0.9714 loss_val: 0.7415 acc_val: 0.7867 time: 0.0314s\n",
            "Epoch: 0598 loss_train: 0.2734 acc_train: 0.9571 loss_val: 0.7408 acc_val: 0.7833 time: 0.0389s\n",
            "Epoch: 0599 loss_train: 0.1940 acc_train: 0.9643 loss_val: 0.7390 acc_val: 0.7833 time: 0.0401s\n",
            "Epoch: 0600 loss_train: 0.2204 acc_train: 0.9714 loss_val: 0.7386 acc_val: 0.7833 time: 0.0312s\n",
            "Epoch: 0601 loss_train: 0.2631 acc_train: 0.9286 loss_val: 0.7384 acc_val: 0.7833 time: 0.0308s\n",
            "Epoch: 0602 loss_train: 0.2230 acc_train: 0.9500 loss_val: 0.7397 acc_val: 0.7833 time: 0.0311s\n",
            "Epoch: 0603 loss_train: 0.2345 acc_train: 0.9571 loss_val: 0.7407 acc_val: 0.7833 time: 0.0325s\n",
            "Epoch: 0604 loss_train: 0.2311 acc_train: 0.9786 loss_val: 0.7424 acc_val: 0.7833 time: 0.0413s\n",
            "Epoch: 0605 loss_train: 0.2777 acc_train: 0.8929 loss_val: 0.7428 acc_val: 0.7833 time: 0.0319s\n",
            "Epoch: 0606 loss_train: 0.2570 acc_train: 0.9357 loss_val: 0.7449 acc_val: 0.7833 time: 0.0351s\n",
            "Epoch: 0607 loss_train: 0.2324 acc_train: 0.9357 loss_val: 0.7476 acc_val: 0.7833 time: 0.0314s\n",
            "Epoch: 0608 loss_train: 0.2785 acc_train: 0.9571 loss_val: 0.7512 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0609 loss_train: 0.2791 acc_train: 0.9357 loss_val: 0.7538 acc_val: 0.7800 time: 0.0336s\n",
            "Epoch: 0610 loss_train: 0.2729 acc_train: 0.9214 loss_val: 0.7524 acc_val: 0.7800 time: 0.0384s\n",
            "Epoch: 0611 loss_train: 0.2599 acc_train: 0.9429 loss_val: 0.7487 acc_val: 0.7800 time: 0.0313s\n",
            "Epoch: 0612 loss_train: 0.2528 acc_train: 0.9214 loss_val: 0.7440 acc_val: 0.7833 time: 0.0313s\n",
            "Epoch: 0613 loss_train: 0.2506 acc_train: 0.9571 loss_val: 0.7383 acc_val: 0.7833 time: 0.0308s\n",
            "Epoch: 0614 loss_train: 0.2428 acc_train: 0.9500 loss_val: 0.7328 acc_val: 0.7800 time: 0.0304s\n",
            "Epoch: 0615 loss_train: 0.2299 acc_train: 0.9714 loss_val: 0.7306 acc_val: 0.7800 time: 0.0322s\n",
            "Epoch: 0616 loss_train: 0.2394 acc_train: 0.9571 loss_val: 0.7314 acc_val: 0.7800 time: 0.0355s\n",
            "Epoch: 0617 loss_train: 0.2390 acc_train: 0.9500 loss_val: 0.7350 acc_val: 0.7833 time: 0.0319s\n",
            "Epoch: 0618 loss_train: 0.2357 acc_train: 0.9571 loss_val: 0.7403 acc_val: 0.7867 time: 0.0357s\n",
            "Epoch: 0619 loss_train: 0.2576 acc_train: 0.9286 loss_val: 0.7453 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0620 loss_train: 0.2487 acc_train: 0.9643 loss_val: 0.7500 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0621 loss_train: 0.2317 acc_train: 0.9571 loss_val: 0.7527 acc_val: 0.7800 time: 0.0339s\n",
            "Epoch: 0622 loss_train: 0.2275 acc_train: 0.9571 loss_val: 0.7523 acc_val: 0.7800 time: 0.0410s\n",
            "Epoch: 0623 loss_train: 0.2286 acc_train: 0.9714 loss_val: 0.7503 acc_val: 0.7833 time: 0.0316s\n",
            "Epoch: 0624 loss_train: 0.2332 acc_train: 0.9571 loss_val: 0.7481 acc_val: 0.7833 time: 0.0318s\n",
            "Epoch: 0625 loss_train: 0.2545 acc_train: 0.9429 loss_val: 0.7467 acc_val: 0.7867 time: 0.0313s\n",
            "Epoch: 0626 loss_train: 0.2445 acc_train: 0.9857 loss_val: 0.7460 acc_val: 0.7833 time: 0.0313s\n",
            "Epoch: 0627 loss_train: 0.2670 acc_train: 0.9357 loss_val: 0.7474 acc_val: 0.7867 time: 0.0380s\n",
            "Epoch: 0628 loss_train: 0.2461 acc_train: 0.9429 loss_val: 0.7484 acc_val: 0.7867 time: 0.0444s\n",
            "Epoch: 0629 loss_train: 0.2561 acc_train: 0.9429 loss_val: 0.7482 acc_val: 0.7833 time: 0.0350s\n",
            "Epoch: 0630 loss_train: 0.1983 acc_train: 0.9786 loss_val: 0.7455 acc_val: 0.7867 time: 0.0341s\n",
            "Epoch: 0631 loss_train: 0.2570 acc_train: 0.9500 loss_val: 0.7431 acc_val: 0.7867 time: 0.0357s\n",
            "Epoch: 0632 loss_train: 0.1914 acc_train: 0.9786 loss_val: 0.7415 acc_val: 0.7900 time: 0.0333s\n",
            "Epoch: 0633 loss_train: 0.2606 acc_train: 0.9429 loss_val: 0.7410 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0634 loss_train: 0.2655 acc_train: 0.9429 loss_val: 0.7430 acc_val: 0.7900 time: 0.0298s\n",
            "Epoch: 0635 loss_train: 0.2470 acc_train: 0.9214 loss_val: 0.7467 acc_val: 0.7867 time: 0.0299s\n",
            "Epoch: 0636 loss_train: 0.2843 acc_train: 0.9357 loss_val: 0.7513 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0637 loss_train: 0.2660 acc_train: 0.9357 loss_val: 0.7557 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0638 loss_train: 0.2940 acc_train: 0.9286 loss_val: 0.7576 acc_val: 0.7833 time: 0.0251s\n",
            "Epoch: 0639 loss_train: 0.2414 acc_train: 0.9714 loss_val: 0.7557 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0640 loss_train: 0.2478 acc_train: 0.9357 loss_val: 0.7528 acc_val: 0.7833 time: 0.0256s\n",
            "Epoch: 0641 loss_train: 0.2463 acc_train: 0.9500 loss_val: 0.7490 acc_val: 0.7833 time: 0.0277s\n",
            "Epoch: 0642 loss_train: 0.2396 acc_train: 0.9500 loss_val: 0.7452 acc_val: 0.7900 time: 0.0308s\n",
            "Epoch: 0643 loss_train: 0.2651 acc_train: 0.9286 loss_val: 0.7429 acc_val: 0.7900 time: 0.0265s\n",
            "Epoch: 0644 loss_train: 0.2563 acc_train: 0.9571 loss_val: 0.7414 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0645 loss_train: 0.2581 acc_train: 0.9357 loss_val: 0.7393 acc_val: 0.7900 time: 0.0256s\n",
            "Epoch: 0646 loss_train: 0.2119 acc_train: 0.9714 loss_val: 0.7379 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0647 loss_train: 0.2602 acc_train: 0.9286 loss_val: 0.7398 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0648 loss_train: 0.2314 acc_train: 0.9571 loss_val: 0.7421 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0649 loss_train: 0.2322 acc_train: 0.9714 loss_val: 0.7444 acc_val: 0.7833 time: 0.0338s\n",
            "Epoch: 0650 loss_train: 0.2551 acc_train: 0.9571 loss_val: 0.7469 acc_val: 0.7833 time: 0.0251s\n",
            "Epoch: 0651 loss_train: 0.2276 acc_train: 0.9429 loss_val: 0.7519 acc_val: 0.7833 time: 0.0262s\n",
            "Epoch: 0652 loss_train: 0.2281 acc_train: 0.9643 loss_val: 0.7548 acc_val: 0.7800 time: 0.0247s\n",
            "Epoch: 0653 loss_train: 0.1806 acc_train: 0.9929 loss_val: 0.7571 acc_val: 0.7800 time: 0.0242s\n",
            "Epoch: 0654 loss_train: 0.2381 acc_train: 0.9714 loss_val: 0.7601 acc_val: 0.7800 time: 0.0260s\n",
            "Epoch: 0655 loss_train: 0.2600 acc_train: 0.9286 loss_val: 0.7609 acc_val: 0.7833 time: 0.0235s\n",
            "Epoch: 0656 loss_train: 0.2379 acc_train: 0.9643 loss_val: 0.7577 acc_val: 0.7833 time: 0.0270s\n",
            "Epoch: 0657 loss_train: 0.2969 acc_train: 0.9357 loss_val: 0.7504 acc_val: 0.7833 time: 0.0266s\n",
            "Epoch: 0658 loss_train: 0.2433 acc_train: 0.9643 loss_val: 0.7431 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0659 loss_train: 0.2722 acc_train: 0.9357 loss_val: 0.7380 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0660 loss_train: 0.2250 acc_train: 0.9714 loss_val: 0.7354 acc_val: 0.7900 time: 0.0235s\n",
            "Epoch: 0661 loss_train: 0.2266 acc_train: 0.9643 loss_val: 0.7319 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0662 loss_train: 0.2787 acc_train: 0.9500 loss_val: 0.7289 acc_val: 0.7867 time: 0.0230s\n",
            "Epoch: 0663 loss_train: 0.2922 acc_train: 0.9286 loss_val: 0.7280 acc_val: 0.7900 time: 0.0283s\n",
            "Epoch: 0664 loss_train: 0.2085 acc_train: 0.9786 loss_val: 0.7307 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0665 loss_train: 0.2601 acc_train: 0.9643 loss_val: 0.7348 acc_val: 0.7900 time: 0.0266s\n",
            "Epoch: 0666 loss_train: 0.2066 acc_train: 0.9571 loss_val: 0.7408 acc_val: 0.7900 time: 0.0228s\n",
            "Epoch: 0667 loss_train: 0.2609 acc_train: 0.9643 loss_val: 0.7459 acc_val: 0.7867 time: 0.0223s\n",
            "Epoch: 0668 loss_train: 0.2198 acc_train: 0.9571 loss_val: 0.7513 acc_val: 0.7800 time: 0.0259s\n",
            "Epoch: 0669 loss_train: 0.2094 acc_train: 0.9786 loss_val: 0.7575 acc_val: 0.7767 time: 0.0235s\n",
            "Epoch: 0670 loss_train: 0.2260 acc_train: 0.9643 loss_val: 0.7590 acc_val: 0.7800 time: 0.0253s\n",
            "Epoch: 0671 loss_train: 0.2216 acc_train: 0.9643 loss_val: 0.7598 acc_val: 0.7800 time: 0.0245s\n",
            "Epoch: 0672 loss_train: 0.2806 acc_train: 0.9357 loss_val: 0.7553 acc_val: 0.7833 time: 0.0296s\n",
            "Epoch: 0673 loss_train: 0.2692 acc_train: 0.9500 loss_val: 0.7504 acc_val: 0.7900 time: 0.0280s\n",
            "Epoch: 0674 loss_train: 0.2597 acc_train: 0.9571 loss_val: 0.7460 acc_val: 0.7900 time: 0.0289s\n",
            "Epoch: 0675 loss_train: 0.2768 acc_train: 0.9429 loss_val: 0.7415 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0676 loss_train: 0.3064 acc_train: 0.9357 loss_val: 0.7385 acc_val: 0.7900 time: 0.0243s\n",
            "Epoch: 0677 loss_train: 0.2305 acc_train: 0.9500 loss_val: 0.7377 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0678 loss_train: 0.2304 acc_train: 0.9571 loss_val: 0.7386 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0679 loss_train: 0.2338 acc_train: 0.9571 loss_val: 0.7395 acc_val: 0.7833 time: 0.0250s\n",
            "Epoch: 0680 loss_train: 0.2618 acc_train: 0.9500 loss_val: 0.7403 acc_val: 0.7833 time: 0.0247s\n",
            "Epoch: 0681 loss_train: 0.2483 acc_train: 0.9571 loss_val: 0.7402 acc_val: 0.7833 time: 0.0273s\n",
            "Epoch: 0682 loss_train: 0.2414 acc_train: 0.9571 loss_val: 0.7407 acc_val: 0.7833 time: 0.0237s\n",
            "Epoch: 0683 loss_train: 0.2273 acc_train: 0.9500 loss_val: 0.7431 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0684 loss_train: 0.2333 acc_train: 0.9643 loss_val: 0.7427 acc_val: 0.7867 time: 0.0272s\n",
            "Epoch: 0685 loss_train: 0.2426 acc_train: 0.9500 loss_val: 0.7409 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0686 loss_train: 0.2231 acc_train: 0.9643 loss_val: 0.7394 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0687 loss_train: 0.2419 acc_train: 0.9357 loss_val: 0.7378 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0688 loss_train: 0.2448 acc_train: 0.9571 loss_val: 0.7370 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0689 loss_train: 0.2091 acc_train: 0.9643 loss_val: 0.7358 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0690 loss_train: 0.1937 acc_train: 0.9714 loss_val: 0.7362 acc_val: 0.7833 time: 0.0269s\n",
            "Epoch: 0691 loss_train: 0.1936 acc_train: 0.9714 loss_val: 0.7380 acc_val: 0.7833 time: 0.0267s\n",
            "Epoch: 0692 loss_train: 0.2248 acc_train: 0.9571 loss_val: 0.7405 acc_val: 0.7800 time: 0.0257s\n",
            "Epoch: 0693 loss_train: 0.2506 acc_train: 0.9571 loss_val: 0.7435 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0694 loss_train: 0.2405 acc_train: 0.9571 loss_val: 0.7461 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0695 loss_train: 0.2400 acc_train: 0.9286 loss_val: 0.7513 acc_val: 0.7833 time: 0.0238s\n",
            "Epoch: 0696 loss_train: 0.2281 acc_train: 0.9500 loss_val: 0.7555 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0697 loss_train: 0.2207 acc_train: 0.9643 loss_val: 0.7564 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0698 loss_train: 0.2254 acc_train: 0.9571 loss_val: 0.7568 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0699 loss_train: 0.3080 acc_train: 0.9214 loss_val: 0.7585 acc_val: 0.7867 time: 0.0275s\n",
            "Epoch: 0700 loss_train: 0.2243 acc_train: 0.9500 loss_val: 0.7580 acc_val: 0.7867 time: 0.0281s\n",
            "Epoch: 0701 loss_train: 0.2092 acc_train: 0.9643 loss_val: 0.7570 acc_val: 0.7800 time: 0.0252s\n",
            "Epoch: 0702 loss_train: 0.2076 acc_train: 0.9571 loss_val: 0.7549 acc_val: 0.7833 time: 0.0303s\n",
            "Epoch: 0703 loss_train: 0.1974 acc_train: 0.9571 loss_val: 0.7532 acc_val: 0.7833 time: 0.0260s\n",
            "Epoch: 0704 loss_train: 0.2310 acc_train: 0.9500 loss_val: 0.7532 acc_val: 0.7833 time: 0.0296s\n",
            "Epoch: 0705 loss_train: 0.2924 acc_train: 0.9571 loss_val: 0.7536 acc_val: 0.7833 time: 0.0272s\n",
            "Epoch: 0706 loss_train: 0.2742 acc_train: 0.9214 loss_val: 0.7527 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0707 loss_train: 0.2488 acc_train: 0.9286 loss_val: 0.7503 acc_val: 0.7833 time: 0.0311s\n",
            "Epoch: 0708 loss_train: 0.2339 acc_train: 0.9571 loss_val: 0.7472 acc_val: 0.7833 time: 0.0246s\n",
            "Epoch: 0709 loss_train: 0.2424 acc_train: 0.9643 loss_val: 0.7445 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0710 loss_train: 0.2538 acc_train: 0.9429 loss_val: 0.7417 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0711 loss_train: 0.2216 acc_train: 0.9643 loss_val: 0.7396 acc_val: 0.7900 time: 0.0297s\n",
            "Epoch: 0712 loss_train: 0.2051 acc_train: 0.9643 loss_val: 0.7377 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0713 loss_train: 0.2104 acc_train: 0.9571 loss_val: 0.7370 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0714 loss_train: 0.2426 acc_train: 0.9429 loss_val: 0.7375 acc_val: 0.7867 time: 0.0252s\n",
            "Epoch: 0715 loss_train: 0.2461 acc_train: 0.9500 loss_val: 0.7394 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0716 loss_train: 0.2453 acc_train: 0.9571 loss_val: 0.7416 acc_val: 0.7867 time: 0.0231s\n",
            "Epoch: 0717 loss_train: 0.2146 acc_train: 0.9786 loss_val: 0.7441 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0718 loss_train: 0.2619 acc_train: 0.9429 loss_val: 0.7422 acc_val: 0.7867 time: 0.0264s\n",
            "Epoch: 0719 loss_train: 0.2681 acc_train: 0.9357 loss_val: 0.7434 acc_val: 0.7833 time: 0.0285s\n",
            "Epoch: 0720 loss_train: 0.2290 acc_train: 0.9786 loss_val: 0.7447 acc_val: 0.7833 time: 0.0247s\n",
            "Epoch: 0721 loss_train: 0.2705 acc_train: 0.9357 loss_val: 0.7463 acc_val: 0.7833 time: 0.0252s\n",
            "Epoch: 0722 loss_train: 0.2598 acc_train: 0.9214 loss_val: 0.7452 acc_val: 0.7833 time: 0.0246s\n",
            "Epoch: 0723 loss_train: 0.2398 acc_train: 0.9357 loss_val: 0.7432 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0724 loss_train: 0.2188 acc_train: 0.9786 loss_val: 0.7441 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0725 loss_train: 0.2375 acc_train: 0.9571 loss_val: 0.7464 acc_val: 0.7833 time: 0.0256s\n",
            "Epoch: 0726 loss_train: 0.2255 acc_train: 0.9857 loss_val: 0.7493 acc_val: 0.7833 time: 0.0277s\n",
            "Epoch: 0727 loss_train: 0.2569 acc_train: 0.9714 loss_val: 0.7512 acc_val: 0.7833 time: 0.0229s\n",
            "Epoch: 0728 loss_train: 0.2196 acc_train: 0.9571 loss_val: 0.7528 acc_val: 0.7833 time: 0.0234s\n",
            "Epoch: 0729 loss_train: 0.2338 acc_train: 0.9786 loss_val: 0.7516 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0730 loss_train: 0.2664 acc_train: 0.9286 loss_val: 0.7507 acc_val: 0.7867 time: 0.0256s\n",
            "Epoch: 0731 loss_train: 0.2509 acc_train: 0.9643 loss_val: 0.7493 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0732 loss_train: 0.2178 acc_train: 0.9643 loss_val: 0.7502 acc_val: 0.7867 time: 0.0231s\n",
            "Epoch: 0733 loss_train: 0.1929 acc_train: 0.9857 loss_val: 0.7491 acc_val: 0.7867 time: 0.0261s\n",
            "Epoch: 0734 loss_train: 0.2370 acc_train: 0.9643 loss_val: 0.7487 acc_val: 0.7867 time: 0.0270s\n",
            "Epoch: 0735 loss_train: 0.1863 acc_train: 0.9643 loss_val: 0.7484 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0736 loss_train: 0.2065 acc_train: 0.9643 loss_val: 0.7479 acc_val: 0.7833 time: 0.0287s\n",
            "Epoch: 0737 loss_train: 0.1871 acc_train: 0.9857 loss_val: 0.7493 acc_val: 0.7800 time: 0.0254s\n",
            "Epoch: 0738 loss_train: 0.2196 acc_train: 0.9714 loss_val: 0.7512 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0739 loss_train: 0.2062 acc_train: 0.9643 loss_val: 0.7521 acc_val: 0.7833 time: 0.0235s\n",
            "Epoch: 0740 loss_train: 0.2276 acc_train: 0.9714 loss_val: 0.7515 acc_val: 0.7800 time: 0.0252s\n",
            "Epoch: 0741 loss_train: 0.1775 acc_train: 0.9786 loss_val: 0.7502 acc_val: 0.7867 time: 0.0263s\n",
            "Epoch: 0742 loss_train: 0.2237 acc_train: 0.9571 loss_val: 0.7487 acc_val: 0.7867 time: 0.0279s\n",
            "Epoch: 0743 loss_train: 0.2650 acc_train: 0.9357 loss_val: 0.7462 acc_val: 0.7867 time: 0.0265s\n",
            "Epoch: 0744 loss_train: 0.2843 acc_train: 0.9357 loss_val: 0.7433 acc_val: 0.7933 time: 0.0272s\n",
            "Epoch: 0745 loss_train: 0.2382 acc_train: 0.9571 loss_val: 0.7398 acc_val: 0.7933 time: 0.0241s\n",
            "Epoch: 0746 loss_train: 0.2769 acc_train: 0.9286 loss_val: 0.7366 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0747 loss_train: 0.2514 acc_train: 0.9571 loss_val: 0.7356 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0748 loss_train: 0.2379 acc_train: 0.9714 loss_val: 0.7349 acc_val: 0.7933 time: 0.0272s\n",
            "Epoch: 0749 loss_train: 0.2141 acc_train: 0.9643 loss_val: 0.7352 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0750 loss_train: 0.2280 acc_train: 0.9643 loss_val: 0.7377 acc_val: 0.7900 time: 0.0276s\n",
            "Epoch: 0751 loss_train: 0.2403 acc_train: 0.9571 loss_val: 0.7409 acc_val: 0.7900 time: 0.0272s\n",
            "Epoch: 0752 loss_train: 0.2406 acc_train: 0.9571 loss_val: 0.7430 acc_val: 0.7933 time: 0.0229s\n",
            "Epoch: 0753 loss_train: 0.2239 acc_train: 0.9571 loss_val: 0.7419 acc_val: 0.7933 time: 0.0233s\n",
            "Epoch: 0754 loss_train: 0.2894 acc_train: 0.9357 loss_val: 0.7399 acc_val: 0.7933 time: 0.0238s\n",
            "Epoch: 0755 loss_train: 0.2367 acc_train: 0.9500 loss_val: 0.7375 acc_val: 0.7900 time: 0.0272s\n",
            "Epoch: 0756 loss_train: 0.2509 acc_train: 0.9429 loss_val: 0.7360 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0757 loss_train: 0.2224 acc_train: 0.9429 loss_val: 0.7348 acc_val: 0.7900 time: 0.0244s\n",
            "Epoch: 0758 loss_train: 0.2187 acc_train: 0.9571 loss_val: 0.7359 acc_val: 0.7900 time: 0.0272s\n",
            "Epoch: 0759 loss_train: 0.2036 acc_train: 0.9643 loss_val: 0.7362 acc_val: 0.7900 time: 0.0258s\n",
            "Epoch: 0760 loss_train: 0.2311 acc_train: 0.9571 loss_val: 0.7370 acc_val: 0.7900 time: 0.0251s\n",
            "Epoch: 0761 loss_train: 0.2119 acc_train: 0.9571 loss_val: 0.7392 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0762 loss_train: 0.2315 acc_train: 0.9571 loss_val: 0.7417 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0763 loss_train: 0.2105 acc_train: 0.9571 loss_val: 0.7433 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0764 loss_train: 0.1930 acc_train: 0.9714 loss_val: 0.7432 acc_val: 0.7867 time: 0.0262s\n",
            "Epoch: 0765 loss_train: 0.2173 acc_train: 0.9857 loss_val: 0.7407 acc_val: 0.7867 time: 0.0231s\n",
            "Epoch: 0766 loss_train: 0.2264 acc_train: 0.9571 loss_val: 0.7397 acc_val: 0.7900 time: 0.0262s\n",
            "Epoch: 0767 loss_train: 0.2481 acc_train: 0.9571 loss_val: 0.7374 acc_val: 0.7900 time: 0.0234s\n",
            "Epoch: 0768 loss_train: 0.2478 acc_train: 0.9429 loss_val: 0.7354 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0769 loss_train: 0.2057 acc_train: 0.9857 loss_val: 0.7329 acc_val: 0.7900 time: 0.0237s\n",
            "Epoch: 0770 loss_train: 0.2266 acc_train: 0.9571 loss_val: 0.7303 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0771 loss_train: 0.2578 acc_train: 0.9571 loss_val: 0.7288 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0772 loss_train: 0.2325 acc_train: 0.9500 loss_val: 0.7296 acc_val: 0.7867 time: 0.0284s\n",
            "Epoch: 0773 loss_train: 0.2544 acc_train: 0.9286 loss_val: 0.7326 acc_val: 0.7900 time: 0.0285s\n",
            "Epoch: 0774 loss_train: 0.2048 acc_train: 0.9714 loss_val: 0.7365 acc_val: 0.7867 time: 0.0276s\n",
            "Epoch: 0775 loss_train: 0.2341 acc_train: 0.9429 loss_val: 0.7407 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0776 loss_train: 0.2080 acc_train: 0.9571 loss_val: 0.7458 acc_val: 0.7833 time: 0.0234s\n",
            "Epoch: 0777 loss_train: 0.2218 acc_train: 0.9643 loss_val: 0.7504 acc_val: 0.7867 time: 0.0234s\n",
            "Epoch: 0778 loss_train: 0.2041 acc_train: 0.9857 loss_val: 0.7515 acc_val: 0.7900 time: 0.0234s\n",
            "Epoch: 0779 loss_train: 0.2497 acc_train: 0.9500 loss_val: 0.7512 acc_val: 0.7900 time: 0.0251s\n",
            "Epoch: 0780 loss_train: 0.2256 acc_train: 0.9643 loss_val: 0.7459 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0781 loss_train: 0.2225 acc_train: 0.9500 loss_val: 0.7412 acc_val: 0.7867 time: 0.0287s\n",
            "Epoch: 0782 loss_train: 0.2364 acc_train: 0.9571 loss_val: 0.7345 acc_val: 0.7867 time: 0.0274s\n",
            "Epoch: 0783 loss_train: 0.2321 acc_train: 0.9643 loss_val: 0.7307 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0784 loss_train: 0.2234 acc_train: 0.9500 loss_val: 0.7295 acc_val: 0.7833 time: 0.0251s\n",
            "Epoch: 0785 loss_train: 0.2043 acc_train: 0.9714 loss_val: 0.7298 acc_val: 0.7833 time: 0.0261s\n",
            "Epoch: 0786 loss_train: 0.2461 acc_train: 0.9571 loss_val: 0.7325 acc_val: 0.7833 time: 0.0256s\n",
            "Epoch: 0787 loss_train: 0.2426 acc_train: 0.9286 loss_val: 0.7360 acc_val: 0.7833 time: 0.0255s\n",
            "Epoch: 0788 loss_train: 0.2402 acc_train: 0.9643 loss_val: 0.7431 acc_val: 0.7867 time: 0.0236s\n",
            "Epoch: 0789 loss_train: 0.2252 acc_train: 0.9357 loss_val: 0.7503 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0790 loss_train: 0.2183 acc_train: 0.9571 loss_val: 0.7560 acc_val: 0.7800 time: 0.0324s\n",
            "Epoch: 0791 loss_train: 0.2278 acc_train: 0.9500 loss_val: 0.7591 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0792 loss_train: 0.2540 acc_train: 0.9500 loss_val: 0.7593 acc_val: 0.7833 time: 0.0238s\n",
            "Epoch: 0793 loss_train: 0.1988 acc_train: 0.9643 loss_val: 0.7565 acc_val: 0.7867 time: 0.0237s\n",
            "Epoch: 0794 loss_train: 0.1982 acc_train: 0.9786 loss_val: 0.7514 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0795 loss_train: 0.2467 acc_train: 0.9357 loss_val: 0.7457 acc_val: 0.7867 time: 0.0305s\n",
            "Epoch: 0796 loss_train: 0.2230 acc_train: 0.9286 loss_val: 0.7394 acc_val: 0.7833 time: 0.0258s\n",
            "Epoch: 0797 loss_train: 0.2298 acc_train: 0.9500 loss_val: 0.7348 acc_val: 0.7900 time: 0.0245s\n",
            "Epoch: 0798 loss_train: 0.2517 acc_train: 0.9643 loss_val: 0.7336 acc_val: 0.7900 time: 0.0262s\n",
            "Epoch: 0799 loss_train: 0.2733 acc_train: 0.9571 loss_val: 0.7360 acc_val: 0.7900 time: 0.0244s\n",
            "Epoch: 0800 loss_train: 0.2219 acc_train: 0.9286 loss_val: 0.7395 acc_val: 0.7900 time: 0.0240s\n",
            "Epoch: 0801 loss_train: 0.2641 acc_train: 0.9500 loss_val: 0.7444 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0802 loss_train: 0.2070 acc_train: 0.9786 loss_val: 0.7494 acc_val: 0.7800 time: 0.0237s\n",
            "Epoch: 0803 loss_train: 0.2119 acc_train: 0.9571 loss_val: 0.7556 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0804 loss_train: 0.2466 acc_train: 0.9500 loss_val: 0.7609 acc_val: 0.7733 time: 0.0241s\n",
            "Epoch: 0805 loss_train: 0.2419 acc_train: 0.9500 loss_val: 0.7638 acc_val: 0.7733 time: 0.0237s\n",
            "Epoch: 0806 loss_train: 0.2535 acc_train: 0.9571 loss_val: 0.7627 acc_val: 0.7733 time: 0.0262s\n",
            "Epoch: 0807 loss_train: 0.2088 acc_train: 0.9714 loss_val: 0.7600 acc_val: 0.7800 time: 0.0265s\n",
            "Epoch: 0808 loss_train: 0.2791 acc_train: 0.9429 loss_val: 0.7524 acc_val: 0.7833 time: 0.0248s\n",
            "Epoch: 0809 loss_train: 0.2690 acc_train: 0.9429 loss_val: 0.7433 acc_val: 0.7833 time: 0.0306s\n",
            "Epoch: 0810 loss_train: 0.2223 acc_train: 0.9500 loss_val: 0.7361 acc_val: 0.7833 time: 0.0237s\n",
            "Epoch: 0811 loss_train: 0.2374 acc_train: 0.9500 loss_val: 0.7311 acc_val: 0.7867 time: 0.0254s\n",
            "Epoch: 0812 loss_train: 0.2527 acc_train: 0.9071 loss_val: 0.7290 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0813 loss_train: 0.1902 acc_train: 0.9786 loss_val: 0.7280 acc_val: 0.7867 time: 0.0283s\n",
            "Epoch: 0814 loss_train: 0.2502 acc_train: 0.9643 loss_val: 0.7293 acc_val: 0.7833 time: 0.0270s\n",
            "Epoch: 0815 loss_train: 0.2365 acc_train: 0.9643 loss_val: 0.7341 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0816 loss_train: 0.2324 acc_train: 0.9571 loss_val: 0.7405 acc_val: 0.7833 time: 0.0235s\n",
            "Epoch: 0817 loss_train: 0.2287 acc_train: 0.9357 loss_val: 0.7481 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0818 loss_train: 0.2024 acc_train: 0.9643 loss_val: 0.7548 acc_val: 0.7800 time: 0.0237s\n",
            "Epoch: 0819 loss_train: 0.2913 acc_train: 0.9429 loss_val: 0.7559 acc_val: 0.7767 time: 0.0233s\n",
            "Epoch: 0820 loss_train: 0.2756 acc_train: 0.9429 loss_val: 0.7562 acc_val: 0.7733 time: 0.0248s\n",
            "Epoch: 0821 loss_train: 0.2510 acc_train: 0.9357 loss_val: 0.7520 acc_val: 0.7833 time: 0.0275s\n",
            "Epoch: 0822 loss_train: 0.2117 acc_train: 0.9714 loss_val: 0.7463 acc_val: 0.7833 time: 0.0233s\n",
            "Epoch: 0823 loss_train: 0.2187 acc_train: 0.9571 loss_val: 0.7424 acc_val: 0.7833 time: 0.0235s\n",
            "Epoch: 0824 loss_train: 0.2580 acc_train: 0.9643 loss_val: 0.7377 acc_val: 0.7800 time: 0.0235s\n",
            "Epoch: 0825 loss_train: 0.2278 acc_train: 0.9643 loss_val: 0.7351 acc_val: 0.7800 time: 0.0260s\n",
            "Epoch: 0826 loss_train: 0.2102 acc_train: 0.9571 loss_val: 0.7342 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0827 loss_train: 0.2175 acc_train: 0.9643 loss_val: 0.7345 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0828 loss_train: 0.1914 acc_train: 0.9571 loss_val: 0.7354 acc_val: 0.7833 time: 0.0244s\n",
            "Epoch: 0829 loss_train: 0.2458 acc_train: 0.9429 loss_val: 0.7374 acc_val: 0.7833 time: 0.0267s\n",
            "Epoch: 0830 loss_train: 0.2219 acc_train: 0.9571 loss_val: 0.7378 acc_val: 0.7833 time: 0.0272s\n",
            "Epoch: 0831 loss_train: 0.2130 acc_train: 0.9500 loss_val: 0.7391 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0832 loss_train: 0.2065 acc_train: 0.9786 loss_val: 0.7395 acc_val: 0.7833 time: 0.0275s\n",
            "Epoch: 0833 loss_train: 0.2079 acc_train: 0.9714 loss_val: 0.7387 acc_val: 0.7833 time: 0.0245s\n",
            "Epoch: 0834 loss_train: 0.2130 acc_train: 0.9714 loss_val: 0.7386 acc_val: 0.7833 time: 0.0252s\n",
            "Epoch: 0835 loss_train: 0.2429 acc_train: 0.9357 loss_val: 0.7383 acc_val: 0.7833 time: 0.0253s\n",
            "Epoch: 0836 loss_train: 0.2837 acc_train: 0.9357 loss_val: 0.7387 acc_val: 0.7800 time: 0.0248s\n",
            "Epoch: 0837 loss_train: 0.2752 acc_train: 0.9429 loss_val: 0.7386 acc_val: 0.7800 time: 0.0243s\n",
            "Epoch: 0838 loss_train: 0.2016 acc_train: 0.9786 loss_val: 0.7362 acc_val: 0.7800 time: 0.0237s\n",
            "Epoch: 0839 loss_train: 0.2333 acc_train: 0.9643 loss_val: 0.7351 acc_val: 0.7800 time: 0.0240s\n",
            "Epoch: 0840 loss_train: 0.2053 acc_train: 0.9786 loss_val: 0.7347 acc_val: 0.7800 time: 0.0249s\n",
            "Epoch: 0841 loss_train: 0.2452 acc_train: 0.9500 loss_val: 0.7347 acc_val: 0.7800 time: 0.0241s\n",
            "Epoch: 0842 loss_train: 0.2003 acc_train: 0.9571 loss_val: 0.7356 acc_val: 0.7800 time: 0.0245s\n",
            "Epoch: 0843 loss_train: 0.2029 acc_train: 0.9786 loss_val: 0.7366 acc_val: 0.7800 time: 0.0242s\n",
            "Epoch: 0844 loss_train: 0.2592 acc_train: 0.9357 loss_val: 0.7399 acc_val: 0.7833 time: 0.0245s\n",
            "Epoch: 0845 loss_train: 0.2889 acc_train: 0.9143 loss_val: 0.7440 acc_val: 0.7833 time: 0.0309s\n",
            "Epoch: 0846 loss_train: 0.2464 acc_train: 0.9571 loss_val: 0.7458 acc_val: 0.7833 time: 0.0342s\n",
            "Epoch: 0847 loss_train: 0.2437 acc_train: 0.9286 loss_val: 0.7450 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0848 loss_train: 0.2577 acc_train: 0.9571 loss_val: 0.7420 acc_val: 0.7867 time: 0.0247s\n",
            "Epoch: 0849 loss_train: 0.2079 acc_train: 0.9714 loss_val: 0.7369 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0850 loss_train: 0.2334 acc_train: 0.9643 loss_val: 0.7315 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0851 loss_train: 0.2258 acc_train: 0.9571 loss_val: 0.7286 acc_val: 0.7867 time: 0.0260s\n",
            "Epoch: 0852 loss_train: 0.2273 acc_train: 0.9500 loss_val: 0.7278 acc_val: 0.7867 time: 0.0251s\n",
            "Epoch: 0853 loss_train: 0.2412 acc_train: 0.9571 loss_val: 0.7279 acc_val: 0.7867 time: 0.0268s\n",
            "Epoch: 0854 loss_train: 0.1893 acc_train: 0.9714 loss_val: 0.7297 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0855 loss_train: 0.1625 acc_train: 0.9714 loss_val: 0.7337 acc_val: 0.7900 time: 0.0240s\n",
            "Epoch: 0856 loss_train: 0.1819 acc_train: 0.9857 loss_val: 0.7412 acc_val: 0.7833 time: 0.0294s\n",
            "Epoch: 0857 loss_train: 0.2454 acc_train: 0.9286 loss_val: 0.7453 acc_val: 0.7833 time: 0.0253s\n",
            "Epoch: 0858 loss_train: 0.2186 acc_train: 0.9786 loss_val: 0.7466 acc_val: 0.7833 time: 0.0233s\n",
            "Epoch: 0859 loss_train: 0.2262 acc_train: 0.9714 loss_val: 0.7475 acc_val: 0.7833 time: 0.0231s\n",
            "Epoch: 0860 loss_train: 0.2454 acc_train: 0.9643 loss_val: 0.7453 acc_val: 0.7833 time: 0.0287s\n",
            "Epoch: 0861 loss_train: 0.2763 acc_train: 0.9429 loss_val: 0.7425 acc_val: 0.7800 time: 0.0263s\n",
            "Epoch: 0862 loss_train: 0.1953 acc_train: 0.9786 loss_val: 0.7384 acc_val: 0.7800 time: 0.0242s\n",
            "Epoch: 0863 loss_train: 0.2122 acc_train: 0.9500 loss_val: 0.7354 acc_val: 0.7800 time: 0.0252s\n",
            "Epoch: 0864 loss_train: 0.2101 acc_train: 0.9571 loss_val: 0.7329 acc_val: 0.7800 time: 0.0244s\n",
            "Epoch: 0865 loss_train: 0.2403 acc_train: 0.9500 loss_val: 0.7301 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0866 loss_train: 0.2160 acc_train: 0.9714 loss_val: 0.7286 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0867 loss_train: 0.2574 acc_train: 0.9286 loss_val: 0.7294 acc_val: 0.7867 time: 0.0254s\n",
            "Epoch: 0868 loss_train: 0.2523 acc_train: 0.9500 loss_val: 0.7313 acc_val: 0.7867 time: 0.0278s\n",
            "Epoch: 0869 loss_train: 0.2161 acc_train: 0.9500 loss_val: 0.7348 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0870 loss_train: 0.2412 acc_train: 0.9714 loss_val: 0.7379 acc_val: 0.7800 time: 0.0252s\n",
            "Epoch: 0871 loss_train: 0.2495 acc_train: 0.9357 loss_val: 0.7386 acc_val: 0.7800 time: 0.0242s\n",
            "Epoch: 0872 loss_train: 0.2107 acc_train: 0.9714 loss_val: 0.7370 acc_val: 0.7800 time: 0.0232s\n",
            "Epoch: 0873 loss_train: 0.2293 acc_train: 0.9500 loss_val: 0.7353 acc_val: 0.7833 time: 0.0232s\n",
            "Epoch: 0874 loss_train: 0.2203 acc_train: 0.9500 loss_val: 0.7330 acc_val: 0.7833 time: 0.0231s\n",
            "Epoch: 0875 loss_train: 0.2478 acc_train: 0.9571 loss_val: 0.7317 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0876 loss_train: 0.2299 acc_train: 0.9429 loss_val: 0.7322 acc_val: 0.7800 time: 0.0264s\n",
            "Epoch: 0877 loss_train: 0.2188 acc_train: 0.9571 loss_val: 0.7338 acc_val: 0.7800 time: 0.0249s\n",
            "Epoch: 0878 loss_train: 0.2434 acc_train: 0.9500 loss_val: 0.7363 acc_val: 0.7800 time: 0.0245s\n",
            "Epoch: 0879 loss_train: 0.2041 acc_train: 0.9500 loss_val: 0.7377 acc_val: 0.7800 time: 0.0256s\n",
            "Epoch: 0880 loss_train: 0.1966 acc_train: 0.9857 loss_val: 0.7378 acc_val: 0.7800 time: 0.0255s\n",
            "Epoch: 0881 loss_train: 0.1817 acc_train: 0.9786 loss_val: 0.7386 acc_val: 0.7833 time: 0.0236s\n",
            "Epoch: 0882 loss_train: 0.2486 acc_train: 0.9357 loss_val: 0.7402 acc_val: 0.7867 time: 0.0244s\n",
            "Epoch: 0883 loss_train: 0.2184 acc_train: 0.9714 loss_val: 0.7394 acc_val: 0.7867 time: 0.0341s\n",
            "Epoch: 0884 loss_train: 0.2420 acc_train: 0.9214 loss_val: 0.7390 acc_val: 0.7867 time: 0.0270s\n",
            "Epoch: 0885 loss_train: 0.1791 acc_train: 0.9571 loss_val: 0.7392 acc_val: 0.7900 time: 0.0234s\n",
            "Epoch: 0886 loss_train: 0.2241 acc_train: 0.9429 loss_val: 0.7396 acc_val: 0.7900 time: 0.0276s\n",
            "Epoch: 0887 loss_train: 0.2684 acc_train: 0.9214 loss_val: 0.7425 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0888 loss_train: 0.2099 acc_train: 0.9786 loss_val: 0.7459 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0889 loss_train: 0.2242 acc_train: 0.9786 loss_val: 0.7475 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0890 loss_train: 0.2130 acc_train: 0.9571 loss_val: 0.7485 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0891 loss_train: 0.2251 acc_train: 0.9643 loss_val: 0.7473 acc_val: 0.7867 time: 0.0231s\n",
            "Epoch: 0892 loss_train: 0.2462 acc_train: 0.9500 loss_val: 0.7463 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0893 loss_train: 0.2211 acc_train: 0.9571 loss_val: 0.7461 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0894 loss_train: 0.2262 acc_train: 0.9357 loss_val: 0.7446 acc_val: 0.7867 time: 0.0240s\n",
            "Epoch: 0895 loss_train: 0.2116 acc_train: 0.9643 loss_val: 0.7433 acc_val: 0.7867 time: 0.0225s\n",
            "Epoch: 0896 loss_train: 0.2159 acc_train: 0.9500 loss_val: 0.7432 acc_val: 0.7867 time: 0.0250s\n",
            "Epoch: 0897 loss_train: 0.1980 acc_train: 0.9786 loss_val: 0.7441 acc_val: 0.7800 time: 0.0263s\n",
            "Epoch: 0898 loss_train: 0.1805 acc_train: 0.9714 loss_val: 0.7439 acc_val: 0.7800 time: 0.0246s\n",
            "Epoch: 0899 loss_train: 0.2486 acc_train: 0.9214 loss_val: 0.7440 acc_val: 0.7800 time: 0.0251s\n",
            "Epoch: 0900 loss_train: 0.2077 acc_train: 0.9714 loss_val: 0.7449 acc_val: 0.7833 time: 0.0291s\n",
            "Epoch: 0901 loss_train: 0.1744 acc_train: 0.9786 loss_val: 0.7471 acc_val: 0.7833 time: 0.0248s\n",
            "Epoch: 0902 loss_train: 0.2044 acc_train: 0.9786 loss_val: 0.7471 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0903 loss_train: 0.2243 acc_train: 0.9500 loss_val: 0.7458 acc_val: 0.7833 time: 0.0265s\n",
            "Epoch: 0904 loss_train: 0.2187 acc_train: 0.9571 loss_val: 0.7461 acc_val: 0.7833 time: 0.0245s\n",
            "Epoch: 0905 loss_train: 0.2749 acc_train: 0.9500 loss_val: 0.7478 acc_val: 0.7833 time: 0.0237s\n",
            "Epoch: 0906 loss_train: 0.2159 acc_train: 0.9643 loss_val: 0.7493 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0907 loss_train: 0.2254 acc_train: 0.9429 loss_val: 0.7478 acc_val: 0.7833 time: 0.0235s\n",
            "Epoch: 0908 loss_train: 0.2608 acc_train: 0.9357 loss_val: 0.7461 acc_val: 0.7833 time: 0.0266s\n",
            "Epoch: 0909 loss_train: 0.1703 acc_train: 0.9857 loss_val: 0.7439 acc_val: 0.7867 time: 0.0234s\n",
            "Epoch: 0910 loss_train: 0.2179 acc_train: 0.9500 loss_val: 0.7418 acc_val: 0.7867 time: 0.0249s\n",
            "Epoch: 0911 loss_train: 0.2520 acc_train: 0.9429 loss_val: 0.7392 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0912 loss_train: 0.1978 acc_train: 0.9643 loss_val: 0.7387 acc_val: 0.7867 time: 0.0238s\n",
            "Epoch: 0913 loss_train: 0.2027 acc_train: 0.9714 loss_val: 0.7396 acc_val: 0.7900 time: 0.0238s\n",
            "Epoch: 0914 loss_train: 0.2138 acc_train: 0.9571 loss_val: 0.7415 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0915 loss_train: 0.2117 acc_train: 0.9714 loss_val: 0.7426 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0916 loss_train: 0.1979 acc_train: 0.9786 loss_val: 0.7452 acc_val: 0.7833 time: 0.0274s\n",
            "Epoch: 0917 loss_train: 0.2053 acc_train: 0.9571 loss_val: 0.7485 acc_val: 0.7800 time: 0.0234s\n",
            "Epoch: 0918 loss_train: 0.2394 acc_train: 0.9357 loss_val: 0.7509 acc_val: 0.7800 time: 0.0259s\n",
            "Epoch: 0919 loss_train: 0.2260 acc_train: 0.9643 loss_val: 0.7517 acc_val: 0.7800 time: 0.0290s\n",
            "Epoch: 0920 loss_train: 0.2664 acc_train: 0.9500 loss_val: 0.7482 acc_val: 0.7833 time: 0.0296s\n",
            "Epoch: 0921 loss_train: 0.2412 acc_train: 0.9643 loss_val: 0.7447 acc_val: 0.7900 time: 0.0253s\n",
            "Epoch: 0922 loss_train: 0.2548 acc_train: 0.9571 loss_val: 0.7429 acc_val: 0.7900 time: 0.0256s\n",
            "Epoch: 0923 loss_train: 0.2498 acc_train: 0.9571 loss_val: 0.7396 acc_val: 0.7867 time: 0.0271s\n",
            "Epoch: 0924 loss_train: 0.2461 acc_train: 0.9429 loss_val: 0.7370 acc_val: 0.7867 time: 0.0253s\n",
            "Epoch: 0925 loss_train: 0.2061 acc_train: 0.9786 loss_val: 0.7337 acc_val: 0.7800 time: 0.0240s\n",
            "Epoch: 0926 loss_train: 0.2054 acc_train: 0.9714 loss_val: 0.7322 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0927 loss_train: 0.2648 acc_train: 0.9357 loss_val: 0.7318 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0928 loss_train: 0.2278 acc_train: 0.9714 loss_val: 0.7326 acc_val: 0.7867 time: 0.0241s\n",
            "Epoch: 0929 loss_train: 0.2562 acc_train: 0.9500 loss_val: 0.7327 acc_val: 0.7867 time: 0.0253s\n",
            "Epoch: 0930 loss_train: 0.2079 acc_train: 0.9643 loss_val: 0.7318 acc_val: 0.7867 time: 0.0242s\n",
            "Epoch: 0931 loss_train: 0.2321 acc_train: 0.9643 loss_val: 0.7324 acc_val: 0.7900 time: 0.0295s\n",
            "Epoch: 0932 loss_train: 0.2468 acc_train: 0.9571 loss_val: 0.7347 acc_val: 0.7900 time: 0.0291s\n",
            "Epoch: 0933 loss_train: 0.2718 acc_train: 0.9357 loss_val: 0.7361 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0934 loss_train: 0.2045 acc_train: 0.9571 loss_val: 0.7368 acc_val: 0.7867 time: 0.0243s\n",
            "Epoch: 0935 loss_train: 0.2451 acc_train: 0.9571 loss_val: 0.7384 acc_val: 0.7867 time: 0.0239s\n",
            "Epoch: 0936 loss_train: 0.2594 acc_train: 0.9357 loss_val: 0.7391 acc_val: 0.7900 time: 0.0281s\n",
            "Epoch: 0937 loss_train: 0.2522 acc_train: 0.9429 loss_val: 0.7395 acc_val: 0.7933 time: 0.0244s\n",
            "Epoch: 0938 loss_train: 0.2134 acc_train: 0.9714 loss_val: 0.7384 acc_val: 0.7933 time: 0.0264s\n",
            "Epoch: 0939 loss_train: 0.2719 acc_train: 0.9429 loss_val: 0.7367 acc_val: 0.7933 time: 0.0305s\n",
            "Epoch: 0940 loss_train: 0.2167 acc_train: 0.9571 loss_val: 0.7342 acc_val: 0.7900 time: 0.0241s\n",
            "Epoch: 0941 loss_train: 0.2365 acc_train: 0.9500 loss_val: 0.7335 acc_val: 0.7833 time: 0.0239s\n",
            "Epoch: 0942 loss_train: 0.2439 acc_train: 0.9500 loss_val: 0.7334 acc_val: 0.7833 time: 0.0240s\n",
            "Epoch: 0943 loss_train: 0.2370 acc_train: 0.9500 loss_val: 0.7325 acc_val: 0.7867 time: 0.0235s\n",
            "Epoch: 0944 loss_train: 0.2149 acc_train: 0.9786 loss_val: 0.7331 acc_val: 0.7900 time: 0.0233s\n",
            "Epoch: 0945 loss_train: 0.1938 acc_train: 0.9786 loss_val: 0.7334 acc_val: 0.7933 time: 0.0236s\n",
            "Epoch: 0946 loss_train: 0.2546 acc_train: 0.9357 loss_val: 0.7328 acc_val: 0.7900 time: 0.0240s\n",
            "Epoch: 0947 loss_train: 0.2064 acc_train: 0.9643 loss_val: 0.7332 acc_val: 0.7900 time: 0.0285s\n",
            "Epoch: 0948 loss_train: 0.2200 acc_train: 0.9500 loss_val: 0.7337 acc_val: 0.7900 time: 0.0250s\n",
            "Epoch: 0949 loss_train: 0.2123 acc_train: 0.9571 loss_val: 0.7341 acc_val: 0.7867 time: 0.0246s\n",
            "Epoch: 0950 loss_train: 0.2224 acc_train: 0.9714 loss_val: 0.7353 acc_val: 0.7900 time: 0.0260s\n",
            "Epoch: 0951 loss_train: 0.2010 acc_train: 0.9643 loss_val: 0.7373 acc_val: 0.7933 time: 0.0246s\n",
            "Epoch: 0952 loss_train: 0.2206 acc_train: 0.9571 loss_val: 0.7406 acc_val: 0.7933 time: 0.0254s\n",
            "Epoch: 0953 loss_train: 0.1881 acc_train: 0.9643 loss_val: 0.7424 acc_val: 0.7900 time: 0.0246s\n",
            "Epoch: 0954 loss_train: 0.2053 acc_train: 0.9714 loss_val: 0.7434 acc_val: 0.7833 time: 0.0241s\n",
            "Epoch: 0955 loss_train: 0.2209 acc_train: 0.9643 loss_val: 0.7452 acc_val: 0.7800 time: 0.0304s\n",
            "Epoch: 0956 loss_train: 0.2385 acc_train: 0.9571 loss_val: 0.7472 acc_val: 0.7800 time: 0.0303s\n",
            "Epoch: 0957 loss_train: 0.1692 acc_train: 0.9714 loss_val: 0.7481 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0958 loss_train: 0.2331 acc_train: 0.9500 loss_val: 0.7454 acc_val: 0.7833 time: 0.0293s\n",
            "Epoch: 0959 loss_train: 0.2025 acc_train: 0.9786 loss_val: 0.7408 acc_val: 0.7833 time: 0.0237s\n",
            "Epoch: 0960 loss_train: 0.2389 acc_train: 0.9500 loss_val: 0.7377 acc_val: 0.7867 time: 0.0248s\n",
            "Epoch: 0961 loss_train: 0.2074 acc_train: 0.9714 loss_val: 0.7343 acc_val: 0.7833 time: 0.0243s\n",
            "Epoch: 0962 loss_train: 0.2285 acc_train: 0.9500 loss_val: 0.7319 acc_val: 0.7900 time: 0.0242s\n",
            "Epoch: 0963 loss_train: 0.1691 acc_train: 0.9714 loss_val: 0.7315 acc_val: 0.7900 time: 0.0305s\n",
            "Epoch: 0964 loss_train: 0.2037 acc_train: 0.9643 loss_val: 0.7317 acc_val: 0.7900 time: 0.0288s\n",
            "Epoch: 0965 loss_train: 0.1972 acc_train: 0.9714 loss_val: 0.7343 acc_val: 0.7900 time: 0.0234s\n",
            "Epoch: 0966 loss_train: 0.2077 acc_train: 0.9786 loss_val: 0.7370 acc_val: 0.7900 time: 0.0239s\n",
            "Epoch: 0967 loss_train: 0.2221 acc_train: 0.9571 loss_val: 0.7407 acc_val: 0.7900 time: 0.0249s\n",
            "Epoch: 0968 loss_train: 0.2313 acc_train: 0.9643 loss_val: 0.7437 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0969 loss_train: 0.2547 acc_train: 0.9571 loss_val: 0.7445 acc_val: 0.7833 time: 0.0242s\n",
            "Epoch: 0970 loss_train: 0.1872 acc_train: 0.9786 loss_val: 0.7439 acc_val: 0.7833 time: 0.0250s\n",
            "Epoch: 0971 loss_train: 0.2281 acc_train: 0.9643 loss_val: 0.7437 acc_val: 0.7833 time: 0.0310s\n",
            "Epoch: 0972 loss_train: 0.2757 acc_train: 0.9357 loss_val: 0.7449 acc_val: 0.7833 time: 0.0246s\n",
            "Epoch: 0973 loss_train: 0.1942 acc_train: 0.9786 loss_val: 0.7433 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0974 loss_train: 0.2199 acc_train: 0.9643 loss_val: 0.7413 acc_val: 0.7833 time: 0.0255s\n",
            "Epoch: 0975 loss_train: 0.2024 acc_train: 0.9571 loss_val: 0.7376 acc_val: 0.7833 time: 0.0248s\n",
            "Epoch: 0976 loss_train: 0.2737 acc_train: 0.9714 loss_val: 0.7352 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0977 loss_train: 0.2478 acc_train: 0.9429 loss_val: 0.7335 acc_val: 0.7867 time: 0.0301s\n",
            "Epoch: 0978 loss_train: 0.2063 acc_train: 0.9643 loss_val: 0.7331 acc_val: 0.7833 time: 0.0291s\n",
            "Epoch: 0979 loss_train: 0.2204 acc_train: 0.9857 loss_val: 0.7339 acc_val: 0.7833 time: 0.0273s\n",
            "Epoch: 0980 loss_train: 0.2130 acc_train: 0.9714 loss_val: 0.7364 acc_val: 0.7867 time: 0.0251s\n",
            "Epoch: 0981 loss_train: 0.2075 acc_train: 0.9714 loss_val: 0.7378 acc_val: 0.7867 time: 0.0255s\n",
            "Epoch: 0982 loss_train: 0.2159 acc_train: 0.9500 loss_val: 0.7382 acc_val: 0.7833 time: 0.0308s\n",
            "Epoch: 0983 loss_train: 0.2079 acc_train: 0.9714 loss_val: 0.7383 acc_val: 0.7833 time: 0.0254s\n",
            "Epoch: 0984 loss_train: 0.2652 acc_train: 0.9214 loss_val: 0.7416 acc_val: 0.7833 time: 0.0239s\n",
            "Epoch: 0985 loss_train: 0.2450 acc_train: 0.9714 loss_val: 0.7454 acc_val: 0.7800 time: 0.0242s\n",
            "Epoch: 0986 loss_train: 0.2189 acc_train: 0.9571 loss_val: 0.7489 acc_val: 0.7800 time: 0.0381s\n",
            "Epoch: 0987 loss_train: 0.2323 acc_train: 0.9500 loss_val: 0.7493 acc_val: 0.7867 time: 0.0340s\n",
            "Epoch: 0988 loss_train: 0.2345 acc_train: 0.9571 loss_val: 0.7464 acc_val: 0.7867 time: 0.0355s\n",
            "Epoch: 0989 loss_train: 0.2289 acc_train: 0.9571 loss_val: 0.7435 acc_val: 0.7900 time: 0.0331s\n",
            "Epoch: 0990 loss_train: 0.2381 acc_train: 0.9500 loss_val: 0.7394 acc_val: 0.7900 time: 0.0385s\n",
            "Epoch: 0991 loss_train: 0.2206 acc_train: 0.9571 loss_val: 0.7356 acc_val: 0.7867 time: 0.0388s\n",
            "Epoch: 0992 loss_train: 0.2148 acc_train: 0.9714 loss_val: 0.7315 acc_val: 0.7900 time: 0.0360s\n",
            "Epoch: 0993 loss_train: 0.2301 acc_train: 0.9357 loss_val: 0.7309 acc_val: 0.7933 time: 0.0329s\n",
            "Epoch: 0994 loss_train: 0.2713 acc_train: 0.9286 loss_val: 0.7323 acc_val: 0.7900 time: 0.0328s\n",
            "Epoch: 0995 loss_train: 0.2049 acc_train: 0.9857 loss_val: 0.7328 acc_val: 0.7867 time: 0.0329s\n",
            "Epoch: 0996 loss_train: 0.2523 acc_train: 0.9429 loss_val: 0.7348 acc_val: 0.7867 time: 0.0331s\n",
            "Epoch: 0997 loss_train: 0.2206 acc_train: 0.9643 loss_val: 0.7373 acc_val: 0.7867 time: 0.0309s\n",
            "Epoch: 0998 loss_train: 0.2117 acc_train: 0.9786 loss_val: 0.7400 acc_val: 0.7833 time: 0.0414s\n",
            "Epoch: 0999 loss_train: 0.2305 acc_train: 0.9714 loss_val: 0.7442 acc_val: 0.7833 time: 0.0314s\n",
            "Epoch: 1000 loss_train: 0.2379 acc_train: 0.9571 loss_val: 0.7459 acc_val: 0.7867 time: 0.0315s\n",
            "Optimization Finished!\n",
            "Total time slapsed: 30.9696s\n",
            "Test set results loss= 0.8263 accuracy= 0.7610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0gjYslwDFXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}